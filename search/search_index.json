{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"index.html","title":"\ud83d\udc4b Welcome to EmbodiedGen","text":"<p>EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence.</p> <p></p> <p>EmbodiedGen is a generative engine to create diverse and interactive 3D worlds composed of high-quality 3D assets(mesh &amp; 3DGS) with plausible physics, leveraging generative AI to address the challenges of generalization in embodied intelligence related research.</p>"},{"location":"acknowledgement.html","title":"\ud83d\ude4c Acknowledgement","text":"<p>EmbodiedGen builds upon the following amazing projects and models: \ud83c\udf1f Trellis | \ud83c\udf1f Hunyuan-Delight | \ud83c\udf1f Segment Anything | \ud83c\udf1f Rembg | \ud83c\udf1f RMBG-1.4 | \ud83c\udf1f Stable Diffusion x4 | \ud83c\udf1f Real-ESRGAN | \ud83c\udf1f Kolors | \ud83c\udf1f ChatGLM3 | \ud83c\udf1f Aesthetic Score | \ud83c\udf1f Pano2Room | \ud83c\udf1f Diffusion360 | \ud83c\udf1f Kaolin | \ud83c\udf1f diffusers | \ud83c\udf1f gsplat | \ud83c\udf1f QWEN-2.5VL | \ud83c\udf1f GPT4o | \ud83c\udf1f SD3.5 | \ud83c\udf1f ManiSkill | \ud83c\udf1f SAM3D</p>"},{"location":"acknowledgement.html#citation","title":"\ud83d\udcda Citation","text":"<p>If you use EmbodiedGen in your research or projects, please cite:</p> <pre><code>@misc{wang2025embodiedgengenerative3dworld,\n      title={EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence},\n      author={Xinjie Wang and Liu Liu and Yu Cao and Ruiqi Wu and Wenkang Qin and Dehui Wang and Wei Sui and Zhizhong Su},\n      year={2025},\n      eprint={2506.10600},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO},\n      url={https://arxiv.org/abs/2506.10600},\n}\n</code></pre>"},{"location":"acknowledgement.html#license","title":"\u2696\ufe0f License","text":"<p>This project is licensed under the Apache License 2.0. See the <code>LICENSE</code> file for details.</p>"},{"location":"install.html","title":"\ud83d\ude80 Installation","text":""},{"location":"install.html#setup-environment","title":"\u2705 Setup Environment","text":"<pre><code>git clone https://github.com/HorizonRobotics/EmbodiedGen.git\ncd EmbodiedGen\ngit checkout v0.1.7\ngit submodule update --init --recursive --progress\nconda create -n embodiedgen python=3.10.13 -y # recommended to use a new env.\nconda activate embodiedgen\nbash install.sh basic # around 20 mins\n# Optional: `bash install.sh extra` for scene3d-cli\n</code></pre> <p>Please <code>huggingface-cli login</code> to ensure that the ckpts can be downloaded automatically afterwards.</p>"},{"location":"install.html#starting-from-docker","title":"\u2705 Starting from Docker","text":"<p>We provide a pre-built Docker image on Docker Hub with a configured environment for your convenience. For more details, please refer to Docker documentation.</p> <p>Note: Model checkpoints are not included in the image, they will be automatically downloaded on first run. You still need to set up the GPT Agent manually.</p> <pre><code>IMAGE=wangxinjie/embodiedgen:env_v0.1.x\nCONTAINER=EmbodiedGen-docker-${USER}\ndocker pull ${IMAGE}\ndocker run -itd --shm-size=\"64g\" --gpus all --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --privileged --net=host --name ${CONTAINER} ${IMAGE}\ndocker exec -it ${CONTAINER} bash\n</code></pre>"},{"location":"install.html#setup-gpt-agent","title":"\u2705 Setup GPT Agent","text":"<p>Update the API key in file: <code>embodied_gen/utils/gpt_config.yaml</code>.</p> <p>You can choose between two backends for the GPT agent:</p> <ul> <li><code>gpt-4o</code> (Recommended) \u2013 Use this if you have access to Azure OpenAI.</li> <li><code>qwen2.5-vl</code> \u2013 An alternative with free usage via OpenRouter, apply a free key here and update <code>api_key</code> in <code>embodied_gen/utils/gpt_config.yaml</code> (50 free requests per day)</li> </ul>"},{"location":"api/index.html","title":"API Reference","text":"<p>Welcome to the API reference for EmbodiedGen.</p> <p>This section contains detailed documentation for all public modules, classes, and functions. Use the navigation on the left (or the list below) to browse the different components.</p> <ul> <li>Data API: Tools for data processing, conversion, and rendering.</li> <li>Envs API: Simulation environment definitions.</li> <li>Models API: The core generative models (Texture, 3DGS, Layout, etc.).</li> <li>Trainer API: PyTorch-Lightning style trainers for models.</li> <li>Utilities API: Helper functions and configuration.</li> <li>Validators API: Tools for checking and validating assets.</li> </ul>"},{"location":"api/data.html","title":"Data API","text":""},{"location":"api/data.html#embodied_gen.data.asset_converter","title":"embodied_gen.data.asset_converter","text":""},{"location":"api/data.html#embodied_gen.data.asset_converter.AssetConverterBase","title":"AssetConverterBase","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for asset converters.</p> <p>Provides context management and mesh transformation utilities.</p>"},{"location":"api/data.html#embodied_gen.data.asset_converter.AssetConverterBase.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Context manager entry.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry.\"\"\"\n    return self\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.AssetConverterBase.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Context manager exit.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit.\"\"\"\n    return False\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.AssetConverterBase.convert","title":"convert  <code>abstractmethod</code>","text":"<pre><code>convert(urdf_path: str, output_path: str, **kwargs) -&gt; str\n</code></pre> <p>Convert an asset file.</p> <p>Parameters:</p> Name Type Description Default <code>urdf_path</code> <code>str</code> <p>Path to input URDF file.</p> required <code>output_path</code> <code>str</code> <p>Path to output file.</p> required <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to converted asset.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>@abstractmethod\ndef convert(self, urdf_path: str, output_path: str, **kwargs) -&gt; str:\n    \"\"\"Convert an asset file.\n\n    Args:\n        urdf_path (str): Path to input URDF file.\n        output_path (str): Path to output file.\n        **kwargs: Additional arguments.\n\n    Returns:\n        str: Path to converted asset.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.AssetConverterBase.transform_mesh","title":"transform_mesh","text":"<pre><code>transform_mesh(input_mesh: str, output_mesh: str, mesh_origin: Element) -&gt; None\n</code></pre> <p>Apply transform to mesh based on URDF origin element.</p> <p>Parameters:</p> Name Type Description Default <code>input_mesh</code> <code>str</code> <p>Path to input mesh.</p> required <code>output_mesh</code> <code>str</code> <p>Path to output mesh.</p> required <code>mesh_origin</code> <code>Element</code> <p>Origin element from URDF.</p> required Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def transform_mesh(\n    self, input_mesh: str, output_mesh: str, mesh_origin: ET.Element\n) -&gt; None:\n    \"\"\"Apply transform to mesh based on URDF origin element.\n\n    Args:\n        input_mesh (str): Path to input mesh.\n        output_mesh (str): Path to output mesh.\n        mesh_origin (ET.Element): Origin element from URDF.\n    \"\"\"\n    mesh = trimesh.load(input_mesh, group_material=False)\n    rpy = list(map(float, mesh_origin.get(\"rpy\").split(\" \")))\n    rotation = Rotation.from_euler(\"xyz\", rpy, degrees=False)\n    offset = list(map(float, mesh_origin.get(\"xyz\").split(\" \")))\n    os.makedirs(os.path.dirname(output_mesh), exist_ok=True)\n\n    if isinstance(mesh, trimesh.Scene):\n        combined = trimesh.Scene()\n        for mesh_part in mesh.geometry.values():\n            mesh_part.vertices = (\n                mesh_part.vertices @ rotation.as_matrix().T\n            ) + offset\n            combined.add_geometry(mesh_part)\n        _ = combined.export(output_mesh)\n    else:\n        mesh.vertices = (mesh.vertices @ rotation.as_matrix().T) + offset\n        _ = mesh.export(output_mesh)\n\n    return\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.AssetConverterFactory","title":"AssetConverterFactory","text":"<p>Factory for creating asset converters based on target and source types.</p> Example <pre><code>from embodied_gen.data.asset_converter import AssetConverterFactory\nfrom embodied_gen.utils.enum import AssetType\n\nconverter = AssetConverterFactory.create(\n    target_type=AssetType.USD, source_type=AssetType.MESH\n)\nwith converter:\n    for urdf_path, output_file in zip(urdf_paths, output_files):\n        converter.convert(urdf_path, output_file)\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.AssetConverterFactory.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(target_type: AssetType, source_type: AssetType = 'urdf', **kwargs) -&gt; AssetConverterBase\n</code></pre> <p>Creates an asset converter instance.</p> <p>Parameters:</p> Name Type Description Default <code>target_type</code> <code>AssetType</code> <p>Target asset type.</p> required <code>source_type</code> <code>AssetType</code> <p>Source asset type.</p> <code>'urdf'</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AssetConverterBase</code> <code>AssetConverterBase</code> <p>Converter instance.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>@staticmethod\ndef create(\n    target_type: AssetType, source_type: AssetType = \"urdf\", **kwargs\n) -&gt; AssetConverterBase:\n    \"\"\"Creates an asset converter instance.\n\n    Args:\n        target_type (AssetType): Target asset type.\n        source_type (AssetType, optional): Source asset type.\n        **kwargs: Additional arguments.\n\n    Returns:\n        AssetConverterBase: Converter instance.\n    \"\"\"\n    if target_type == AssetType.MJCF and source_type == AssetType.MESH:\n        converter = MeshtoMJCFConverter(**kwargs)\n    elif target_type == AssetType.MJCF and source_type == AssetType.URDF:\n        converter = URDFtoMJCFConverter(**kwargs)\n    elif target_type == AssetType.USD and source_type == AssetType.MESH:\n        converter = MeshtoUSDConverter(**kwargs)\n    elif target_type == AssetType.USD and source_type == AssetType.URDF:\n        converter = URDFtoUSDConverter(**kwargs)\n    else:\n        raise ValueError(\n            f\"Unsupported converter type: {source_type} -&gt; {target_type}.\"\n        )\n\n    return converter\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.MeshtoMJCFConverter","title":"MeshtoMJCFConverter","text":"<pre><code>MeshtoMJCFConverter(**kwargs)\n</code></pre> <p>               Bases: <code>AssetConverterBase</code></p> <p>Converts mesh-based URDF files to MJCF format.</p> <p>Handles geometry, materials, and asset copying.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n) -&gt; None:\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.MeshtoMJCFConverter.add_geometry","title":"add_geometry","text":"<pre><code>add_geometry(mujoco_element: Element, link: Element, body: Element, tag: str, input_dir: str, output_dir: str, mesh_name: str, material: Element | None = None, is_collision: bool = False) -&gt; None\n</code></pre> <p>Adds geometry to MJCF body from URDF link.</p> <p>Parameters:</p> Name Type Description Default <code>mujoco_element</code> <code>Element</code> <p>MJCF asset element.</p> required <code>link</code> <code>Element</code> <p>URDF link element.</p> required <code>body</code> <code>Element</code> <p>MJCF body element.</p> required <code>tag</code> <code>str</code> <p>Tag name (\"visual\" or \"collision\").</p> required <code>input_dir</code> <code>str</code> <p>Input directory.</p> required <code>output_dir</code> <code>str</code> <p>Output directory.</p> required <code>mesh_name</code> <code>str</code> <p>Mesh name.</p> required <code>material</code> <code>Element</code> <p>Material element.</p> <code>None</code> <code>is_collision</code> <code>bool</code> <p>If True, treat as collision geometry.</p> <code>False</code> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def add_geometry(\n    self,\n    mujoco_element: ET.Element,\n    link: ET.Element,\n    body: ET.Element,\n    tag: str,\n    input_dir: str,\n    output_dir: str,\n    mesh_name: str,\n    material: ET.Element | None = None,\n    is_collision: bool = False,\n) -&gt; None:\n    \"\"\"Adds geometry to MJCF body from URDF link.\n\n    Args:\n        mujoco_element (ET.Element): MJCF asset element.\n        link (ET.Element): URDF link element.\n        body (ET.Element): MJCF body element.\n        tag (str): Tag name (\"visual\" or \"collision\").\n        input_dir (str): Input directory.\n        output_dir (str): Output directory.\n        mesh_name (str): Mesh name.\n        material (ET.Element, optional): Material element.\n        is_collision (bool, optional): If True, treat as collision geometry.\n    \"\"\"\n    element = link.find(tag)\n    geometry = element.find(\"geometry\")\n    mesh = geometry.find(\"mesh\")\n    filename = mesh.get(\"filename\")\n    scale = mesh.get(\"scale\", \"1.0 1.0 1.0\")\n    input_mesh = f\"{input_dir}/{filename}\"\n    output_mesh = f\"{output_dir}/{filename}\"\n    self._copy_asset_file(input_mesh, output_mesh)\n\n    mesh_origin = element.find(\"origin\")\n    if mesh_origin is not None:\n        self.transform_mesh(input_mesh, output_mesh, mesh_origin)\n\n    if is_collision:\n        mesh_parts = trimesh.load(\n            output_mesh, group_material=False, force=\"scene\"\n        )\n        mesh_parts = mesh_parts.geometry.values()\n    else:\n        mesh_parts = [trimesh.load(output_mesh, force=\"mesh\")]\n    for idx, mesh_part in enumerate(mesh_parts):\n        if is_collision:\n            idx_mesh_name = f\"{mesh_name}_{idx}\"\n            base, ext = os.path.splitext(filename)\n            idx_filename = f\"{base}_{idx}{ext}\"\n            base_outdir = os.path.dirname(output_mesh)\n            mesh_part.export(os.path.join(base_outdir, '..', idx_filename))\n            geom_attrs = {\n                \"contype\": \"1\",\n                \"conaffinity\": \"1\",\n                \"rgba\": \"1 1 1 0\",\n            }\n        else:\n            idx_mesh_name, idx_filename = mesh_name, filename\n            geom_attrs = {\"contype\": \"0\", \"conaffinity\": \"0\"}\n\n        ET.SubElement(\n            mujoco_element,\n            \"mesh\",\n            name=idx_mesh_name,\n            file=idx_filename,\n            scale=scale,\n        )\n        geom = ET.SubElement(body, \"geom\", type=\"mesh\", mesh=idx_mesh_name)\n        geom.attrib.update(geom_attrs)\n        if material is not None:\n            geom.set(\"material\", material.get(\"name\"))\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.MeshtoMJCFConverter.add_materials","title":"add_materials","text":"<pre><code>add_materials(mujoco_element: Element, link: Element, tag: str, input_dir: str, output_dir: str, name: str, reflectance: float = 0.2) -&gt; ET.Element\n</code></pre> <p>Adds materials to MJCF asset from URDF link.</p> <p>Parameters:</p> Name Type Description Default <code>mujoco_element</code> <code>Element</code> <p>MJCF asset element.</p> required <code>link</code> <code>Element</code> <p>URDF link element.</p> required <code>tag</code> <code>str</code> <p>Tag name.</p> required <code>input_dir</code> <code>str</code> <p>Input directory.</p> required <code>output_dir</code> <code>str</code> <p>Output directory.</p> required <code>name</code> <code>str</code> <p>Material name.</p> required <code>reflectance</code> <code>float</code> <p>Reflectance value.</p> <code>0.2</code> <p>Returns:</p> Type Description <code>Element</code> <p>ET.Element: Material element.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def add_materials(\n    self,\n    mujoco_element: ET.Element,\n    link: ET.Element,\n    tag: str,\n    input_dir: str,\n    output_dir: str,\n    name: str,\n    reflectance: float = 0.2,\n) -&gt; ET.Element:\n    \"\"\"Adds materials to MJCF asset from URDF link.\n\n    Args:\n        mujoco_element (ET.Element): MJCF asset element.\n        link (ET.Element): URDF link element.\n        tag (str): Tag name.\n        input_dir (str): Input directory.\n        output_dir (str): Output directory.\n        name (str): Material name.\n        reflectance (float, optional): Reflectance value.\n\n    Returns:\n        ET.Element: Material element.\n    \"\"\"\n    element = link.find(tag)\n    geometry = element.find(\"geometry\")\n    mesh = geometry.find(\"mesh\")\n    filename = mesh.get(\"filename\")\n    dirname = os.path.dirname(filename)\n    material = None\n    for path in glob(f\"{input_dir}/{dirname}/*.png\"):\n        file_name = os.path.basename(path)\n        if \"keep_materials\" in self.kwargs:\n            find_flag = False\n            for keep_key in self.kwargs[\"keep_materials\"]:\n                if keep_key in file_name.lower():\n                    find_flag = True\n            if find_flag is False:\n                continue\n\n        self._copy_asset_file(\n            path,\n            f\"{output_dir}/{dirname}/{file_name}\",\n        )\n        texture_name = f\"texture_{name}_{os.path.splitext(file_name)[0]}\"\n        material = ET.SubElement(\n            mujoco_element,\n            \"material\",\n            name=f\"material_{name}\",\n            texture=texture_name,\n            reflectance=str(reflectance),\n        )\n        ET.SubElement(\n            mujoco_element,\n            \"texture\",\n            name=texture_name,\n            type=\"2d\",\n            file=f\"{dirname}/{file_name}\",\n        )\n\n    return material\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.MeshtoMJCFConverter.convert","title":"convert","text":"<pre><code>convert(urdf_path: str, mjcf_path: str)\n</code></pre> <p>Converts a URDF file to MJCF format.</p> <p>Parameters:</p> Name Type Description Default <code>urdf_path</code> <code>str</code> <p>Path to URDF file.</p> required <code>mjcf_path</code> <code>str</code> <p>Path to output MJCF file.</p> required Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def convert(self, urdf_path: str, mjcf_path: str):\n    \"\"\"Converts a URDF file to MJCF format.\n\n    Args:\n        urdf_path (str): Path to URDF file.\n        mjcf_path (str): Path to output MJCF file.\n    \"\"\"\n    tree = ET.parse(urdf_path)\n    root = tree.getroot()\n\n    mujoco_struct = ET.Element(\"mujoco\")\n    mujoco_struct.set(\"model\", root.get(\"name\"))\n    mujoco_asset = ET.SubElement(mujoco_struct, \"asset\")\n    mujoco_worldbody = ET.SubElement(mujoco_struct, \"worldbody\")\n\n    input_dir = os.path.dirname(urdf_path)\n    output_dir = os.path.dirname(mjcf_path)\n    os.makedirs(output_dir, exist_ok=True)\n    for idx, link in enumerate(root.findall(\"link\")):\n        link_name = link.get(\"name\", \"unnamed_link\")\n        body = ET.SubElement(mujoco_worldbody, \"body\", name=link_name)\n\n        material = self.add_materials(\n            mujoco_asset,\n            link,\n            \"visual\",\n            input_dir,\n            output_dir,\n            name=str(idx),\n        )\n        joint = ET.SubElement(body, \"joint\", attrib={\"type\": \"free\"})\n        self.add_geometry(\n            mujoco_asset,\n            link,\n            body,\n            \"visual\",\n            input_dir,\n            output_dir,\n            f\"visual_mesh_{idx}\",\n            material,\n        )\n        self.add_geometry(\n            mujoco_asset,\n            link,\n            body,\n            \"collision\",\n            input_dir,\n            output_dir,\n            f\"collision_mesh_{idx}\",\n            is_collision=True,\n        )\n\n    tree = ET.ElementTree(mujoco_struct)\n    ET.indent(tree, space=\"  \", level=0)\n\n    tree.write(mjcf_path, encoding=\"utf-8\", xml_declaration=True)\n    logger.info(f\"Successfully converted {urdf_path} \u2192 {mjcf_path}\")\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.MeshtoUSDConverter","title":"MeshtoUSDConverter","text":"<pre><code>MeshtoUSDConverter(force_usd_conversion: bool = True, make_instanceable: bool = False, simulation_app=None, **kwargs)\n</code></pre> <p>               Bases: <code>AssetConverterBase</code></p> <p>Converts mesh-based URDF files to USD format.</p> <p>Adds physics APIs and post-processes collision meshes.</p> <p>Initializes the converter.</p> <p>Parameters:</p> Name Type Description Default <code>force_usd_conversion</code> <code>bool</code> <p>Force USD conversion.</p> <code>True</code> <code>make_instanceable</code> <code>bool</code> <p>Make prims instanceable.</p> <code>False</code> <code>simulation_app</code> <code>optional</code> <p>Simulation app instance.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __init__(\n    self,\n    force_usd_conversion: bool = True,\n    make_instanceable: bool = False,\n    simulation_app=None,\n    **kwargs,\n):\n    \"\"\"Initializes the converter.\n\n    Args:\n        force_usd_conversion (bool, optional): Force USD conversion.\n        make_instanceable (bool, optional): Make prims instanceable.\n        simulation_app (optional): Simulation app instance.\n        **kwargs: Additional arguments.\n    \"\"\"\n    if simulation_app is not None:\n        self.simulation_app = simulation_app\n\n    self.exit_close = kwargs.pop(\"exit_close\", True)\n    self.physx_max_convex_hulls = kwargs.pop(\"physx_max_convex_hulls\", 32)\n    self.physx_max_vertices = kwargs.pop(\"physx_max_vertices\", 16)\n    self.physx_max_voxel_res = kwargs.pop(\"physx_max_voxel_res\", 10000)\n\n    self.usd_parms = dict(\n        force_usd_conversion=force_usd_conversion,\n        make_instanceable=make_instanceable,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.MeshtoUSDConverter.__enter__","title":"__enter__","text":"<pre><code>__enter__()\n</code></pre> <p>Context manager entry, launches simulation app if needed.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __enter__(self):\n    \"\"\"Context manager entry, launches simulation app if needed.\"\"\"\n    from isaaclab.app import AppLauncher\n\n    if not hasattr(self, \"simulation_app\"):\n        if \"launch_args\" not in self.usd_parms:\n            launch_args = dict(\n                headless=True,\n                no_splash=True,\n                fast_shutdown=True,\n                disable_gpu=True,\n            )\n        else:\n            launch_args = self.usd_parms.pop(\"launch_args\")\n        self.app_launcher = AppLauncher(launch_args)\n        self.simulation_app = self.app_launcher.app\n\n    return self\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.MeshtoUSDConverter.__exit__","title":"__exit__","text":"<pre><code>__exit__(exc_type, exc_val, exc_tb)\n</code></pre> <p>Context manager exit, closes simulation app if created.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __exit__(self, exc_type, exc_val, exc_tb):\n    \"\"\"Context manager exit, closes simulation app if created.\"\"\"\n    # Close the simulation app if it was created here\n    if exc_val is not None:\n        logger.error(f\"Exception occurred: {exc_val}.\")\n\n    if hasattr(self, \"app_launcher\") and self.exit_close:\n        self.simulation_app.close()\n\n    return False\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.MeshtoUSDConverter.convert","title":"convert","text":"<pre><code>convert(urdf_path: str, output_file: str)\n</code></pre> <p>Converts a URDF file to USD and post-processes collision meshes.</p> <p>Parameters:</p> Name Type Description Default <code>urdf_path</code> <code>str</code> <p>Path to URDF file.</p> required <code>output_file</code> <code>str</code> <p>Path to output USD file.</p> required Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def convert(self, urdf_path: str, output_file: str):\n    \"\"\"Converts a URDF file to USD and post-processes collision meshes.\n\n    Args:\n        urdf_path (str): Path to URDF file.\n        output_file (str): Path to output USD file.\n    \"\"\"\n    from isaaclab.sim.converters import MeshConverter, MeshConverterCfg\n    from pxr import PhysxSchema, Sdf, Usd, UsdShade\n\n    tree = ET.parse(urdf_path)\n    root = tree.getroot()\n    mesh_file = root.find(\"link/visual/geometry/mesh\").get(\"filename\")\n    input_mesh = os.path.join(os.path.dirname(urdf_path), mesh_file)\n    output_dir = os.path.abspath(os.path.dirname(output_file))\n    output_mesh = f\"{output_dir}/mesh/{os.path.basename(mesh_file)}\"\n    mesh_origin = root.find(\"link/visual/origin\")\n    if mesh_origin is not None:\n        self.transform_mesh(input_mesh, output_mesh, mesh_origin)\n\n    cfg = MeshConverterCfg(\n        asset_path=output_mesh,\n        usd_dir=output_dir,\n        usd_file_name=os.path.basename(output_file),\n        **self.usd_parms,\n    )\n    urdf_converter = MeshConverter(cfg)\n    usd_path = urdf_converter.usd_path\n    rmtree(os.path.dirname(output_mesh))\n\n    stage = Usd.Stage.Open(usd_path)\n    layer = stage.GetRootLayer()\n    with Usd.EditContext(stage, layer):\n        base_prim = stage.GetPseudoRoot().GetChildren()[0]\n        base_prim.SetMetadata(\"kind\", \"component\")\n        for prim in stage.Traverse():\n            # Change texture path to relative path.\n            if prim.GetName() == \"material_0\":\n                shader = UsdShade.Shader(prim).GetInput(\"diffuse_texture\")\n                if shader.Get() is not None:\n                    relative_path = shader.Get().path.replace(\n                        f\"{output_dir}/\", \"\"\n                    )\n                    shader.Set(Sdf.AssetPath(relative_path))\n\n            # Add convex decomposition collision and set ShrinkWrap.\n            elif prim.GetName() == \"mesh\":\n                approx_attr = prim.CreateAttribute(\n                    \"physics:approximation\", Sdf.ValueTypeNames.Token\n                )\n                approx_attr.Set(\"convexDecomposition\")\n\n                physx_conv_api = (\n                    PhysxSchema.PhysxConvexDecompositionCollisionAPI.Apply(\n                        prim\n                    )\n                )\n                physx_conv_api.GetMaxConvexHullsAttr().Set(\n                    self.physx_max_convex_hulls\n                )\n                physx_conv_api.GetHullVertexLimitAttr().Set(\n                    self.physx_max_vertices\n                )\n                physx_conv_api.GetVoxelResolutionAttr().Set(\n                    self.physx_max_voxel_res\n                )\n                physx_conv_api.GetShrinkWrapAttr().Set(True)\n\n                api_schemas = prim.GetMetadata(\"apiSchemas\")\n                if api_schemas is None:\n                    api_schemas = Sdf.TokenListOp()\n\n                api_list = list(api_schemas.GetAddedOrExplicitItems())\n                for api in self.DEFAULT_BIND_APIS:\n                    if api not in api_list:\n                        api_list.append(api)\n\n                api_schemas.appendedItems = api_list\n                prim.SetMetadata(\"apiSchemas\", api_schemas)\n\n    layer.Save()\n    logger.info(f\"Successfully converted {urdf_path} \u2192 {usd_path}\")\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.PhysicsUSDAdder","title":"PhysicsUSDAdder","text":"<pre><code>PhysicsUSDAdder(force_usd_conversion: bool = True, make_instanceable: bool = False, simulation_app=None, **kwargs)\n</code></pre> <p>               Bases: <code>MeshtoUSDConverter</code></p> <p>Adds physics APIs and collision properties to USD assets.</p> <p>Useful for post-processing USD files for simulation.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __init__(\n    self,\n    force_usd_conversion: bool = True,\n    make_instanceable: bool = False,\n    simulation_app=None,\n    **kwargs,\n):\n    \"\"\"Initializes the converter.\n\n    Args:\n        force_usd_conversion (bool, optional): Force USD conversion.\n        make_instanceable (bool, optional): Make prims instanceable.\n        simulation_app (optional): Simulation app instance.\n        **kwargs: Additional arguments.\n    \"\"\"\n    if simulation_app is not None:\n        self.simulation_app = simulation_app\n\n    self.exit_close = kwargs.pop(\"exit_close\", True)\n    self.physx_max_convex_hulls = kwargs.pop(\"physx_max_convex_hulls\", 32)\n    self.physx_max_vertices = kwargs.pop(\"physx_max_vertices\", 16)\n    self.physx_max_voxel_res = kwargs.pop(\"physx_max_voxel_res\", 10000)\n\n    self.usd_parms = dict(\n        force_usd_conversion=force_usd_conversion,\n        make_instanceable=make_instanceable,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.PhysicsUSDAdder.convert","title":"convert","text":"<pre><code>convert(usd_path: str, output_file: str = None)\n</code></pre> <p>Adds physics APIs and collision properties to a USD file.</p> <p>Parameters:</p> Name Type Description Default <code>usd_path</code> <code>str</code> <p>Path to input USD file.</p> required <code>output_file</code> <code>str</code> <p>Path to output USD file.</p> <code>None</code> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def convert(self, usd_path: str, output_file: str = None):\n    \"\"\"Adds physics APIs and collision properties to a USD file.\n\n    Args:\n        usd_path (str): Path to input USD file.\n        output_file (str, optional): Path to output USD file.\n    \"\"\"\n    from pxr import PhysxSchema, Sdf, Usd, UsdGeom, UsdPhysics\n\n    if output_file is None:\n        output_file = usd_path\n    else:\n        dst_dir = os.path.dirname(output_file)\n        src_dir = os.path.dirname(usd_path)\n        copytree(src_dir, dst_dir, dirs_exist_ok=True)\n\n    stage = Usd.Stage.Open(output_file)\n    layer = stage.GetRootLayer()\n    with Usd.EditContext(stage, layer):\n        for prim in stage.Traverse():\n            if prim.IsA(UsdGeom.Xform):\n                for child in prim.GetChildren():\n                    if not child.IsA(UsdGeom.Mesh):\n                        continue\n\n                    # Skip the lightfactory in Infinigen\n                    if \"lightfactory\" in prim.GetName().lower():\n                        continue\n\n                    approx_attr = prim.CreateAttribute(\n                        \"physics:approximation\", Sdf.ValueTypeNames.Token\n                    )\n                    approx_attr.Set(\"convexDecomposition\")\n\n                    physx_conv_api = PhysxSchema.PhysxConvexDecompositionCollisionAPI.Apply(\n                        prim\n                    )\n                    physx_conv_api.GetMaxConvexHullsAttr().Set(\n                        self.physx_max_convex_hulls\n                    )\n                    physx_conv_api.GetHullVertexLimitAttr().Set(\n                        self.physx_max_vertices\n                    )\n                    physx_conv_api.GetVoxelResolutionAttr().Set(\n                        self.physx_max_voxel_res\n                    )\n                    physx_conv_api.GetShrinkWrapAttr().Set(True)\n\n                    rigid_body_api = UsdPhysics.RigidBodyAPI.Apply(prim)\n                    rigid_body_api.CreateKinematicEnabledAttr().Set(True)\n                    if prim.GetAttribute(\"physics:mass\"):\n                        prim.RemoveProperty(\"physics:mass\")\n                    if prim.GetAttribute(\"physics:velocity\"):\n                        prim.RemoveProperty(\"physics:velocity\")\n\n                    api_schemas = prim.GetMetadata(\"apiSchemas\")\n                    if api_schemas is None:\n                        api_schemas = Sdf.TokenListOp()\n\n                    api_list = list(api_schemas.GetAddedOrExplicitItems())\n                    for api in self.DEFAULT_BIND_APIS:\n                        if api not in api_list:\n                            api_list.append(api)\n\n                    api_schemas.appendedItems = api_list\n                    prim.SetMetadata(\"apiSchemas\", api_schemas)\n\n    layer.Save()\n    logger.info(f\"Successfully converted {usd_path} to {output_file}\")\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.URDFtoMJCFConverter","title":"URDFtoMJCFConverter","text":"<pre><code>URDFtoMJCFConverter(**kwargs)\n</code></pre> <p>               Bases: <code>MeshtoMJCFConverter</code></p> <p>Converts URDF files with joints to MJCF format, handling joint transformations.</p> <p>Handles fixed joints and hierarchical body structure.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n) -&gt; None:\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.URDFtoMJCFConverter.convert","title":"convert","text":"<pre><code>convert(urdf_path: str, mjcf_path: str, **kwargs) -&gt; str\n</code></pre> <p>Converts a URDF file with joints to MJCF format.</p> <p>Parameters:</p> Name Type Description Default <code>urdf_path</code> <code>str</code> <p>Path to URDF file.</p> required <code>mjcf_path</code> <code>str</code> <p>Path to output MJCF file.</p> required <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to converted MJCF file.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def convert(self, urdf_path: str, mjcf_path: str, **kwargs) -&gt; str:\n    \"\"\"Converts a URDF file with joints to MJCF format.\n\n    Args:\n        urdf_path (str): Path to URDF file.\n        mjcf_path (str): Path to output MJCF file.\n        **kwargs: Additional arguments.\n\n    Returns:\n        str: Path to converted MJCF file.\n    \"\"\"\n    tree = ET.parse(urdf_path)\n    root = tree.getroot()\n\n    mujoco_struct = ET.Element(\"mujoco\")\n    mujoco_struct.set(\"model\", root.get(\"name\"))\n    mujoco_asset = ET.SubElement(mujoco_struct, \"asset\")\n    mujoco_worldbody = ET.SubElement(mujoco_struct, \"worldbody\")\n\n    input_dir = os.path.dirname(urdf_path)\n    output_dir = os.path.dirname(mjcf_path)\n    os.makedirs(output_dir, exist_ok=True)\n\n    body_dict = {}\n    for idx, link in enumerate(root.findall(\"link\")):\n        link_name = link.get(\"name\", f\"unnamed_link_{idx}\")\n        body = ET.SubElement(mujoco_worldbody, \"body\", name=link_name)\n        body_dict[link_name] = body\n        if link.find(\"visual\") is not None:\n            material = self.add_materials(\n                mujoco_asset,\n                link,\n                \"visual\",\n                input_dir,\n                output_dir,\n                name=str(idx),\n            )\n            self.add_geometry(\n                mujoco_asset,\n                link,\n                body,\n                \"visual\",\n                input_dir,\n                output_dir,\n                f\"visual_mesh_{idx}\",\n                material,\n            )\n        if link.find(\"collision\") is not None:\n            self.add_geometry(\n                mujoco_asset,\n                link,\n                body,\n                \"collision\",\n                input_dir,\n                output_dir,\n                f\"collision_mesh_{idx}\",\n                is_collision=True,\n            )\n\n    # Process joints to set transformations and hierarchy\n    for joint in root.findall(\"joint\"):\n        joint_type = joint.get(\"type\")\n        if joint_type != \"fixed\":\n            logger.warning(\"Only support fixed joints in conversion now.\")\n            continue\n\n        parent_link = joint.find(\"parent\").get(\"link\")\n        child_link = joint.find(\"child\").get(\"link\")\n        origin = joint.find(\"origin\")\n        if parent_link not in body_dict or child_link not in body_dict:\n            logger.warning(\n                f\"Parent or child link not found for joint: {joint.get('name')}\"\n            )\n            continue\n\n        child_body = body_dict[child_link]\n        mujoco_worldbody.remove(child_body)\n        parent_body = body_dict[parent_link]\n        parent_body.append(child_body)\n        if origin is not None:\n            xyz = origin.get(\"xyz\", \"0 0 0\")\n            rpy = origin.get(\"rpy\", \"0 0 0\")\n            child_body.set(\"pos\", xyz)\n            child_body.set(\"euler\", rpy)\n\n    tree = ET.ElementTree(mujoco_struct)\n    ET.indent(tree, space=\"  \", level=0)\n    tree.write(mjcf_path, encoding=\"utf-8\", xml_declaration=True)\n    logger.info(f\"Successfully converted {urdf_path} \u2192 {mjcf_path}\")\n\n    return mjcf_path\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.URDFtoUSDConverter","title":"URDFtoUSDConverter","text":"<pre><code>URDFtoUSDConverter(fix_base: bool = False, merge_fixed_joints: bool = False, make_instanceable: bool = True, force_usd_conversion: bool = True, collision_from_visuals: bool = True, joint_drive=None, rotate_wxyz: tuple[float] | None = None, simulation_app=None, **kwargs)\n</code></pre> <p>               Bases: <code>MeshtoUSDConverter</code></p> <p>Converts URDF files to USD format.</p> <p>Parameters:</p> Name Type Description Default <code>fix_base</code> <code>bool</code> <p>Fix the base link.</p> <code>False</code> <code>merge_fixed_joints</code> <code>bool</code> <p>Merge fixed joints.</p> <code>False</code> <code>make_instanceable</code> <code>bool</code> <p>Make prims instanceable.</p> <code>True</code> <code>force_usd_conversion</code> <code>bool</code> <p>Force conversion to USD.</p> <code>True</code> <code>collision_from_visuals</code> <code>bool</code> <p>Generate collisions from visuals.</p> <code>True</code> <code>joint_drive</code> <code>optional</code> <p>Joint drive configuration.</p> <code>None</code> <code>rotate_wxyz</code> <code>tuple[float]</code> <p>Quaternion for rotation.</p> <code>None</code> <code>simulation_app</code> <code>optional</code> <p>Simulation app instance.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Initializes the converter.</p> <p>Parameters:</p> Name Type Description Default <code>fix_base</code> <code>bool</code> <p>Fix the base link.</p> <code>False</code> <code>merge_fixed_joints</code> <code>bool</code> <p>Merge fixed joints.</p> <code>False</code> <code>make_instanceable</code> <code>bool</code> <p>Make prims instanceable.</p> <code>True</code> <code>force_usd_conversion</code> <code>bool</code> <p>Force conversion to USD.</p> <code>True</code> <code>collision_from_visuals</code> <code>bool</code> <p>Generate collisions from visuals.</p> <code>True</code> <code>joint_drive</code> <code>optional</code> <p>Joint drive configuration.</p> <code>None</code> <code>rotate_wxyz</code> <code>tuple[float]</code> <p>Quaternion for rotation.</p> <code>None</code> <code>simulation_app</code> <code>optional</code> <p>Simulation app instance.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __init__(\n    self,\n    fix_base: bool = False,\n    merge_fixed_joints: bool = False,\n    make_instanceable: bool = True,\n    force_usd_conversion: bool = True,\n    collision_from_visuals: bool = True,\n    joint_drive=None,\n    rotate_wxyz: tuple[float] | None = None,\n    simulation_app=None,\n    **kwargs,\n):\n    \"\"\"Initializes the converter.\n\n    Args:\n        fix_base (bool, optional): Fix the base link.\n        merge_fixed_joints (bool, optional): Merge fixed joints.\n        make_instanceable (bool, optional): Make prims instanceable.\n        force_usd_conversion (bool, optional): Force conversion to USD.\n        collision_from_visuals (bool, optional): Generate collisions from visuals.\n        joint_drive (optional): Joint drive configuration.\n        rotate_wxyz (tuple[float], optional): Quaternion for rotation.\n        simulation_app (optional): Simulation app instance.\n        **kwargs: Additional arguments.\n    \"\"\"\n    self.usd_parms = dict(\n        fix_base=fix_base,\n        merge_fixed_joints=merge_fixed_joints,\n        make_instanceable=make_instanceable,\n        force_usd_conversion=force_usd_conversion,\n        collision_from_visuals=collision_from_visuals,\n        joint_drive=joint_drive,\n        **kwargs,\n    )\n    self.rotate_wxyz = rotate_wxyz\n    if simulation_app is not None:\n        self.simulation_app = simulation_app\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.URDFtoUSDConverter.convert","title":"convert","text":"<pre><code>convert(urdf_path: str, output_file: str)\n</code></pre> <p>Converts a URDF file to USD and post-processes collision meshes.</p> <p>Parameters:</p> Name Type Description Default <code>urdf_path</code> <code>str</code> <p>Path to URDF file.</p> required <code>output_file</code> <code>str</code> <p>Path to output USD file.</p> required Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def convert(self, urdf_path: str, output_file: str):\n    \"\"\"Converts a URDF file to USD and post-processes collision meshes.\n\n    Args:\n        urdf_path (str): Path to URDF file.\n        output_file (str): Path to output USD file.\n    \"\"\"\n    from isaaclab.sim.converters import UrdfConverter, UrdfConverterCfg\n    from pxr import Gf, PhysxSchema, Sdf, Usd, UsdGeom\n\n    cfg = UrdfConverterCfg(\n        asset_path=urdf_path,\n        usd_dir=os.path.abspath(os.path.dirname(output_file)),\n        usd_file_name=os.path.basename(output_file),\n        **self.usd_parms,\n    )\n\n    urdf_converter = UrdfConverter(cfg)\n    usd_path = urdf_converter.usd_path\n\n    stage = Usd.Stage.Open(usd_path)\n    layer = stage.GetRootLayer()\n    with Usd.EditContext(stage, layer):\n        for prim in stage.Traverse():\n            if prim.GetName() == \"collisions\":\n                approx_attr = prim.CreateAttribute(\n                    \"physics:approximation\", Sdf.ValueTypeNames.Token\n                )\n                approx_attr.Set(\"convexDecomposition\")\n\n                physx_conv_api = (\n                    PhysxSchema.PhysxConvexDecompositionCollisionAPI.Apply(\n                        prim\n                    )\n                )\n                physx_conv_api.GetMaxConvexHullsAttr().Set(32)\n                physx_conv_api.GetHullVertexLimitAttr().Set(16)\n                physx_conv_api.GetVoxelResolutionAttr().Set(10000)\n                physx_conv_api.GetShrinkWrapAttr().Set(True)\n\n                api_schemas = prim.GetMetadata(\"apiSchemas\")\n                if api_schemas is None:\n                    api_schemas = Sdf.TokenListOp()\n\n                api_list = list(api_schemas.GetAddedOrExplicitItems())\n                for api in self.DEFAULT_BIND_APIS:\n                    if api not in api_list:\n                        api_list.append(api)\n\n                api_schemas.appendedItems = api_list\n                prim.SetMetadata(\"apiSchemas\", api_schemas)\n\n    if self.rotate_wxyz is not None:\n        inner_prim = next(\n            p\n            for p in stage.GetDefaultPrim().GetChildren()\n            if p.IsA(UsdGeom.Xform)\n        )\n        xformable = UsdGeom.Xformable(inner_prim)\n        xformable.ClearXformOpOrder()\n        orient_op = xformable.AddOrientOp(UsdGeom.XformOp.PrecisionDouble)\n        orient_op.Set(Gf.Quatd(*self.rotate_wxyz))\n\n    layer.Save()\n    logger.info(f\"Successfully converted {urdf_path} \u2192 {usd_path}\")\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.asset_converter.cvt_embodiedgen_asset_to_anysim","title":"cvt_embodiedgen_asset_to_anysim","text":"<pre><code>cvt_embodiedgen_asset_to_anysim(urdf_files: list[str], target_dirs: list[str], target_type: AssetType, source_type: AssetType, overwrite: bool = False, **kwargs) -&gt; dict[str, str]\n</code></pre> <p>Convert URDF files generated by EmbodiedGen into formats required by simulators.</p> <p>Supported simulators include SAPIEN, Isaac Sim, MuJoCo, Isaac Gym, Genesis, and Pybullet. Converting to the <code>USD</code> format requires <code>isaacsim</code> to be installed.</p> Example <pre><code>from embodied_gen.data.asset_converter import cvt_embodiedgen_asset_to_anysim\nfrom embodied_gen.utils.enum import AssetType\n\ndst_asset_path = cvt_embodiedgen_asset_to_anysim(\n    urdf_files=[\n        \"path1_to_embodiedgen_asset/asset.urdf\",\n        \"path2_to_embodiedgen_asset/asset.urdf\",\n    ],\n    target_dirs=[\n        \"path1_to_target_dir/asset.usd\",\n        \"path2_to_target_dir/asset.usd\",\n    ],\n    target_type=AssetType.USD,\n    source_type=AssetType.MESH,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>urdf_files</code> <code>list[str]</code> <p>List of URDF file paths.</p> required <code>target_dirs</code> <code>list[str]</code> <p>List of target directories.</p> required <code>target_type</code> <code>AssetType</code> <p>Target asset type.</p> required <code>source_type</code> <code>AssetType</code> <p>Source asset type.</p> required <code>overwrite</code> <code>bool</code> <p>Overwrite existing files.</p> <code>False</code> <code>**kwargs</code> <p>Additional converter arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: Mapping from URDF file to converted asset file.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def cvt_embodiedgen_asset_to_anysim(\n    urdf_files: list[str],\n    target_dirs: list[str],\n    target_type: AssetType,\n    source_type: AssetType,\n    overwrite: bool = False,\n    **kwargs,\n) -&gt; dict[str, str]:\n    \"\"\"Convert URDF files generated by EmbodiedGen into formats required by simulators.\n\n    Supported simulators include SAPIEN, Isaac Sim, MuJoCo, Isaac Gym, Genesis, and Pybullet.\n    Converting to the `USD` format requires `isaacsim` to be installed.\n\n    Example:\n        ```py\n        from embodied_gen.data.asset_converter import cvt_embodiedgen_asset_to_anysim\n        from embodied_gen.utils.enum import AssetType\n\n        dst_asset_path = cvt_embodiedgen_asset_to_anysim(\n            urdf_files=[\n                \"path1_to_embodiedgen_asset/asset.urdf\",\n                \"path2_to_embodiedgen_asset/asset.urdf\",\n            ],\n            target_dirs=[\n                \"path1_to_target_dir/asset.usd\",\n                \"path2_to_target_dir/asset.usd\",\n            ],\n            target_type=AssetType.USD,\n            source_type=AssetType.MESH,\n        )\n        ```\n\n    Args:\n        urdf_files (list[str]): List of URDF file paths.\n        target_dirs (list[str]): List of target directories.\n        target_type (AssetType): Target asset type.\n        source_type (AssetType): Source asset type.\n        overwrite (bool, optional): Overwrite existing files.\n        **kwargs: Additional converter arguments.\n\n    Returns:\n        dict[str, str]: Mapping from URDF file to converted asset file.\n    \"\"\"\n\n    if isinstance(urdf_files, str):\n        urdf_files = [urdf_files]\n    if isinstance(target_dirs, str):\n        urdf_files = [target_dirs]\n\n    # If the target type is URDF, no conversion is needed.\n    if target_type == AssetType.URDF:\n        return {key: key for key in urdf_files}\n\n    asset_converter = AssetConverterFactory.create(\n        target_type=target_type,\n        source_type=source_type,\n        **kwargs,\n    )\n    asset_paths = dict()\n\n    with asset_converter:\n        for urdf_file, target_dir in zip(urdf_files, target_dirs):\n            filename = os.path.basename(urdf_file).replace(\".urdf\", \"\")\n            if target_type == AssetType.MJCF:\n                target_file = f\"{target_dir}/{filename}.xml\"\n            elif target_type == AssetType.USD:\n                target_file = f\"{target_dir}/{filename}.usd\"\n            else:\n                raise NotImplementedError(\n                    f\"Target type {target_type} not supported.\"\n                )\n            if not os.path.exists(target_file) or overwrite:\n                asset_converter.convert(urdf_file, target_file)\n\n            asset_paths[urdf_file] = target_file\n\n    return asset_paths\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.datasets","title":"embodied_gen.data.datasets","text":""},{"location":"api/data.html#embodied_gen.data.datasets.PanoGSplatDataset","title":"PanoGSplatDataset","text":"<pre><code>PanoGSplatDataset(data_dir: str, split: str = Literal['train', 'eval'], data_name: str = 'gs_data.pt', max_sample_num: int = None)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for loading panorama-based 3D Gaussian Splatting data.</p> <p>This dataset is designed to be compatible with train and eval pipelines that use COLMAP-style camera conventions.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Root directory where the dataset file is located.</p> required <code>split</code> <code>str</code> <p>Dataset split to use, either \"train\" or \"eval\".</p> <code>Literal['train', 'eval']</code> <code>data_name</code> <code>str</code> <p>Name of the dataset file (default: \"gs_data.pt\").</p> <code>'gs_data.pt'</code> <code>max_sample_num</code> <code>int</code> <p>Maximum number of samples to load. If None, all available samples in the split will be used.</p> <code>None</code> Source code in <code>embodied_gen/data/datasets.py</code> <pre><code>def __init__(\n    self,\n    data_dir: str,\n    split: str = Literal[\"train\", \"eval\"],\n    data_name: str = \"gs_data.pt\",\n    max_sample_num: int = None,\n) -&gt; None:\n    self.data_path = os.path.join(data_dir, data_name)\n    self.split = split\n    self.max_sample_num = max_sample_num\n    if not os.path.exists(self.data_path):\n        raise FileNotFoundError(\n            f\"Dataset file {self.data_path} not found. Please provide the correct path.\"\n        )\n    self.data = torch.load(self.data_path, weights_only=False)\n    self.frames = self.data[split]\n    if max_sample_num is not None:\n        self.frames = self.frames[:max_sample_num]\n    self.points = self.data.get(\"points\", None)\n    self.points_rgb = self.data.get(\"points_rgb\", None)\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.differentiable_render","title":"embodied_gen.data.differentiable_render","text":""},{"location":"api/data.html#embodied_gen.data.differentiable_render.ImageRender","title":"ImageRender","text":"<pre><code>ImageRender(render_items: list[RenderItems], camera_params: CameraSetting, recompute_vtx_normal: bool = True, with_mtl: bool = False, gen_color_gif: bool = False, gen_color_mp4: bool = False, gen_viewnormal_mp4: bool = False, gen_glonormal_mp4: bool = False, no_index_file: bool = False, light_factor: float = 1.0)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Differentiable mesh renderer supporting multi-view rendering.</p> <p>This class wraps differentiable rasterization using <code>nvdiffrast</code> to render mesh geometry to various maps (normal, depth, alpha, albedo, etc.) and supports saving images and videos.</p> <p>Parameters:</p> Name Type Description Default <code>render_items</code> <code>list[RenderItems]</code> <p>List of rendering targets.</p> required <code>camera_params</code> <code>CameraSetting</code> <p>Camera parameters for rendering.</p> required <code>recompute_vtx_normal</code> <code>bool</code> <p>Recompute vertex normals. Defaults to True.</p> <code>True</code> <code>with_mtl</code> <code>bool</code> <p>Load mesh material files. Defaults to False.</p> <code>False</code> <code>gen_color_gif</code> <code>bool</code> <p>Generate GIF of color images. Defaults to False.</p> <code>False</code> <code>gen_color_mp4</code> <code>bool</code> <p>Generate MP4 of color images. Defaults to False.</p> <code>False</code> <code>gen_viewnormal_mp4</code> <code>bool</code> <p>Generate MP4 of view-space normals. Defaults to False.</p> <code>False</code> <code>gen_glonormal_mp4</code> <code>bool</code> <p>Generate MP4 of global-space normals. Defaults to False.</p> <code>False</code> <code>no_index_file</code> <code>bool</code> <p>Skip saving index file. Defaults to False.</p> <code>False</code> <code>light_factor</code> <code>float</code> <p>PBR light intensity multiplier. Defaults to 1.0.</p> <code>1.0</code> Example <pre><code>from embodied_gen.data.differentiable_render import ImageRender\nfrom embodied_gen.data.utils import CameraSetting\nfrom embodied_gen.utils.enum import RenderItems\n\ncamera_params = CameraSetting(\n    num_images=6,\n    elevation=[20, -10],\n    distance=5,\n    resolution_hw=(512,512),\n    fov=math.radians(30),\n    device='cuda',\n)\nrender_items = [RenderItems.IMAGE.value, RenderItems.DEPTH.value]\nrenderer = ImageRender(\n    render_items,\n    camera_params,\n    with_mtl=args.with_mtl,\n    gen_color_mp4=True,\n)\nrenderer.render_mesh(mesh_path='mesh.obj', output_root='./renders')\n</code></pre> Source code in <code>embodied_gen/data/differentiable_render.py</code> <pre><code>def __init__(\n    self,\n    render_items: list[RenderItems],\n    camera_params: CameraSetting,\n    recompute_vtx_normal: bool = True,\n    with_mtl: bool = False,\n    gen_color_gif: bool = False,\n    gen_color_mp4: bool = False,\n    gen_viewnormal_mp4: bool = False,\n    gen_glonormal_mp4: bool = False,\n    no_index_file: bool = False,\n    light_factor: float = 1.0,\n) -&gt; None:\n    camera = init_kal_camera(camera_params)\n    self.camera = camera\n\n    # Setup MVP matrix and renderer.\n    mv = camera.view_matrix()  # (n 4 4) world2cam\n    p = camera.intrinsics.projection_matrix()\n    # NOTE: add a negative sign at P[0, 2] as the y axis is flipped in `nvdiffrast` output.  # noqa\n    p[:, 1, 1] = -p[:, 1, 1]\n    # mvp = torch.bmm(p, mv) # camera.view_projection_matrix()\n    self.mv = mv\n    self.p = p\n\n    renderer = DiffrastRender(\n        p_matrix=p,\n        mv_matrix=mv,\n        resolution_hw=camera_params.resolution_hw,\n        context=dr.RasterizeCudaContext(),\n        mask_thresh=0.5,\n        grad_db=False,\n        device=camera_params.device,\n        antialias_mask=True,\n    )\n    self.renderer = renderer\n    self.recompute_vtx_normal = recompute_vtx_normal\n    self.render_items = render_items\n    self.device = camera_params.device\n    self.with_mtl = with_mtl\n    self.gen_color_gif = gen_color_gif\n    self.gen_color_mp4 = gen_color_mp4\n    self.gen_viewnormal_mp4 = gen_viewnormal_mp4\n    self.gen_glonormal_mp4 = gen_glonormal_mp4\n    self.light_factor = light_factor\n    self.no_index_file = no_index_file\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.differentiable_render.ImageRender.__call__","title":"__call__","text":"<pre><code>__call__(mesh_path: str, output_dir: str, prompt: str = None) -&gt; dict[str, str]\n</code></pre> <p>Renders a single mesh and returns output paths.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_path</code> <code>str</code> <p>Path to mesh file.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save outputs.</p> required <code>prompt</code> <code>str</code> <p>Caption prompt for MP4 metadata.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: Mapping of render types to saved image paths.</p> Source code in <code>embodied_gen/data/differentiable_render.py</code> <pre><code>def __call__(\n    self, mesh_path: str, output_dir: str, prompt: str = None\n) -&gt; dict[str, str]:\n    \"\"\"Renders a single mesh and returns output paths.\n\n    Args:\n        mesh_path (str): Path to mesh file.\n        output_dir (str): Directory to save outputs.\n        prompt (str, optional): Caption prompt for MP4 metadata.\n\n    Returns:\n        dict[str, str]: Mapping of render types to saved image paths.\n    \"\"\"\n    try:\n        mesh = import_kaolin_mesh(mesh_path, self.with_mtl)\n    except Exception as e:\n        logger.error(f\"[ERROR MESH LOAD]: {e}, skip {mesh_path}\")\n        return\n\n    mesh.vertices, scale, center = normalize_vertices_array(mesh.vertices)\n    if self.recompute_vtx_normal:\n        mesh.vertex_normals = calc_vertex_normals(\n            mesh.vertices, mesh.faces\n        )\n\n    mesh = mesh.to(self.device)\n    vertices, faces, vertex_normals = (\n        mesh.vertices,\n        mesh.faces,\n        mesh.vertex_normals,\n    )\n\n    # Perform rendering.\n    data_dict = defaultdict(list)\n    if RenderItems.ALPHA.value in self.render_items:\n        masks, _ = self.renderer.render_rast_alpha(vertices, faces)\n        render_paths = save_images(\n            masks, f\"{output_dir}/{RenderItems.ALPHA}\"\n        )\n        data_dict[RenderItems.ALPHA.value] = render_paths\n\n    if RenderItems.GLOBAL_NORMAL.value in self.render_items:\n        rendered_normals, masks = self.renderer.render_global_normal(\n            vertices, faces, vertex_normals\n        )\n        if self.gen_glonormal_mp4:\n            if isinstance(rendered_normals, torch.Tensor):\n                rendered_normals = rendered_normals.detach().cpu().numpy()\n            create_mp4_from_images(\n                rendered_normals,\n                output_path=f\"{output_dir}/normal.mp4\",\n                fps=15,\n                prompt=prompt,\n            )\n        else:\n            render_paths = save_images(\n                rendered_normals,\n                f\"{output_dir}/{RenderItems.GLOBAL_NORMAL}\",\n                cvt_color=cv2.COLOR_BGR2RGB,\n            )\n            data_dict[RenderItems.GLOBAL_NORMAL.value] = render_paths\n\n        if RenderItems.VIEW_NORMAL.value in self.render_items:\n            assert (\n                RenderItems.GLOBAL_NORMAL in self.render_items\n            ), f\"Must render global normal firstly, got render_items: {self.render_items}.\"  # noqa\n            rendered_view_normals = self.renderer.transform_normal(\n                rendered_normals, self.mv, masks, to_view=True\n            )\n\n            if self.gen_viewnormal_mp4:\n                create_mp4_from_images(\n                    rendered_view_normals,\n                    output_path=f\"{output_dir}/view_normal.mp4\",\n                    fps=15,\n                    prompt=prompt,\n                )\n            else:\n                render_paths = save_images(\n                    rendered_view_normals,\n                    f\"{output_dir}/{RenderItems.VIEW_NORMAL}\",\n                    cvt_color=cv2.COLOR_BGR2RGB,\n                )\n                data_dict[RenderItems.VIEW_NORMAL.value] = render_paths\n\n    if RenderItems.POSITION_MAP.value in self.render_items:\n        rendered_position, masks = self.renderer.render_position(\n            vertices, faces\n        )\n        norm_position = self.renderer.normalize_map_by_mask(\n            rendered_position, masks\n        )\n        render_paths = save_images(\n            norm_position,\n            f\"{output_dir}/{RenderItems.POSITION_MAP}\",\n            cvt_color=cv2.COLOR_BGR2RGB,\n        )\n        data_dict[RenderItems.POSITION_MAP.value] = render_paths\n\n    if RenderItems.DEPTH.value in self.render_items:\n        rendered_depth, masks = self.renderer.render_depth(vertices, faces)\n        norm_depth = self.renderer.normalize_map_by_mask(\n            rendered_depth, masks\n        )\n        render_paths = save_images(\n            norm_depth,\n            f\"{output_dir}/{RenderItems.DEPTH}\",\n        )\n        data_dict[RenderItems.DEPTH.value] = render_paths\n\n        render_paths = save_images(\n            rendered_depth,\n            f\"{output_dir}/{RenderItems.DEPTH}_exr\",\n            to_uint8=False,\n            format=\".exr\",\n        )\n        data_dict[f\"{RenderItems.DEPTH.value}_exr\"] = render_paths\n\n    if RenderItems.IMAGE.value in self.render_items:\n        images = []\n        albedos = []\n        diffuses = []\n        masks, _ = self.renderer.render_rast_alpha(vertices, faces)\n        try:\n            for idx, cam in enumerate(self.camera):\n                image, albedo, diffuse, _ = render_pbr(\n                    mesh, cam, light_factor=self.light_factor\n                )\n                image = torch.cat([image[0], masks[idx]], axis=-1)\n                images.append(image.detach().cpu().numpy())\n\n                if RenderItems.ALBEDO.value in self.render_items:\n                    albedo = torch.cat([albedo[0], masks[idx]], axis=-1)\n                    albedos.append(albedo.detach().cpu().numpy())\n\n                if RenderItems.DIFFUSE.value in self.render_items:\n                    diffuse = torch.cat([diffuse[0], masks[idx]], axis=-1)\n                    diffuses.append(diffuse.detach().cpu().numpy())\n\n        except Exception as e:\n            logger.error(f\"[ERROR pbr render]: {e}, skip {mesh_path}\")\n            return\n\n        if self.gen_color_gif:\n            create_gif_from_images(\n                images,\n                output_path=f\"{output_dir}/color.gif\",\n                fps=15,\n            )\n\n        if self.gen_color_mp4:\n            create_mp4_from_images(\n                images,\n                output_path=f\"{output_dir}/color.mp4\",\n                fps=15,\n                prompt=prompt,\n            )\n\n        if self.gen_color_mp4 or self.gen_color_gif:\n            return data_dict\n\n        render_paths = save_images(\n            images,\n            f\"{output_dir}/{RenderItems.IMAGE}\",\n            cvt_color=cv2.COLOR_BGRA2RGBA,\n        )\n        data_dict[RenderItems.IMAGE.value] = render_paths\n\n        render_paths = save_images(\n            albedos,\n            f\"{output_dir}/{RenderItems.ALBEDO}\",\n            cvt_color=cv2.COLOR_BGRA2RGBA,\n        )\n        data_dict[RenderItems.ALBEDO.value] = render_paths\n\n        render_paths = save_images(\n            diffuses,\n            f\"{output_dir}/{RenderItems.DIFFUSE}\",\n            cvt_color=cv2.COLOR_BGRA2RGBA,\n        )\n        data_dict[RenderItems.DIFFUSE.value] = render_paths\n\n    data_dict[\"status\"] = \"success\"\n\n    logger.info(f\"Finish rendering in {output_dir}\")\n\n    return data_dict\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.differentiable_render.ImageRender.render_mesh","title":"render_mesh","text":"<pre><code>render_mesh(mesh_path: Union[str, List[str]], output_root: str, uuid: Union[str, List[str]] = None, prompts: List[str] = None) -&gt; None\n</code></pre> <p>Renders one or more meshes and saves outputs.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_path</code> <code>Union[str, List[str]]</code> <p>Path(s) to mesh files.</p> required <code>output_root</code> <code>str</code> <p>Directory to save outputs.</p> required <code>uuid</code> <code>Union[str, List[str]]</code> <p>Unique IDs for outputs.</p> <code>None</code> <code>prompts</code> <code>List[str]</code> <p>Text prompts for videos.</p> <code>None</code> Source code in <code>embodied_gen/data/differentiable_render.py</code> <pre><code>def render_mesh(\n    self,\n    mesh_path: Union[str, List[str]],\n    output_root: str,\n    uuid: Union[str, List[str]] = None,\n    prompts: List[str] = None,\n) -&gt; None:\n    \"\"\"Renders one or more meshes and saves outputs.\n\n    Args:\n        mesh_path (Union[str, List[str]]): Path(s) to mesh files.\n        output_root (str): Directory to save outputs.\n        uuid (Union[str, List[str]], optional): Unique IDs for outputs.\n        prompts (List[str], optional): Text prompts for videos.\n    \"\"\"\n    mesh_path = as_list(mesh_path)\n    if uuid is None:\n        uuid = [os.path.basename(p).split(\".\")[0] for p in mesh_path]\n    uuid = as_list(uuid)\n    assert len(mesh_path) == len(uuid)\n    os.makedirs(output_root, exist_ok=True)\n\n    meta_info = dict()\n    for idx, (path, uid) in tqdm(\n        enumerate(zip(mesh_path, uuid)), total=len(mesh_path)\n    ):\n        output_dir = os.path.join(output_root, uid)\n        os.makedirs(output_dir, exist_ok=True)\n        prompt = prompts[idx] if prompts else None\n        data_dict = self(path, output_dir, prompt)\n        meta_info[uid] = data_dict\n\n    if self.no_index_file:\n        return\n\n    index_file = os.path.join(output_root, \"index.json\")\n    with open(index_file, \"w\") as fout:\n        json.dump(meta_info, fout)\n\n    logger.info(f\"Rendering meta info logged in {index_file}\")\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.differentiable_render.create_gif_from_images","title":"create_gif_from_images","text":"<pre><code>create_gif_from_images(images: list[ndarray], output_path: str, fps: int = 10) -&gt; None\n</code></pre> <p>Creates a GIF animation from a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[ndarray]</code> <p>List of images as numpy arrays.</p> required <code>output_path</code> <code>str</code> <p>Path to save the GIF file.</p> required <code>fps</code> <code>int</code> <p>Frames per second. Defaults to 10.</p> <code>10</code> Source code in <code>embodied_gen/data/differentiable_render.py</code> <pre><code>def create_gif_from_images(\n    images: list[np.ndarray], output_path: str, fps: int = 10\n) -&gt; None:\n    \"\"\"Creates a GIF animation from a list of images.\n\n    Args:\n        images (list[np.ndarray]): List of images as numpy arrays.\n        output_path (str): Path to save the GIF file.\n        fps (int, optional): Frames per second. Defaults to 10.\n    \"\"\"\n    pil_images = []\n    for image in images:\n        image = image.clip(min=0, max=1)\n        image = (255.0 * image).astype(np.uint8)\n        image = Image.fromarray(image, mode=\"RGBA\")\n        pil_images.append(image.convert(\"RGB\"))\n\n    duration = 1000 // fps\n    pil_images[0].save(\n        output_path,\n        save_all=True,\n        append_images=pil_images[1:],\n        duration=duration,\n        loop=0,\n    )\n\n    logger.info(f\"GIF saved to {output_path}\")\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.differentiable_render.create_mp4_from_images","title":"create_mp4_from_images","text":"<pre><code>create_mp4_from_images(images: list[ndarray], output_path: str, fps: int = 10, prompt: str = None)\n</code></pre> <p>Creates an MP4 video from a list of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[ndarray]</code> <p>List of images as numpy arrays.</p> required <code>output_path</code> <code>str</code> <p>Path to save the MP4 file.</p> required <code>fps</code> <code>int</code> <p>Frames per second. Defaults to 10.</p> <code>10</code> <code>prompt</code> <code>str</code> <p>Optional text prompt overlay.</p> <code>None</code> Source code in <code>embodied_gen/data/differentiable_render.py</code> <pre><code>def create_mp4_from_images(\n    images: list[np.ndarray],\n    output_path: str,\n    fps: int = 10,\n    prompt: str = None,\n):\n    \"\"\"Creates an MP4 video from a list of images.\n\n    Args:\n        images (list[np.ndarray]): List of images as numpy arrays.\n        output_path (str): Path to save the MP4 file.\n        fps (int, optional): Frames per second. Defaults to 10.\n        prompt (str, optional): Optional text prompt overlay.\n    \"\"\"\n    font = cv2.FONT_HERSHEY_SIMPLEX\n    font_scale = 0.5\n    font_thickness = 1\n    color = (255, 255, 255)\n    position = (20, 25)\n\n    with imageio.get_writer(output_path, fps=fps) as writer:\n        for image in images:\n            image = image.clip(min=0, max=1)\n            image = (255.0 * image).astype(np.uint8)\n            image = image[..., :3]\n            if prompt is not None:\n                cv2.putText(\n                    image,\n                    prompt,\n                    position,\n                    font,\n                    font_scale,\n                    color,\n                    font_thickness,\n                )\n\n            writer.append_data(image)\n\n    logger.info(f\"MP4 video saved to {output_path}\")\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.mesh_operator","title":"embodied_gen.data.mesh_operator","text":""},{"location":"api/data.html#embodied_gen.data.mesh_operator.MeshFixer","title":"MeshFixer","text":"<pre><code>MeshFixer(vertices: Union[Tensor, ndarray], faces: Union[Tensor, ndarray], device: str = 'cuda')\n</code></pre> <p>               Bases: <code>object</code></p> <p>MeshFixer simplifies and repairs 3D triangle meshes by TSDF.</p> <p>Attributes:</p> Name Type Description <code>vertices</code> <code>Tensor</code> <p>A tensor of shape (V, 3) representing vertex positions.</p> <code>faces</code> <code>Tensor</code> <p>A tensor of shape (F, 3) representing face indices.</p> <code>device</code> <code>str</code> <p>Device to run computations on, typically \"cuda\" or \"cpu\".</p> <p>Main logic reference: https://github.com/microsoft/TRELLIS/blob/main/trellis/utils/postprocessing_utils.py#L22</p> Source code in <code>embodied_gen/data/mesh_operator.py</code> <pre><code>def __init__(\n    self,\n    vertices: Union[torch.Tensor, np.ndarray],\n    faces: Union[torch.Tensor, np.ndarray],\n    device: str = \"cuda\",\n) -&gt; None:\n    self.device = device\n    if isinstance(vertices, np.ndarray):\n        vertices = torch.tensor(vertices)\n    self.vertices = vertices\n\n    if isinstance(faces, np.ndarray):\n        faces = torch.tensor(faces)\n    self.faces = faces\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.mesh_operator.MeshFixer.__call__","title":"__call__","text":"<pre><code>__call__(filter_ratio: float, max_hole_size: float, resolution: int, num_views: int, norm_mesh_ratio: float = 1.0) -&gt; Tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Post-process the mesh by simplifying and filling holes.</p> <p>This method performs a two-step process: 1. Simplifies mesh by reducing faces using quadric edge decimation. 2. Fills holes by removing invisible faces, repairing small boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_ratio</code> <code>float</code> <p>Ratio of faces to simplify out. Must be in the range (0, 1).</p> required <code>max_hole_size</code> <code>float</code> <p>Maximum area of a hole to fill. Connected components of holes larger than this size will not be repaired.</p> required <code>resolution</code> <code>int</code> <p>Resolution of the rasterization buffer.</p> required <code>num_views</code> <code>int</code> <p>Number of viewpoints to sample for rasterization.</p> required <code>norm_mesh_ratio</code> <code>float</code> <p>A scaling factor applied to the vertices of the mesh during processing.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: - vertices: Simplified and repaired vertex array of (V, 3). - faces: Simplified and repaired face array of (F, 3).</p> Source code in <code>embodied_gen/data/mesh_operator.py</code> <pre><code>@spaces.GPU\ndef __call__(\n    self,\n    filter_ratio: float,\n    max_hole_size: float,\n    resolution: int,\n    num_views: int,\n    norm_mesh_ratio: float = 1.0,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Post-process the mesh by simplifying and filling holes.\n\n    This method performs a two-step process:\n    1. Simplifies mesh by reducing faces using quadric edge decimation.\n    2. Fills holes by removing invisible faces, repairing small boundaries.\n\n    Args:\n        filter_ratio (float): Ratio of faces to simplify out.\n            Must be in the range (0, 1).\n        max_hole_size (float): Maximum area of a hole to fill. Connected\n            components of holes larger than this size will not be repaired.\n        resolution (int): Resolution of the rasterization buffer.\n        num_views (int): Number of viewpoints to sample for rasterization.\n        norm_mesh_ratio (float, optional): A scaling factor applied to the\n            vertices of the mesh during processing.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]:\n            - vertices: Simplified and repaired vertex array of (V, 3).\n            - faces: Simplified and repaired face array of (F, 3).\n    \"\"\"\n    self.vertices = self.vertices.to(self.device)\n    self.faces = self.faces.to(self.device)\n\n    self.simplify(ratio=filter_ratio)\n    self.fill_holes(\n        max_hole_size=max_hole_size,\n        max_hole_nbe=int(250 * np.sqrt(1 - filter_ratio)),\n        resolution=resolution,\n        num_views=num_views,\n        norm_mesh_ratio=norm_mesh_ratio,\n    )\n\n    return self.vertices_np, self.faces_np\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.mesh_operator.MeshFixer.simplify","title":"simplify","text":"<pre><code>simplify(ratio: float) -&gt; None\n</code></pre> <p>Simplify the mesh using quadric edge collapse decimation.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Ratio of faces to filter out.</p> required Source code in <code>embodied_gen/data/mesh_operator.py</code> <pre><code>@log_mesh_changes\ndef simplify(self, ratio: float) -&gt; None:\n    \"\"\"Simplify the mesh using quadric edge collapse decimation.\n\n    Args:\n        ratio (float): Ratio of faces to filter out.\n    \"\"\"\n    if ratio &lt;= 0 or ratio &gt;= 1:\n        raise ValueError(\"Simplify ratio must be between 0 and 1.\")\n\n    # Convert to PyVista format for simplification\n    mesh = pv.PolyData(\n        self.vertices_np,\n        np.hstack([np.full((self.faces.shape[0], 1), 3), self.faces_np]),\n    )\n    mesh.clean(inplace=True)\n    mesh.clear_data()\n    mesh = mesh.triangulate()\n    mesh = mesh.decimate(ratio, progress_bar=True)\n\n    # Update vertices and faces\n    self.vertices = torch.tensor(\n        mesh.points, device=self.device, dtype=torch.float32\n    )\n    self.faces = torch.tensor(\n        mesh.faces.reshape(-1, 4)[:, 1:],\n        device=self.device,\n        dtype=torch.int32,\n    )\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2","title":"embodied_gen.data.backproject_v2","text":""},{"location":"api/data.html#embodied_gen.data.backproject_v2.TextureBacker","title":"TextureBacker","text":"<pre><code>TextureBacker(camera_params: CameraSetting, view_weights: list[float], render_wh: tuple[int, int] = (2048, 2048), texture_wh: tuple[int, int] = (2048, 2048), bake_angle_thresh: int = 75, mask_thresh: float = 0.5, smooth_texture: bool = True, inpaint_smooth: bool = False, mesh_post_process: bool = True)\n</code></pre> <p>Texture baking pipeline for multi-view projection and fusion.</p> <p>This class generates UV-based textures for a 3D mesh using multi-view images, depth, and normal information. It includes mesh normalization, UV unwrapping, visibility-aware back-projection, confidence-weighted fusion, and inpainting.</p> <p>Parameters:</p> Name Type Description Default <code>camera_params</code> <code>CameraSetting</code> <p>Camera intrinsics and extrinsics.</p> required <code>view_weights</code> <code>list[float]</code> <p>Weights for each view in texture fusion.</p> required <code>render_wh</code> <code>tuple[int, int]</code> <p>Intermediate rendering resolution.</p> <code>(2048, 2048)</code> <code>texture_wh</code> <code>tuple[int, int]</code> <p>Output texture resolution.</p> <code>(2048, 2048)</code> <code>bake_angle_thresh</code> <code>int</code> <p>Max angle for valid projection.</p> <code>75</code> <code>mask_thresh</code> <code>float</code> <p>Threshold for visibility masks.</p> <code>0.5</code> <code>smooth_texture</code> <code>bool</code> <p>Apply post-processing to texture.</p> <code>True</code> <code>inpaint_smooth</code> <code>bool</code> <p>Apply inpainting smoothing.</p> <code>False</code> <code>mesh_post_process</code> <code>bool</code> <p>False for preventing modification of vertices.</p> <code>True</code> Example <pre><code>from embodied_gen.data.backproject_v2 import TextureBacker\nfrom embodied_gen.data.utils import CameraSetting\nimport trimesh\nfrom PIL import Image\n\ncamera_params = CameraSetting(\n    num_images=6,\n    elevation=[20, -10],\n    distance=5,\n    resolution_hw=(2048,2048),\n    fov=math.radians(30),\n    device='cuda',\n)\nview_weights = [1, 0.1, 0.02, 0.1, 1, 0.02]\nmesh = trimesh.load('mesh.obj')\nimages = [Image.open(f'view_{i}.png') for i in range(6)]\ntexture_backer = TextureBacker(camera_params, view_weights)\ntextured_mesh = texture_backer(images, mesh, 'output.obj')\n</code></pre> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def __init__(\n    self,\n    camera_params: CameraSetting,\n    view_weights: list[float],\n    render_wh: tuple[int, int] = (2048, 2048),\n    texture_wh: tuple[int, int] = (2048, 2048),\n    bake_angle_thresh: int = 75,\n    mask_thresh: float = 0.5,\n    smooth_texture: bool = True,\n    inpaint_smooth: bool = False,\n    mesh_post_process: bool = True,\n) -&gt; None:\n    self.camera_params = camera_params\n    self.renderer = None\n    self.view_weights = view_weights\n    self.device = camera_params.device\n    self.render_wh = render_wh\n    self.texture_wh = texture_wh\n    self.mask_thresh = mask_thresh\n    self.smooth_texture = smooth_texture\n    self.inpaint_smooth = inpaint_smooth\n    self.mesh_post_process = mesh_post_process\n\n    self.bake_angle_thresh = bake_angle_thresh\n    self.bake_unreliable_kernel_size = int(\n        (2 / 512) * max(self.render_wh[0], self.render_wh[1])\n    )\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.TextureBacker.__call__","title":"__call__","text":"<pre><code>__call__(colors: list[Image], mesh: Trimesh, output_path: str) -&gt; trimesh.Trimesh\n</code></pre> <p>Runs the texture baking and exports the textured mesh.</p> <p>Parameters:</p> Name Type Description Default <code>colors</code> <code>list[Image]</code> <p>List of input view images.</p> required <code>mesh</code> <code>Trimesh</code> <p>Input mesh to be textured.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output textured mesh.</p> required <p>Returns:</p> Type Description <code>Trimesh</code> <p>trimesh.Trimesh: The textured mesh with UV and texture image.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def __call__(\n    self,\n    colors: list[Image.Image],\n    mesh: trimesh.Trimesh,\n    output_path: str,\n) -&gt; trimesh.Trimesh:\n    \"\"\"Runs the texture baking and exports the textured mesh.\n\n    Args:\n        colors (list[Image.Image]): List of input view images.\n        mesh (trimesh.Trimesh): Input mesh to be textured.\n        output_path (str): Path to save the output textured mesh.\n\n    Returns:\n        trimesh.Trimesh: The textured mesh with UV and texture image.\n    \"\"\"\n    mesh = self.load_mesh(mesh)\n    texture_np, mask_np = self.compute_texture(colors, mesh)\n\n    texture_np = self.uv_inpaint(mesh, texture_np, mask_np)\n    if self.smooth_texture:\n        texture_np = post_process_texture(texture_np)\n\n    vertices, faces, uv_map = self.get_mesh_np_attrs(\n        mesh, self.scale, self.center\n    )\n    textured_mesh = save_mesh_with_mtl(\n        vertices,\n        faces,\n        uv_map,\n        texture_np,\n        output_path,\n        mesh_process=self.mesh_post_process,\n    )\n\n    return textured_mesh\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.TextureBacker.back_project","title":"back_project","text":"<pre><code>back_project(image, vis_mask, depth, normal, uv) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Back-projects image and confidence to UV texture space.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image or ndarray</code> <p>Input image.</p> required <code>vis_mask</code> <code>Tensor</code> <p>Visibility mask.</p> required <code>depth</code> <code>Tensor</code> <p>Depth map.</p> required <code>normal</code> <code>Tensor</code> <p>Normal map.</p> required <code>uv</code> <code>Tensor</code> <p>UV coordinates.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor]: Texture and confidence map.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def back_project(\n    self, image, vis_mask, depth, normal, uv\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Back-projects image and confidence to UV texture space.\n\n    Args:\n        image (PIL.Image or np.ndarray): Input image.\n        vis_mask (torch.Tensor): Visibility mask.\n        depth (torch.Tensor): Depth map.\n        normal (torch.Tensor): Normal map.\n        uv (torch.Tensor): UV coordinates.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: Texture and confidence map.\n    \"\"\"\n    image = np.array(image)\n    image = torch.as_tensor(image, device=self.device, dtype=torch.float32)\n    if image.ndim == 2:\n        image = image.unsqueeze(-1)\n    image = image / 255\n\n    depth_inv = (1.0 - depth) * vis_mask\n    sketch_image = self._render_depth_edges(depth_inv)\n\n    cos = F.cosine_similarity(\n        torch.tensor([[0, 0, 1]], device=self.device),\n        normal.view(-1, 3),\n    ).view_as(normal[..., :1])\n    cos[cos &lt; np.cos(np.radians(self.bake_angle_thresh))] = 0\n\n    k = self.bake_unreliable_kernel_size * 2 + 1\n    kernel = torch.ones((1, 1, k, k), device=self.device)\n\n    vis_mask = vis_mask.permute(2, 0, 1).unsqueeze(0).float()\n    vis_mask = F.conv2d(\n        1.0 - vis_mask,\n        kernel,\n        padding=k // 2,\n    )\n    vis_mask = 1.0 - (vis_mask &gt; 0).float()\n    vis_mask = vis_mask.squeeze(0).permute(1, 2, 0)\n\n    sketch_image = sketch_image.permute(2, 0, 1).unsqueeze(0)\n    sketch_image = F.conv2d(sketch_image, kernel, padding=k // 2)\n    sketch_image = (sketch_image &gt; 0).float()\n    sketch_image = sketch_image.squeeze(0).permute(1, 2, 0)\n    vis_mask = vis_mask * (sketch_image &lt; 0.5)\n\n    cos[vis_mask == 0] = 0\n    valid_pixels = (vis_mask != 0).view(-1)\n\n    return (\n        self._scatter_texture(uv, image, valid_pixels),\n        self._scatter_texture(uv, cos, valid_pixels),\n    )\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.TextureBacker.compute_enhanced_viewnormal","title":"compute_enhanced_viewnormal","text":"<pre><code>compute_enhanced_viewnormal(mv_mtx: Tensor, vertices: Tensor, faces: Tensor) -&gt; torch.Tensor\n</code></pre> <p>Computes enhanced view normals for mesh faces.</p> <p>Parameters:</p> Name Type Description Default <code>mv_mtx</code> <code>Tensor</code> <p>View matrices.</p> required <code>vertices</code> <code>Tensor</code> <p>Mesh vertices.</p> required <code>faces</code> <code>Tensor</code> <p>Mesh faces.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: View normals.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def compute_enhanced_viewnormal(\n    self, mv_mtx: torch.Tensor, vertices: torch.Tensor, faces: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Computes enhanced view normals for mesh faces.\n\n    Args:\n        mv_mtx (torch.Tensor): View matrices.\n        vertices (torch.Tensor): Mesh vertices.\n        faces (torch.Tensor): Mesh faces.\n\n    Returns:\n        torch.Tensor: View normals.\n    \"\"\"\n    rast, _ = self.renderer.compute_dr_raster(vertices, faces)\n    rendered_view_normals = []\n    for idx in range(len(mv_mtx)):\n        pos_cam = _transform_vertices(mv_mtx[idx], vertices, keepdim=True)\n        pos_cam = pos_cam[:, :3] / pos_cam[:, 3:]\n        v0, v1, v2 = (pos_cam[faces[:, i]] for i in range(3))\n        face_norm = F.normalize(\n            torch.cross(v1 - v0, v2 - v0, dim=-1), dim=-1\n        )\n        vertex_norm = (\n            torch.from_numpy(\n                trimesh.geometry.mean_vertex_normals(\n                    len(pos_cam), faces.cpu(), face_norm.cpu()\n                )\n            )\n            .to(vertices.device)\n            .contiguous()\n        )\n        im_base_normals, _ = dr.interpolate(\n            vertex_norm[None, ...].float(),\n            rast[idx : idx + 1],\n            faces.to(torch.int32),\n        )\n        rendered_view_normals.append(im_base_normals)\n\n    rendered_view_normals = torch.cat(rendered_view_normals, dim=0)\n\n    return rendered_view_normals\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.TextureBacker.compute_texture","title":"compute_texture","text":"<pre><code>compute_texture(colors: list[Image], mesh: Trimesh) -&gt; trimesh.Trimesh\n</code></pre> <p>Computes the fused texture for the mesh from multi-view images.</p> <p>Parameters:</p> Name Type Description Default <code>colors</code> <code>list[Image]</code> <p>List of view images.</p> required <code>mesh</code> <code>Trimesh</code> <p>Mesh to texture.</p> required <p>Returns:</p> Type Description <code>Trimesh</code> <p>tuple[np.ndarray, np.ndarray]: Texture and mask.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>@spaces.GPU\ndef compute_texture(\n    self,\n    colors: list[Image.Image],\n    mesh: trimesh.Trimesh,\n) -&gt; trimesh.Trimesh:\n    \"\"\"Computes the fused texture for the mesh from multi-view images.\n\n    Args:\n        colors (list[Image.Image]): List of view images.\n        mesh (trimesh.Trimesh): Mesh to texture.\n\n    Returns:\n        tuple[np.ndarray, np.ndarray]: Texture and mask.\n    \"\"\"\n    self._lazy_init_render(self.camera_params, self.mask_thresh)\n\n    vertices = torch.from_numpy(mesh.vertices).to(self.device).float()\n    faces = torch.from_numpy(mesh.faces).to(self.device).to(torch.int)\n    uv_map = torch.from_numpy(mesh.visual.uv).to(self.device).float()\n\n    rendered_depth, masks = self.renderer.render_depth(vertices, faces)\n    norm_deps = self.renderer.normalize_map_by_mask(rendered_depth, masks)\n    render_uvs, _ = self.renderer.render_uv(vertices, faces, uv_map)\n    view_normals = self.compute_enhanced_viewnormal(\n        self.renderer.mv_mtx, vertices, faces\n    )\n\n    textures, weighted_cos_maps = [], []\n    for color, mask, dep, normal, uv, weight in zip(\n        colors,\n        masks,\n        norm_deps,\n        view_normals,\n        render_uvs,\n        self.view_weights,\n    ):\n        texture, cos_map = self.back_project(color, mask, dep, normal, uv)\n        textures.append(texture)\n        weighted_cos_maps.append(weight * (cos_map**4))\n\n    texture, mask = self.fast_bake_texture(textures, weighted_cos_maps)\n\n    texture_np = texture.cpu().numpy()\n    mask_np = (mask.squeeze(-1).cpu().numpy() * 255).astype(np.uint8)\n\n    return texture_np, mask_np\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.TextureBacker.fast_bake_texture","title":"fast_bake_texture","text":"<pre><code>fast_bake_texture(textures: list[Tensor], confidence_maps: list[Tensor]) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Fuses multiple textures and confidence maps.</p> <p>Parameters:</p> Name Type Description Default <code>textures</code> <code>list[Tensor]</code> <p>List of textures.</p> required <code>confidence_maps</code> <code>list[Tensor]</code> <p>List of confidence maps.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor]: Fused texture and mask.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>@torch.no_grad()\ndef fast_bake_texture(\n    self, textures: list[torch.Tensor], confidence_maps: list[torch.Tensor]\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Fuses multiple textures and confidence maps.\n\n    Args:\n        textures (list[torch.Tensor]): List of textures.\n        confidence_maps (list[torch.Tensor]): List of confidence maps.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: Fused texture and mask.\n    \"\"\"\n    channel = textures[0].shape[-1]\n    texture_merge = torch.zeros(self.texture_wh + [channel]).to(\n        self.device\n    )\n    trust_map_merge = torch.zeros(self.texture_wh + [1]).to(self.device)\n    for texture, cos_map in zip(textures, confidence_maps):\n        view_sum = (cos_map &gt; 0).sum()\n        painted_sum = ((cos_map &gt; 0) * (trust_map_merge &gt; 0)).sum()\n        if painted_sum / view_sum &gt; 0.99:\n            continue\n        texture_merge += texture * cos_map\n        trust_map_merge += cos_map\n    texture_merge = texture_merge / torch.clamp(trust_map_merge, min=1e-8)\n\n    return texture_merge, trust_map_merge &gt; 1e-8\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.TextureBacker.get_mesh_np_attrs","title":"get_mesh_np_attrs","text":"<pre><code>get_mesh_np_attrs(mesh: Trimesh, scale: float = None, center: ndarray = None) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]\n</code></pre> <p>Gets mesh attributes as numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Trimesh</code> <p>Input mesh.</p> required <code>scale</code> <code>float</code> <p>Scale factor.</p> <code>None</code> <code>center</code> <code>ndarray</code> <p>Center offset.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <code>tuple[ndarray, ndarray, ndarray]</code> <p>(vertices, faces, uv_map)</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def get_mesh_np_attrs(\n    self,\n    mesh: trimesh.Trimesh,\n    scale: float = None,\n    center: np.ndarray = None,\n) -&gt; tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"Gets mesh attributes as numpy arrays.\n\n    Args:\n        mesh (trimesh.Trimesh): Input mesh.\n        scale (float, optional): Scale factor.\n        center (np.ndarray, optional): Center offset.\n\n    Returns:\n        tuple: (vertices, faces, uv_map)\n    \"\"\"\n    vertices = mesh.vertices.copy()\n    faces = mesh.faces.copy()\n    uv_map = mesh.visual.uv.copy()\n    uv_map[:, 1] = 1.0 - uv_map[:, 1]\n\n    if scale is not None:\n        vertices = vertices / scale\n    if center is not None:\n        vertices = vertices + center\n\n    return vertices, faces, uv_map\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.TextureBacker.load_mesh","title":"load_mesh","text":"<pre><code>load_mesh(mesh: Trimesh) -&gt; trimesh.Trimesh\n</code></pre> <p>Normalizes mesh and unwraps UVs.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Trimesh</code> <p>Input mesh.</p> required <p>Returns:</p> Type Description <code>Trimesh</code> <p>trimesh.Trimesh: Mesh with normalized vertices and UVs.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def load_mesh(self, mesh: trimesh.Trimesh) -&gt; trimesh.Trimesh:\n    \"\"\"Normalizes mesh and unwraps UVs.\n\n    Args:\n        mesh (trimesh.Trimesh): Input mesh.\n\n    Returns:\n        trimesh.Trimesh: Mesh with normalized vertices and UVs.\n    \"\"\"\n    mesh.vertices, scale, center = normalize_vertices_array(mesh.vertices)\n    self.scale, self.center = scale, center\n\n    vmapping, indices, uvs = xatlas.parametrize(mesh.vertices, mesh.faces)\n    uvs[:, 1] = 1 - uvs[:, 1]\n    mesh.vertices = mesh.vertices[vmapping]\n    mesh.faces = indices\n    mesh.visual.uv = uvs\n\n    return mesh\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.TextureBacker.uv_inpaint","title":"uv_inpaint","text":"<pre><code>uv_inpaint(mesh: Trimesh, texture: ndarray, mask: ndarray) -&gt; np.ndarray\n</code></pre> <p>Inpaints missing regions in the UV texture.</p> <p>Parameters:</p> Name Type Description Default <code>mesh</code> <code>Trimesh</code> <p>Mesh.</p> required <code>texture</code> <code>ndarray</code> <p>Texture image.</p> required <code>mask</code> <code>ndarray</code> <p>Mask image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Inpainted texture.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def uv_inpaint(\n    self, mesh: trimesh.Trimesh, texture: np.ndarray, mask: np.ndarray\n) -&gt; np.ndarray:\n    \"\"\"Inpaints missing regions in the UV texture.\n\n    Args:\n        mesh (trimesh.Trimesh): Mesh.\n        texture (np.ndarray): Texture image.\n        mask (np.ndarray): Mask image.\n\n    Returns:\n        np.ndarray: Inpainted texture.\n    \"\"\"\n    if self.inpaint_smooth:\n        vertices, faces, uv_map = self.get_mesh_np_attrs(mesh)\n        texture, mask = _texture_inpaint_smooth(\n            texture, mask, vertices, faces, uv_map\n        )\n\n    texture = texture.clip(0, 1)\n    texture = cv2.inpaint(\n        (texture * 255).astype(np.uint8),\n        255 - mask,\n        3,\n        cv2.INPAINT_NS,\n    )\n\n    return texture\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.entrypoint","title":"entrypoint","text":"<pre><code>entrypoint(delight_model: DelightingModel = None, imagesr_model: ImageRealESRGAN = None, **kwargs) -&gt; trimesh.Trimesh\n</code></pre> <p>Entrypoint for texture backprojection from multi-view images.</p> <p>Parameters:</p> Name Type Description Default <code>delight_model</code> <code>DelightingModel</code> <p>Delighting model.</p> <code>None</code> <code>imagesr_model</code> <code>ImageRealESRGAN</code> <p>Super-resolution model.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments to override CLI.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Trimesh</code> <p>trimesh.Trimesh: Textured mesh.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def entrypoint(\n    delight_model: DelightingModel = None,\n    imagesr_model: ImageRealESRGAN = None,\n    **kwargs,\n) -&gt; trimesh.Trimesh:\n    \"\"\"Entrypoint for texture backprojection from multi-view images.\n\n    Args:\n        delight_model (DelightingModel, optional): Delighting model.\n        imagesr_model (ImageRealESRGAN, optional): Super-resolution model.\n        **kwargs: Additional arguments to override CLI.\n\n    Returns:\n        trimesh.Trimesh: Textured mesh.\n    \"\"\"\n    args = parse_args()\n    for k, v in kwargs.items():\n        if hasattr(args, k) and v is not None:\n            setattr(args, k, v)\n\n    # Setup camera parameters.\n    camera_params = CameraSetting(\n        num_images=args.num_images,\n        elevation=args.elevation,\n        distance=args.distance,\n        resolution_hw=args.resolution_hw,\n        fov=math.radians(args.fov),\n        device=args.device,\n    )\n\n    args.color_path = as_list(args.color_path)\n    if args.delight and delight_model is None:\n        delight_model = DelightingModel()\n\n    color_grid = [Image.open(color_path) for color_path in args.color_path]\n    color_grid = vcat_pil_images(color_grid, image_mode=\"RGBA\")\n    if args.delight:\n        color_grid = delight_model(color_grid)\n        if not args.no_save_delight_img:\n            save_dir = os.path.dirname(args.output_path)\n            os.makedirs(save_dir, exist_ok=True)\n            color_grid.save(f\"{save_dir}/color_delight.png\")\n\n    multiviews = get_images_from_grid(color_grid, img_size=512)\n    view_weights = [1, 0.1, 0.02, 0.1, 1, 0.02]\n    view_weights += [0.01] * (len(multiviews) - len(view_weights))\n\n    # Use RealESRGAN_x4plus for x4 (512-&gt;2048) image super resolution.\n    if imagesr_model is None:\n        imagesr_model = ImageRealESRGAN(outscale=4)\n    multiviews = [imagesr_model(img) for img in multiviews]\n    multiviews = [img.convert(\"RGB\") for img in multiviews]\n    mesh = trimesh.load(args.mesh_path)\n    if isinstance(mesh, trimesh.Scene):\n        mesh = mesh.dump(concatenate=True)\n\n    if not args.skip_fix_mesh:\n        mesh.vertices, scale, center = normalize_vertices_array(mesh.vertices)\n        mesh_fixer = MeshFixer(mesh.vertices, mesh.faces, args.device)\n        mesh.vertices, mesh.faces = mesh_fixer(\n            filter_ratio=args.mesh_sipmlify_ratio,\n            max_hole_size=0.04,\n            resolution=1024,\n            num_views=1000,\n            norm_mesh_ratio=0.5,\n        )\n        if len(mesh.faces) &gt; args.n_max_faces:\n            mesh.vertices, mesh.faces = mesh_fixer(\n                filter_ratio=0.8,\n                max_hole_size=0.04,\n                resolution=1024,\n                num_views=1000,\n                norm_mesh_ratio=0.5,\n            )\n        # Restore scale.\n        mesh.vertices = mesh.vertices / scale\n        mesh.vertices = mesh.vertices + center\n\n    # Baking texture to mesh.\n    texture_backer = TextureBacker(\n        camera_params=camera_params,\n        view_weights=view_weights,\n        render_wh=args.resolution_hw,\n        texture_wh=args.texture_wh,\n        smooth_texture=not args.no_smooth_texture,\n        mesh_post_process=not args.no_mesh_post_process,\n    )\n\n    textured_mesh = texture_backer(multiviews, mesh, args.output_path)\n\n    if args.save_glb_path is not None:\n        os.makedirs(os.path.dirname(args.save_glb_path), exist_ok=True)\n        textured_mesh.export(args.save_glb_path)\n\n    return textured_mesh\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.backproject_v2.parse_args","title":"parse_args","text":"<pre><code>parse_args()\n</code></pre> <p>Parses command-line arguments for texture backprojection.</p> <p>Returns:</p> Type Description <p>argparse.Namespace: Parsed arguments.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def parse_args():\n    \"\"\"Parses command-line arguments for texture backprojection.\n\n    Returns:\n        argparse.Namespace: Parsed arguments.\n    \"\"\"\n    parser = argparse.ArgumentParser(description=\"Backproject texture\")\n    parser.add_argument(\n        \"--color_path\",\n        nargs=\"+\",\n        type=str,\n        help=\"Multiview color image in grid file paths\",\n    )\n    parser.add_argument(\n        \"--mesh_path\",\n        type=str,\n        help=\"Mesh path, .obj, .glb or .ply\",\n    )\n    parser.add_argument(\n        \"--output_path\",\n        type=str,\n        help=\"Output mesh path with suffix\",\n    )\n    parser.add_argument(\n        \"--num_images\", type=int, default=6, help=\"Number of images to render.\"\n    )\n    parser.add_argument(\n        \"--elevation\",\n        nargs=\"+\",\n        type=float,\n        default=[20.0, -10.0],\n        help=\"Elevation angles for the camera (default: [20.0, -10.0])\",\n    )\n    parser.add_argument(\n        \"--distance\",\n        type=float,\n        default=5,\n        help=\"Camera distance (default: 5)\",\n    )\n    parser.add_argument(\n        \"--resolution_hw\",\n        type=int,\n        nargs=2,\n        default=(2048, 2048),\n        help=\"Resolution of the output images (default: (2048, 2048))\",\n    )\n    parser.add_argument(\n        \"--fov\",\n        type=float,\n        default=30,\n        help=\"Field of view in degrees (default: 30)\",\n    )\n    parser.add_argument(\n        \"--device\",\n        type=str,\n        choices=[\"cpu\", \"cuda\"],\n        default=\"cuda\",\n        help=\"Device to run on (default: `cuda`)\",\n    )\n    parser.add_argument(\n        \"--skip_fix_mesh\", action=\"store_true\", help=\"Fix mesh geometry.\"\n    )\n    parser.add_argument(\n        \"--texture_wh\",\n        nargs=2,\n        type=int,\n        default=[2048, 2048],\n        help=\"Texture resolution width and height\",\n    )\n    parser.add_argument(\n        \"--mesh_sipmlify_ratio\",\n        type=float,\n        default=0.9,\n        help=\"Mesh simplification ratio (default: 0.9)\",\n    )\n    parser.add_argument(\n        \"--delight\", action=\"store_true\", help=\"Use delighting model.\"\n    )\n    parser.add_argument(\n        \"--no_smooth_texture\",\n        action=\"store_true\",\n        help=\"Do not smooth the texture.\",\n    )\n    parser.add_argument(\n        \"--save_glb_path\", type=str, default=None, help=\"Save glb path.\"\n    )\n    parser.add_argument(\n        \"--no_save_delight_img\",\n        action=\"store_true\",\n        help=\"Disable saving delight image\",\n    )\n    parser.add_argument(\"--n_max_faces\", type=int, default=30000)\n    parser.add_argument(\"--no_mesh_post_process\", action=\"store_true\")\n    args, unknown = parser.parse_known_args()\n\n    return args\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.convex_decomposer","title":"embodied_gen.data.convex_decomposer","text":""},{"location":"api/data.html#embodied_gen.data.convex_decomposer.decompose_convex_coacd","title":"decompose_convex_coacd","text":"<pre><code>decompose_convex_coacd(filename: str, outfile: str, params: dict, verbose: bool = False, auto_scale: bool = True, scale_factor: float = 1.0) -&gt; None\n</code></pre> <p>Decomposes a mesh using CoACD and saves the result.</p> <p>This function loads a mesh from a file, runs the CoACD algorithm with the given parameters, optionally scales the resulting convex hulls to match the original mesh's bounding box, and exports the combined result to a file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the input mesh file.</p> required <code>outfile</code> <code>str</code> <p>Path to save the decomposed output mesh.</p> required <code>params</code> <code>dict</code> <p>A dictionary of parameters for the CoACD algorithm.</p> required <code>verbose</code> <code>bool</code> <p>If True, sets the CoACD log level to 'info'.</p> <code>False</code> <code>auto_scale</code> <code>bool</code> <p>If True, automatically computes a scale factor to match the decomposed mesh's bounding box to the visual mesh's bounding box.</p> <code>True</code> <code>scale_factor</code> <code>float</code> <p>An additional scaling factor applied to the vertices of the decomposed mesh parts.</p> <code>1.0</code> Source code in <code>embodied_gen/data/convex_decomposer.py</code> <pre><code>def decompose_convex_coacd(\n    filename: str,\n    outfile: str,\n    params: dict,\n    verbose: bool = False,\n    auto_scale: bool = True,\n    scale_factor: float = 1.0,\n) -&gt; None:\n    \"\"\"Decomposes a mesh using CoACD and saves the result.\n\n    This function loads a mesh from a file, runs the CoACD algorithm with the\n    given parameters, optionally scales the resulting convex hulls to match the\n    original mesh's bounding box, and exports the combined result to a file.\n\n    Args:\n        filename: Path to the input mesh file.\n        outfile: Path to save the decomposed output mesh.\n        params: A dictionary of parameters for the CoACD algorithm.\n        verbose: If True, sets the CoACD log level to 'info'.\n        auto_scale: If True, automatically computes a scale factor to match the\n            decomposed mesh's bounding box to the visual mesh's bounding box.\n        scale_factor: An additional scaling factor applied to the vertices of\n            the decomposed mesh parts.\n    \"\"\"\n    coacd.set_log_level(\"info\" if verbose else \"warn\")\n\n    mesh = trimesh.load(filename, force=\"mesh\")\n    mesh = coacd.Mesh(mesh.vertices, mesh.faces)\n\n    result = coacd.run_coacd(mesh, **params)\n\n    meshes = []\n    for v, f in result:\n        meshes.append(trimesh.Trimesh(v, f))\n\n    # Compute collision_scale because convex decomposition usually makes the mesh larger.\n    if auto_scale:\n        all_mesh = sum([trimesh.Trimesh(*m) for m in result])\n        convex_mesh_shape = np.ptp(all_mesh.vertices, axis=0)\n        visual_mesh_shape = np.ptp(mesh.vertices, axis=0)\n        scale_factor *= visual_mesh_shape / convex_mesh_shape\n\n    combined = trimesh.Scene()\n    for mesh_part in meshes:\n        mesh_part.vertices *= scale_factor\n        combined.add_geometry(mesh_part)\n\n    combined.export(outfile)\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.convex_decomposer.decompose_convex_mesh","title":"decompose_convex_mesh","text":"<pre><code>decompose_convex_mesh(filename: str, outfile: str, threshold: float = 0.05, max_convex_hull: int = -1, preprocess_mode: str = 'auto', preprocess_resolution: int = 30, resolution: int = 2000, mcts_nodes: int = 20, mcts_iterations: int = 150, mcts_max_depth: int = 3, pca: bool = False, merge: bool = True, seed: int = 0, auto_scale: bool = True, scale_factor: float = 1.005, verbose: bool = False) -&gt; str\n</code></pre> <p>Decomposes a mesh into convex parts with retry logic.</p> <p>This function serves as a wrapper for <code>decompose_convex_coacd</code>, providing explicit parameters for the CoACD algorithm and implementing a retry mechanism. If the initial decomposition fails, it attempts again with <code>preprocess_mode</code> set to 'on'.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the input mesh file.</p> required <code>outfile</code> <code>str</code> <p>Path to save the decomposed output mesh.</p> required <code>threshold</code> <code>float</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>0.05</code> <code>max_convex_hull</code> <code>int</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>-1</code> <code>preprocess_mode</code> <code>str</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>'auto'</code> <code>preprocess_resolution</code> <code>int</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>30</code> <code>resolution</code> <code>int</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>2000</code> <code>mcts_nodes</code> <code>int</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>20</code> <code>mcts_iterations</code> <code>int</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>150</code> <code>mcts_max_depth</code> <code>int</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>3</code> <code>pca</code> <code>bool</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>False</code> <code>merge</code> <code>bool</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>True</code> <code>seed</code> <code>int</code> <p>CoACD parameter. See CoACD documentation for details.</p> <code>0</code> <code>auto_scale</code> <code>bool</code> <p>If True, automatically scale the output to match the input bounding box.</p> <code>True</code> <code>scale_factor</code> <code>float</code> <p>Additional scaling factor to apply.</p> <code>1.005</code> <code>verbose</code> <code>bool</code> <p>If True, enables detailed logging.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the output file if decomposition is successful.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If convex decomposition fails after all attempts.</p> Source code in <code>embodied_gen/data/convex_decomposer.py</code> <pre><code>def decompose_convex_mesh(\n    filename: str,\n    outfile: str,\n    threshold: float = 0.05,\n    max_convex_hull: int = -1,\n    preprocess_mode: str = \"auto\",\n    preprocess_resolution: int = 30,\n    resolution: int = 2000,\n    mcts_nodes: int = 20,\n    mcts_iterations: int = 150,\n    mcts_max_depth: int = 3,\n    pca: bool = False,\n    merge: bool = True,\n    seed: int = 0,\n    auto_scale: bool = True,\n    scale_factor: float = 1.005,\n    verbose: bool = False,\n) -&gt; str:\n    \"\"\"Decomposes a mesh into convex parts with retry logic.\n\n    This function serves as a wrapper for `decompose_convex_coacd`, providing\n    explicit parameters for the CoACD algorithm and implementing a retry\n    mechanism. If the initial decomposition fails, it attempts again with\n    `preprocess_mode` set to 'on'.\n\n    Args:\n        filename: Path to the input mesh file.\n        outfile: Path to save the decomposed output mesh.\n        threshold: CoACD parameter. See CoACD documentation for details.\n        max_convex_hull: CoACD parameter. See CoACD documentation for details.\n        preprocess_mode: CoACD parameter. See CoACD documentation for details.\n        preprocess_resolution: CoACD parameter. See CoACD documentation for details.\n        resolution: CoACD parameter. See CoACD documentation for details.\n        mcts_nodes: CoACD parameter. See CoACD documentation for details.\n        mcts_iterations: CoACD parameter. See CoACD documentation for details.\n        mcts_max_depth: CoACD parameter. See CoACD documentation for details.\n        pca: CoACD parameter. See CoACD documentation for details.\n        merge: CoACD parameter. See CoACD documentation for details.\n        seed: CoACD parameter. See CoACD documentation for details.\n        auto_scale: If True, automatically scale the output to match the input\n            bounding box.\n        scale_factor: Additional scaling factor to apply.\n        verbose: If True, enables detailed logging.\n\n    Returns:\n        The path to the output file if decomposition is successful.\n\n    Raises:\n        RuntimeError: If convex decomposition fails after all attempts.\n    \"\"\"\n    coacd.set_log_level(\"info\" if verbose else \"warn\")\n\n    if os.path.exists(outfile):\n        logger.warning(f\"Output file {outfile} already exists, removing it.\")\n        os.remove(outfile)\n\n    params = dict(\n        threshold=threshold,\n        max_convex_hull=max_convex_hull,\n        preprocess_mode=preprocess_mode,\n        preprocess_resolution=preprocess_resolution,\n        resolution=resolution,\n        mcts_nodes=mcts_nodes,\n        mcts_iterations=mcts_iterations,\n        mcts_max_depth=mcts_max_depth,\n        pca=pca,\n        merge=merge,\n        seed=seed,\n    )\n\n    try:\n        decompose_convex_coacd(\n            filename, outfile, params, verbose, auto_scale, scale_factor\n        )\n        if os.path.exists(outfile):\n            return outfile\n    except Exception as e:\n        if verbose:\n            print(f\"Decompose convex first attempt failed: {e}.\")\n\n    if preprocess_mode != \"on\":\n        try:\n            params[\"preprocess_mode\"] = \"on\"\n            decompose_convex_coacd(\n                filename, outfile, params, verbose, auto_scale, scale_factor\n            )\n            if os.path.exists(outfile):\n                return outfile\n        except Exception as e:\n            if verbose:\n                print(\n                    f\"Decompose convex second attempt with preprocess_mode='on' failed: {e}\"\n                )\n\n    raise RuntimeError(f\"Convex decomposition failed on {filename}\")\n</code></pre>"},{"location":"api/data.html#embodied_gen.data.convex_decomposer.decompose_convex_mp","title":"decompose_convex_mp","text":"<pre><code>decompose_convex_mp(filename: str, outfile: str, threshold: float = 0.05, max_convex_hull: int = -1, preprocess_mode: str = 'auto', preprocess_resolution: int = 30, resolution: int = 2000, mcts_nodes: int = 20, mcts_iterations: int = 150, mcts_max_depth: int = 3, pca: bool = False, merge: bool = True, seed: int = 0, verbose: bool = False, auto_scale: bool = True) -&gt; str\n</code></pre> <p>Decomposes a mesh into convex parts in a separate process.</p> <p>This function uses the <code>multiprocessing</code> module to run the CoACD algorithm in a spawned subprocess. This is useful for isolating the decomposition process to prevent potential memory leaks or crashes in the main process. It includes a retry mechanism similar to <code>decompose_convex_mesh</code>.</p> <p>See https://simulately.wiki/docs/toolkits/ConvexDecomp for details.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Path to the input mesh file.</p> required <code>outfile</code> <code>str</code> <p>Path to save the decomposed output mesh.</p> required <code>threshold</code> <code>float</code> <p>CoACD parameter.</p> <code>0.05</code> <code>max_convex_hull</code> <code>int</code> <p>CoACD parameter.</p> <code>-1</code> <code>preprocess_mode</code> <code>str</code> <p>CoACD parameter.</p> <code>'auto'</code> <code>preprocess_resolution</code> <code>int</code> <p>CoACD parameter.</p> <code>30</code> <code>resolution</code> <code>int</code> <p>CoACD parameter.</p> <code>2000</code> <code>mcts_nodes</code> <code>int</code> <p>CoACD parameter.</p> <code>20</code> <code>mcts_iterations</code> <code>int</code> <p>CoACD parameter.</p> <code>150</code> <code>mcts_max_depth</code> <code>int</code> <p>CoACD parameter.</p> <code>3</code> <code>pca</code> <code>bool</code> <p>CoACD parameter.</p> <code>False</code> <code>merge</code> <code>bool</code> <p>CoACD parameter.</p> <code>True</code> <code>seed</code> <code>int</code> <p>CoACD parameter.</p> <code>0</code> <code>verbose</code> <code>bool</code> <p>If True, enables detailed logging in the subprocess.</p> <code>False</code> <code>auto_scale</code> <code>bool</code> <p>If True, automatically scale the output.</p> <code>True</code> <p>Returns:</p> Type Description <code>str</code> <p>The path to the output file if decomposition is successful.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If convex decomposition fails after all attempts.</p> Source code in <code>embodied_gen/data/convex_decomposer.py</code> <pre><code>def decompose_convex_mp(\n    filename: str,\n    outfile: str,\n    threshold: float = 0.05,\n    max_convex_hull: int = -1,\n    preprocess_mode: str = \"auto\",\n    preprocess_resolution: int = 30,\n    resolution: int = 2000,\n    mcts_nodes: int = 20,\n    mcts_iterations: int = 150,\n    mcts_max_depth: int = 3,\n    pca: bool = False,\n    merge: bool = True,\n    seed: int = 0,\n    verbose: bool = False,\n    auto_scale: bool = True,\n) -&gt; str:\n    \"\"\"Decomposes a mesh into convex parts in a separate process.\n\n    This function uses the `multiprocessing` module to run the CoACD algorithm\n    in a spawned subprocess. This is useful for isolating the decomposition\n    process to prevent potential memory leaks or crashes in the main process.\n    It includes a retry mechanism similar to `decompose_convex_mesh`.\n\n    See https://simulately.wiki/docs/toolkits/ConvexDecomp for details.\n\n    Args:\n        filename: Path to the input mesh file.\n        outfile: Path to save the decomposed output mesh.\n        threshold: CoACD parameter.\n        max_convex_hull: CoACD parameter.\n        preprocess_mode: CoACD parameter.\n        preprocess_resolution: CoACD parameter.\n        resolution: CoACD parameter.\n        mcts_nodes: CoACD parameter.\n        mcts_iterations: CoACD parameter.\n        mcts_max_depth: CoACD parameter.\n        pca: CoACD parameter.\n        merge: CoACD parameter.\n        seed: CoACD parameter.\n        verbose: If True, enables detailed logging in the subprocess.\n        auto_scale: If True, automatically scale the output.\n\n    Returns:\n        The path to the output file if decomposition is successful.\n\n    Raises:\n        RuntimeError: If convex decomposition fails after all attempts.\n    \"\"\"\n    params = dict(\n        threshold=threshold,\n        max_convex_hull=max_convex_hull,\n        preprocess_mode=preprocess_mode,\n        preprocess_resolution=preprocess_resolution,\n        resolution=resolution,\n        mcts_nodes=mcts_nodes,\n        mcts_iterations=mcts_iterations,\n        mcts_max_depth=mcts_max_depth,\n        pca=pca,\n        merge=merge,\n        seed=seed,\n    )\n\n    ctx = mp.get_context(\"spawn\")\n    p = ctx.Process(\n        target=decompose_convex_coacd,\n        args=(filename, outfile, params, verbose, auto_scale),\n    )\n    p.start()\n    p.join()\n    if p.exitcode == 0 and os.path.exists(outfile):\n        return outfile\n\n    if preprocess_mode != \"on\":\n        params[\"preprocess_mode\"] = \"on\"\n        p = ctx.Process(\n            target=decompose_convex_coacd,\n            args=(filename, outfile, params, verbose, auto_scale),\n        )\n        p.start()\n        p.join()\n        if p.exitcode == 0 and os.path.exists(outfile):\n            return outfile\n\n    raise RuntimeError(f\"Convex decomposition failed on {filename}\")\n</code></pre>"},{"location":"api/envs.html","title":"Envs API","text":"<p>Documentation for simulation environments and task definitions.</p>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen","title":"embodied_gen.envs.pick_embodiedgen","text":""},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen","title":"PickEmbodiedGen","text":"<pre><code>PickEmbodiedGen(*args, robot_uids: str | list[str] = 'panda', robot_init_qpos_noise: float = 0.02, num_envs: int = 1, reconfiguration_freq: int = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseEnv</code></p> <p>PickEmbodiedGen as gym env example for object pick-and-place tasks.</p> <p>This environment simulates a robot interacting with 3D assets in the embodiedgen generated scene in SAPIEN. It supports multi-environment setups, dynamic reconfiguration, and hybrid rendering with 3D Gaussian Splatting.</p> Example <p>Use <code>gym.make</code> to create the <code>PickEmbodiedGen-v1</code> parallel environment. <pre><code>import gymnasium as gym\nenv = gym.make(\n    \"PickEmbodiedGen-v1\",\n    num_envs=cfg.num_envs,\n    render_mode=cfg.render_mode,\n    enable_shadow=cfg.enable_shadow,\n    layout_file=cfg.layout_file,\n    control_mode=cfg.control_mode,\n    camera_cfg=dict(\n        camera_eye=cfg.camera_eye,\n        camera_target_pt=cfg.camera_target_pt,\n        image_hw=cfg.image_hw,\n        fovy_deg=cfg.fovy_deg,\n    ),\n)\n</code></pre></p> <p>Initializes the PickEmbodiedGen environment.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <p>Variable length argument list for the base class.</p> <code>()</code> <code>robot_uids</code> <code>str | list[str]</code> <p>The robot(s) to use in the environment.</p> <code>'panda'</code> <code>robot_init_qpos_noise</code> <code>float</code> <p>Noise added to the robot's initial joint positions.</p> <code>0.02</code> <code>num_envs</code> <code>int</code> <p>The number of parallel environments to create.</p> <code>1</code> <code>reconfiguration_freq</code> <code>int</code> <p>How often to reconfigure the scene. If None, it is set based on num_envs.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for environment setup, including layout_file, replace_objs, enable_grasp, etc.</p> <code>{}</code> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    robot_uids: str | list[str] = \"panda\",\n    robot_init_qpos_noise: float = 0.02,\n    num_envs: int = 1,\n    reconfiguration_freq: int = None,\n    **kwargs,\n):\n    \"\"\"Initializes the PickEmbodiedGen environment.\n\n    Args:\n        *args: Variable length argument list for the base class.\n        robot_uids: The robot(s) to use in the environment.\n        robot_init_qpos_noise: Noise added to the robot's initial joint\n            positions.\n        num_envs: The number of parallel environments to create.\n        reconfiguration_freq: How often to reconfigure the scene. If None,\n            it is set based on num_envs.\n        **kwargs: Additional keyword arguments for environment setup,\n            including layout_file, replace_objs, enable_grasp, etc.\n    \"\"\"\n    self.robot_init_qpos_noise = robot_init_qpos_noise\n    if reconfiguration_freq is None:\n        if num_envs == 1:\n            reconfiguration_freq = 1\n        else:\n            reconfiguration_freq = 0\n\n    # Init params from kwargs.\n    layout_file = kwargs.pop(\"layout_file\", None)\n    replace_objs = kwargs.pop(\"replace_objs\", True)\n    self.enable_grasp = kwargs.pop(\"enable_grasp\", False)\n    self.init_3dgs_quat = kwargs.pop(\n        \"init_3dgs_quat\", [0.7071, 0, 0, 0.7071]\n    )\n    # Add small offset in z-axis to avoid collision.\n    self.objs_z_offset = kwargs.pop(\"objs_z_offset\", 0.002)\n    self.robot_z_offset = kwargs.pop(\"robot_z_offset\", 0.002)\n    self.camera_cfg = kwargs.pop(\"camera_cfg\", None)\n    if self.camera_cfg is None:\n        self.camera_cfg = dict(\n            camera_eye=[0.9, 0.0, 1.1],\n            camera_target_pt=[0.0, 0.0, 0.9],\n            image_hw=[256, 256],\n            fovy_deg=75,\n        )\n\n    self.layouts = self.init_env_layouts(\n        layout_file, num_envs, replace_objs\n    )\n    self.robot_pose = self.compute_robot_init_pose(\n        self.layouts, num_envs, self.robot_z_offset\n    )\n    self.env_actors = dict()\n    self.image_transform = transforms.PILToTensor()\n\n    super().__init__(\n        *args,\n        robot_uids=robot_uids,\n        reconfiguration_freq=reconfiguration_freq,\n        num_envs=num_envs,\n        **kwargs,\n    )\n\n    self.bg_images = dict()\n    if self.render_mode == \"hybrid\":\n        self.bg_images = self.render_gs3d_images(\n            self.layouts, num_envs, self.init_3dgs_quat\n        )\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.compute_dense_reward","title":"compute_dense_reward","text":"<pre><code>compute_dense_reward(obs: any, action: Tensor, info: dict)\n</code></pre> <p>Computes a dense reward for the current step.</p> <p>The reward is a composite of reaching, grasping, placing, and maintaining a static final pose.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>any</code> <p>The current observation.</p> required <code>action</code> <code>Tensor</code> <p>The action taken in the current step.</p> required <code>info</code> <code>dict</code> <p>A dictionary containing evaluation information from <code>evaluate()</code>.</p> required <p>Returns:</p> Type Description <p>A tensor containing the dense reward for each environment.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>def compute_dense_reward(self, obs: any, action: torch.Tensor, info: dict):\n    \"\"\"Computes a dense reward for the current step.\n\n    The reward is a composite of reaching, grasping, placing, and\n    maintaining a static final pose.\n\n    Args:\n        obs: The current observation.\n        action: The action taken in the current step.\n        info: A dictionary containing evaluation information from `evaluate()`.\n\n    Returns:\n        A tensor containing the dense reward for each environment.\n    \"\"\"\n    tcp_to_obj_dist = torch.linalg.norm(\n        self.obj.pose.p - self.agent.tcp.pose.p, axis=1\n    )\n    reaching_reward = 1 - torch.tanh(5 * tcp_to_obj_dist)\n    reward = reaching_reward\n\n    is_grasped = info[\"is_grasped\"]\n    reward += is_grasped\n\n    # obj_to_goal_dist = torch.linalg.norm(\n    #     self.goal_site.pose.p - self.obj.pose.p, axis=1\n    # )\n    obj_to_goal_dist = torch.linalg.norm(\n        self.obj.pose.p - self.obj.pose.p, axis=1\n    )\n    place_reward = 1 - torch.tanh(5 * obj_to_goal_dist)\n    reward += place_reward * is_grasped\n\n    reward += info[\"is_obj_placed\"] * is_grasped\n\n    static_reward = 1 - torch.tanh(\n        5\n        * torch.linalg.norm(self.agent.robot.get_qvel()[..., :-2], axis=1)\n    )\n    reward += static_reward * info[\"is_obj_placed\"] * is_grasped\n\n    reward[info[\"success\"]] = 6\n    return reward\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.compute_normalized_dense_reward","title":"compute_normalized_dense_reward","text":"<pre><code>compute_normalized_dense_reward(obs: any, action: Tensor, info: dict)\n</code></pre> <p>Computes a dense reward normalized to be between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>obs</code> <code>any</code> <p>The current observation.</p> required <code>action</code> <code>Tensor</code> <p>The action taken in the current step.</p> required <code>info</code> <code>dict</code> <p>A dictionary containing evaluation information from <code>evaluate()</code>.</p> required <p>Returns:</p> Type Description <p>A tensor containing the normalized dense reward for each environment.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>def compute_normalized_dense_reward(\n    self, obs: any, action: torch.Tensor, info: dict\n):\n    \"\"\"Computes a dense reward normalized to be between 0 and 1.\n\n    Args:\n        obs: The current observation.\n        action: The action taken in the current step.\n        info: A dictionary containing evaluation information from `evaluate()`.\n\n    Returns:\n        A tensor containing the normalized dense reward for each environment.\n    \"\"\"\n    return self.compute_dense_reward(obs=obs, action=action, info=info) / 6\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.compute_robot_init_pose","title":"compute_robot_init_pose  <code>staticmethod</code>","text":"<pre><code>compute_robot_init_pose(layouts: list[str], num_envs: int, z_offset: float = 0.0) -&gt; list[list[float]]\n</code></pre> <p>Computes the initial pose for the robot in each environment.</p> <p>Parameters:</p> Name Type Description Default <code>layouts</code> <code>list[str]</code> <p>A list of file paths to the environment layouts.</p> required <code>num_envs</code> <code>int</code> <p>The number of environments.</p> required <code>z_offset</code> <code>float</code> <p>An optional vertical offset to apply to the robot's position to prevent collisions.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>list[list[float]]</code> <p>A list of initial poses ([x, y, z, qw, qx, qy, qz]) for the robot</p> <code>list[list[float]]</code> <p>in each environment.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>@staticmethod\ndef compute_robot_init_pose(\n    layouts: list[str], num_envs: int, z_offset: float = 0.0\n) -&gt; list[list[float]]:\n    \"\"\"Computes the initial pose for the robot in each environment.\n\n    Args:\n        layouts: A list of file paths to the environment layouts.\n        num_envs: The number of environments.\n        z_offset: An optional vertical offset to apply to the robot's\n            position to prevent collisions.\n\n    Returns:\n        A list of initial poses ([x, y, z, qw, qx, qy, qz]) for the robot\n        in each environment.\n    \"\"\"\n    robot_pose = []\n    for env_idx in range(num_envs):\n        layout = json.load(open(layouts[env_idx], \"r\"))\n        layout = LayoutInfo.from_dict(layout)\n        robot_node = layout.relation[Scene3DItemEnum.ROBOT.value]\n        x, y, z, qx, qy, qz, qw = layout.position[robot_node]\n        robot_pose.append([x, y, z + z_offset, qw, qx, qy, qz])\n\n    return robot_pose\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.evaluate","title":"evaluate","text":"<pre><code>evaluate()\n</code></pre> <p>Evaluates the current state of the environment.</p> <p>Checks for task success criteria such as whether the object is grasped, placed at the goal, and if the robot is static.</p> <p>Returns:</p> Type Description <p>A dictionary containing boolean tensors for various success</p> <p>metrics, including 'is_grasped', 'is_obj_placed', and overall</p> <p>'success'.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>def evaluate(self):\n    \"\"\"Evaluates the current state of the environment.\n\n    Checks for task success criteria such as whether the object is grasped,\n    placed at the goal, and if the robot is static.\n\n    Returns:\n        A dictionary containing boolean tensors for various success\n        metrics, including 'is_grasped', 'is_obj_placed', and overall\n        'success'.\n    \"\"\"\n    obj_to_goal_pos = (\n        self.obj.pose.p\n    )  # self.goal_site.pose.p - self.obj.pose.p\n    is_obj_placed = (\n        torch.linalg.norm(obj_to_goal_pos, axis=1) &lt;= self.goal_thresh\n    )\n    is_grasped = self.agent.is_grasping(self.obj)\n    is_robot_static = self.agent.is_static(0.2)\n\n    return dict(\n        is_grasped=is_grasped,\n        obj_to_goal_pos=obj_to_goal_pos,\n        is_obj_placed=is_obj_placed,\n        is_robot_static=is_robot_static,\n        is_grasping=self.agent.is_grasping(self.obj),\n        success=torch.logical_and(is_obj_placed, is_robot_static),\n    )\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.hybrid_render","title":"hybrid_render","text":"<pre><code>hybrid_render()\n</code></pre> <p>Renders a hybrid image by blending simulated foreground with a background.</p> <p>The foreground is rendered with an alpha channel and then blended with the pre-rendered Gaussian Splatting background image.</p> <p>Returns:</p> Type Description <p>A torch tensor of the final blended RGB images.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>def hybrid_render(self):\n    \"\"\"Renders a hybrid image by blending simulated foreground with a background.\n\n    The foreground is rendered with an alpha channel and then blended with\n    the pre-rendered Gaussian Splatting background image.\n\n    Returns:\n        A torch tensor of the final blended RGB images.\n    \"\"\"\n    fg_images = self.render_rgb_array(\n        return_alpha=True\n    )  # (n_env, h, w, 3)\n    images = []\n    for key in self.bg_images:\n        if \"render_camera\" not in key:\n            continue\n        env_idx = int(key.split(\"-env\")[-1])\n        rgba = alpha_blend_rgba(\n            fg_images[env_idx].cpu().numpy(), self.bg_images[key]\n        )\n        images.append(self.image_transform(rgba))\n\n    images = torch.stack(images, dim=0)\n    images = images.permute(0, 2, 3, 1)\n\n    return images[..., :3]\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.init_env_layouts","title":"init_env_layouts  <code>staticmethod</code>","text":"<pre><code>init_env_layouts(layout_file: str, num_envs: int, replace_objs: bool) -&gt; list[LayoutInfo]\n</code></pre> <p>Initializes and saves layout files for each environment instance.</p> <p>For each environment, this method creates a layout configuration. If <code>replace_objs</code> is True, it generates new object placements for each subsequent environment. The generated layouts are saved as new JSON files.</p> <p>Parameters:</p> Name Type Description Default <code>layout_file</code> <code>str</code> <p>Path to the base layout JSON file.</p> required <code>num_envs</code> <code>int</code> <p>The number of environments to create layouts for.</p> required <code>replace_objs</code> <code>bool</code> <p>If True, generates new object placements for each environment after the first one using BFS placement.</p> required <p>Returns:</p> Type Description <code>list[LayoutInfo]</code> <p>A list of file paths to the generated layout for each environment.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>@staticmethod\ndef init_env_layouts(\n    layout_file: str, num_envs: int, replace_objs: bool\n) -&gt; list[LayoutInfo]:\n    \"\"\"Initializes and saves layout files for each environment instance.\n\n    For each environment, this method creates a layout configuration. If\n    `replace_objs` is True, it generates new object placements for each\n    subsequent environment. The generated layouts are saved as new JSON\n    files.\n\n    Args:\n        layout_file: Path to the base layout JSON file.\n        num_envs: The number of environments to create layouts for.\n        replace_objs: If True, generates new object placements for each\n            environment after the first one using BFS placement.\n\n    Returns:\n        A list of file paths to the generated layout for each environment.\n    \"\"\"\n    layouts = []\n    for env_idx in range(num_envs):\n        if replace_objs and env_idx &gt; 0:\n            layout_info = bfs_placement(layout_file)\n        else:\n            layout_info = json.load(open(layout_file, \"r\"))\n            layout_info = LayoutInfo.from_dict(layout_info)\n\n        layout_path = layout_file.replace(\".json\", f\"_env{env_idx}.json\")\n        with open(layout_path, \"w\") as f:\n            json.dump(layout_info.to_dict(), f, indent=4)\n\n        layouts.append(layout_path)\n\n    return layouts\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.render","title":"render","text":"<pre><code>render()\n</code></pre> <p>Renders the environment based on the configured render_mode.</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If <code>render_mode</code> is not set.</p> <code>NotImplementedError</code> <p>If the <code>render_mode</code> is not supported.</p> <p>Returns:</p> Type Description <p>The rendered output, which varies depending on the render mode.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>def render(self):\n    \"\"\"Renders the environment based on the configured render_mode.\n\n    Raises:\n        RuntimeError: If `render_mode` is not set.\n        NotImplementedError: If the `render_mode` is not supported.\n\n    Returns:\n        The rendered output, which varies depending on the render mode.\n    \"\"\"\n    if self.render_mode is None:\n        raise RuntimeError(\"render_mode is not set.\")\n    if self.render_mode == \"human\":\n        return self.render_human()\n    elif self.render_mode == \"rgb_array\":\n        res = self.render_rgb_array()\n        return res\n    elif self.render_mode == \"sensors\":\n        res = self.render_sensors()\n        return res\n    elif self.render_mode == \"all\":\n        return self.render_all()\n    elif self.render_mode == \"hybrid\":\n        return self.hybrid_render()\n    else:\n        raise NotImplementedError(\n            f\"Unsupported render mode {self.render_mode}.\"\n        )\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.render_gs3d_images","title":"render_gs3d_images","text":"<pre><code>render_gs3d_images(layouts: list[str], num_envs: int, init_quat: list[float]) -&gt; dict[str, np.ndarray]\n</code></pre> <p>Renders background images using a pre-trained Gaussian Splatting model.</p> <p>This method pre-renders the static background for each environment from the perspective of all cameras to be used for hybrid rendering.</p> <p>Parameters:</p> Name Type Description Default <code>layouts</code> <code>list[str]</code> <p>A list of file paths to the environment layouts.</p> required <code>num_envs</code> <code>int</code> <p>The number of environments.</p> required <code>init_quat</code> <code>list[float]</code> <p>An initial quaternion to orient the Gaussian Splatting model.</p> required <p>Returns:</p> Type Description <code>dict[str, ndarray]</code> <p>A dictionary mapping a unique key (e.g., 'camera-env_idx') to the</p> <code>dict[str, ndarray]</code> <p>rendered background image as a numpy array.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>def render_gs3d_images(\n    self, layouts: list[str], num_envs: int, init_quat: list[float]\n) -&gt; dict[str, np.ndarray]:\n    \"\"\"Renders background images using a pre-trained Gaussian Splatting model.\n\n    This method pre-renders the static background for each environment from\n    the perspective of all cameras to be used for hybrid rendering.\n\n    Args:\n        layouts: A list of file paths to the environment layouts.\n        num_envs: The number of environments.\n        init_quat: An initial quaternion to orient the Gaussian Splatting\n            model.\n\n    Returns:\n        A dictionary mapping a unique key (e.g., 'camera-env_idx') to the\n        rendered background image as a numpy array.\n    \"\"\"\n    sim_coord_align = (\n        torch.tensor(SIM_COORD_ALIGN).to(torch.float32).to(self.device)\n    )\n    cameras = self.scene.sensors.copy()\n    cameras.update(self.scene.human_render_cameras)\n\n    # Preload the background Gaussian Splatting model.\n    asset_root = os.path.dirname(layouts[0])\n    layout = LayoutInfo.from_dict(json.load(open(layouts[0], \"r\")))\n    bg_node = layout.relation[Scene3DItemEnum.BACKGROUND.value]\n    gs_path = os.path.join(\n        asset_root, layout.assets[bg_node], \"gs_model.ply\"\n    )\n    raw_gs: GaussianOperator = GaussianOperator.load_from_ply(gs_path)\n    bg_images = dict()\n    for env_idx in tqdm(range(num_envs), desc=\"Pre-rendering Background\"):\n        layout = json.load(open(layouts[env_idx], \"r\"))\n        layout = LayoutInfo.from_dict(layout)\n        x, y, z, qx, qy, qz, qw = layout.position[bg_node]\n        qx, qy, qz, qw = quaternion_multiply([qx, qy, qz, qw], init_quat)\n        init_pose = torch.tensor([x, y, z, qx, qy, qz, qw])\n        gs_model = raw_gs.get_gaussians(instance_pose=init_pose)\n        for key in cameras:\n            camera = cameras[key]\n            Ks = camera.camera.get_intrinsic_matrix()  # (n_env, 3, 3)\n            c2w = camera.camera.get_model_matrix()  # (n_env, 4, 4)\n            result = gs_model.render(\n                c2w[env_idx] @ sim_coord_align,\n                Ks[env_idx],\n                image_width=camera.config.width,\n                image_height=camera.config.height,\n            )\n            bg_images[f\"{key}-env{env_idx}\"] = result.rgb[..., ::-1]\n\n    return bg_images\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.render_rgb_array","title":"render_rgb_array","text":"<pre><code>render_rgb_array(camera_name: str = None, return_alpha: bool = False)\n</code></pre> <p>Renders an RGB image from the human-facing render camera.</p> <p>Parameters:</p> Name Type Description Default <code>camera_name</code> <code>str</code> <p>The name of the camera to render from. If None, uses all human render cameras.</p> <code>None</code> <code>return_alpha</code> <code>bool</code> <p>Whether to include the alpha channel in the output.</p> <code>False</code> <p>Returns:</p> Type Description <p>A numpy array representing the rendered image(s). If multiple</p> <p>cameras are used, the images are tiled.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>def render_rgb_array(\n    self, camera_name: str = None, return_alpha: bool = False\n):\n    \"\"\"Renders an RGB image from the human-facing render camera.\n\n    Args:\n        camera_name: The name of the camera to render from. If None, uses\n            all human render cameras.\n        return_alpha: Whether to include the alpha channel in the output.\n\n    Returns:\n        A numpy array representing the rendered image(s). If multiple\n        cameras are used, the images are tiled.\n    \"\"\"\n    for obj in self._hidden_objects:\n        obj.show_visual()\n    self.scene.update_render(\n        update_sensors=False, update_human_render_cameras=True\n    )\n    images = []\n    render_images = self.scene.get_human_render_camera_images(\n        camera_name, return_alpha\n    )\n    for image in render_images.values():\n        images.append(image)\n    if len(images) == 0:\n        return None\n    if len(images) == 1:\n        return images[0]\n    for obj in self._hidden_objects:\n        obj.hide_visual()\n    return tile_images(images)\n</code></pre>"},{"location":"api/envs.html#embodied_gen.envs.pick_embodiedgen.PickEmbodiedGen.render_sensors","title":"render_sensors","text":"<pre><code>render_sensors()\n</code></pre> <p>Renders images from all on-board sensor cameras.</p> <p>Returns:</p> Type Description <p>A tiled image of all sensor outputs as a numpy array.</p> Source code in <code>embodied_gen/envs/pick_embodiedgen.py</code> <pre><code>def render_sensors(self):\n    \"\"\"Renders images from all on-board sensor cameras.\n\n    Returns:\n        A tiled image of all sensor outputs as a numpy array.\n    \"\"\"\n    images = []\n    sensor_images = self.get_sensor_images()\n    for image in sensor_images.values():\n        for img in image.values():\n            images.append(img)\n    return tile_images(images)\n</code></pre>"},{"location":"api/models.html","title":"Models API","text":""},{"location":"api/models.html#embodied_gen.models.texture_model","title":"embodied_gen.models.texture_model","text":""},{"location":"api/models.html#embodied_gen.models.texture_model.build_texture_gen_pipe","title":"build_texture_gen_pipe","text":"<pre><code>build_texture_gen_pipe(base_ckpt_dir: str, controlnet_ckpt: str = None, ip_adapt_scale: float = 0, device: str = 'cuda') -&gt; DiffusionPipeline\n</code></pre> <p>Build and initialize the Kolors + ControlNet (optional IP-Adapter) texture generation pipeline.</p> <p>Loads Kolors tokenizer, text encoder (ChatGLM), VAE, UNet, scheduler and (optionally) a ControlNet checkpoint plus IP-Adapter vision encoder. If <code>controlnet_ckpt</code> is not provided, the default multi-view texture ControlNet weights are downloaded automatically from the hub. When <code>ip_adapt_scale &gt; 0</code> an IP-Adapter vision encoder and its weights are also loaded and activated.</p> <p>Parameters:</p> Name Type Description Default <code>base_ckpt_dir</code> <code>str</code> <p>Root directory where Kolors (and optionally Kolors-IP-Adapter-Plus) weights are or will be stored. Required subfolders: <code>Kolors/{text_encoder,vae,unet,scheduler}</code>.</p> required <code>controlnet_ckpt</code> <code>str</code> <p>Directory containing a ControlNet checkpoint (safetensors). If <code>None</code>, downloads the default <code>texture_gen_mv_v1</code> snapshot.</p> <code>None</code> <code>ip_adapt_scale</code> <code>float</code> <p>Strength (&gt;=0) of IP-Adapter conditioning. Set &gt;0 to enable IP-Adapter; typical values: 0.4-0.8. Default: 0 (disabled).</p> <code>0</code> <code>device</code> <code>str</code> <p>Target device to move the pipeline to (e.g. <code>\"cuda\"</code>, <code>\"cuda:0\"</code>, <code>\"cpu\"</code>). Default: <code>\"cuda\"</code>.</p> <code>'cuda'</code> <p>Returns:</p> Name Type Description <code>DiffusionPipeline</code> <code>DiffusionPipeline</code> <p>A configured</p> <code>DiffusionPipeline</code> <p><code>StableDiffusionXLControlNetImg2ImgPipeline</code> ready for multi-view texture</p> <code>DiffusionPipeline</code> <p>generation (with optional IP-Adapter support).</p> Example <p>Initialize pipeline with IP-Adapter enabled. <pre><code>from embodied_gen.models.texture_model import build_texture_gen_pipe\nip_adapt_scale = 0.7\nPIPELINE = build_texture_gen_pipe(\n    base_ckpt_dir=\"./weights\",\n    ip_adapt_scale=ip_adapt_scale,\n    device=\"cuda\",\n)\nPIPELINE.set_ip_adapter_scale([ip_adapt_scale])\n</code></pre> Initialize pipeline without IP-Adapter. <pre><code>from embodied_gen.models.texture_model import build_texture_gen_pipe\nPIPELINE = build_texture_gen_pipe(\n    base_ckpt_dir=\"./weights\",\n    ip_adapt_scale=0,\n    device=\"cuda\",\n)\n</code></pre></p> Source code in <code>embodied_gen/models/texture_model.py</code> <pre><code>def build_texture_gen_pipe(\n    base_ckpt_dir: str,\n    controlnet_ckpt: str = None,\n    ip_adapt_scale: float = 0,\n    device: str = \"cuda\",\n) -&gt; DiffusionPipeline:\n    \"\"\"Build and initialize the Kolors + ControlNet (optional IP-Adapter) texture generation pipeline.\n\n    Loads Kolors tokenizer, text encoder (ChatGLM), VAE, UNet, scheduler and (optionally)\n    a ControlNet checkpoint plus IP-Adapter vision encoder. If ``controlnet_ckpt`` is\n    not provided, the default multi-view texture ControlNet weights are downloaded\n    automatically from the hub. When ``ip_adapt_scale &gt; 0`` an IP-Adapter vision\n    encoder and its weights are also loaded and activated.\n\n    Args:\n        base_ckpt_dir (str):\n            Root directory where Kolors (and optionally Kolors-IP-Adapter-Plus) weights\n            are or will be stored. Required subfolders: ``Kolors/{text_encoder,vae,unet,scheduler}``.\n        controlnet_ckpt (str, optional):\n            Directory containing a ControlNet checkpoint (safetensors). If ``None``,\n            downloads the default ``texture_gen_mv_v1`` snapshot.\n        ip_adapt_scale (float, optional):\n            Strength (&gt;=0) of IP-Adapter conditioning. Set &gt;0 to enable IP-Adapter;\n            typical values: 0.4-0.8. Default: 0 (disabled).\n        device (str, optional):\n            Target device to move the pipeline to (e.g. ``\"cuda\"``, ``\"cuda:0\"``, ``\"cpu\"``).\n            Default: ``\"cuda\"``.\n\n    Returns:\n        DiffusionPipeline: A configured\n        ``StableDiffusionXLControlNetImg2ImgPipeline`` ready for multi-view texture\n        generation (with optional IP-Adapter support).\n\n    Example:\n        Initialize pipeline with IP-Adapter enabled.\n        ```python\n        from embodied_gen.models.texture_model import build_texture_gen_pipe\n        ip_adapt_scale = 0.7\n        PIPELINE = build_texture_gen_pipe(\n            base_ckpt_dir=\"./weights\",\n            ip_adapt_scale=ip_adapt_scale,\n            device=\"cuda\",\n        )\n        PIPELINE.set_ip_adapter_scale([ip_adapt_scale])\n        ```\n        Initialize pipeline without IP-Adapter.\n        ```python\n        from embodied_gen.models.texture_model import build_texture_gen_pipe\n        PIPELINE = build_texture_gen_pipe(\n            base_ckpt_dir=\"./weights\",\n            ip_adapt_scale=0,\n            device=\"cuda\",\n        )\n        ```\n    \"\"\"\n\n    download_kolors_weights(f\"{base_ckpt_dir}/Kolors\")\n    logger.info(f\"Load Kolors weights...\")\n    tokenizer = ChatGLMTokenizer.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/text_encoder\"\n    )\n    text_encoder = ChatGLMModel.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/text_encoder\", torch_dtype=torch.float16\n    ).half()\n    vae = AutoencoderKL.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/vae\", revision=None\n    ).half()\n    unet = UNet2DConditionModel.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/unet\", revision=None\n    ).half()\n    scheduler = EulerDiscreteScheduler.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/scheduler\"\n    )\n\n    if controlnet_ckpt is None:\n        suffix = \"texture_gen_mv_v1\"  # \"geo_cond_mv\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        controlnet_ckpt = os.path.join(model_path, suffix)\n\n    controlnet = ControlNetModel.from_pretrained(\n        controlnet_ckpt, use_safetensors=True\n    ).half()\n\n    # IP-Adapter model\n    image_encoder = None\n    clip_image_processor = None\n    if ip_adapt_scale &gt; 0:\n        image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n            f\"{base_ckpt_dir}/Kolors-IP-Adapter-Plus/image_encoder\",\n            # ignore_mismatched_sizes=True,\n        ).to(dtype=torch.float16)\n        ip_img_size = 336\n        clip_image_processor = CLIPImageProcessor(\n            size=ip_img_size, crop_size=ip_img_size\n        )\n\n    pipe = StableDiffusionXLControlNetImg2ImgPipeline(\n        vae=vae,\n        controlnet=controlnet,\n        text_encoder=text_encoder,\n        tokenizer=tokenizer,\n        unet=unet,\n        scheduler=scheduler,\n        image_encoder=image_encoder,\n        feature_extractor=clip_image_processor,\n        force_zeros_for_empty_prompt=False,\n    )\n\n    if ip_adapt_scale &gt; 0:\n        if hasattr(pipe.unet, \"encoder_hid_proj\"):\n            pipe.unet.text_encoder_hid_proj = pipe.unet.encoder_hid_proj\n        pipe.load_ip_adapter(\n            f\"{base_ckpt_dir}/Kolors-IP-Adapter-Plus\",\n            subfolder=\"\",\n            weight_name=[\"ip_adapter_plus_general.bin\"],\n        )\n        pipe.set_ip_adapter_scale([ip_adapt_scale])\n\n    pipe = pipe.to(device)\n    pipe.enable_model_cpu_offload()\n\n    return pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.gs_model","title":"embodied_gen.models.gs_model","text":""},{"location":"api/models.html#embodied_gen.models.gs_model.GaussianOperator","title":"GaussianOperator  <code>dataclass</code>","text":"<pre><code>GaussianOperator(_opacities: Tensor, _means: Tensor, _scales: Tensor, _quats: Tensor, _rgbs: Optional[Tensor] = None, _features_dc: Optional[Tensor] = None, _features_rest: Optional[Tensor] = None, sh_degree: Optional[int] = 0, device: str = 'cuda')\n</code></pre> <p>               Bases: <code>GaussianBase</code></p> <p>Gaussian Splatting operator.</p> <p>Supports transformation, scaling, color computation, and rasterization-based rendering.</p> Inherits <p>GaussianBase: Base class with Gaussian params (means, scales, etc.)</p> <p>Functionality includes: - Applying instance poses to transform Gaussian means and quaternions. - Scaling Gaussians to a real-world size. - Computing colors using spherical harmonics. - Rendering images via differentiable rasterization. - Exporting transformed and rescaled models to .ply format.</p>"},{"location":"api/models.html#embodied_gen.models.gs_model.GaussianOperator.get_gaussians","title":"get_gaussians","text":"<pre><code>get_gaussians(c2w: Tensor = None, instance_pose: Tensor = None, apply_activate: bool = False) -&gt; GaussianBase\n</code></pre> <p>Get Gaussian data under the given instance_pose.</p> Source code in <code>embodied_gen/models/gs_model.py</code> <pre><code>def get_gaussians(\n    self,\n    c2w: torch.Tensor = None,\n    instance_pose: torch.Tensor = None,\n    apply_activate: bool = False,\n) -&gt; \"GaussianBase\":\n    \"\"\"Get Gaussian data under the given instance_pose.\"\"\"\n    if c2w is None:\n        c2w = torch.eye(4).to(self.device)\n\n    if instance_pose is not None:\n        # compute the transformed gs means and quats\n        world_means, world_quats = self._compute_transform(\n            self._means, self._quats, instance_pose.float().to(self.device)\n        )\n    else:\n        world_means, world_quats = self._means, self._quats\n\n    # get colors of gaussians\n    if self._features_rest is not None:\n        colors = torch.cat(\n            (self._features_dc[:, None, :], self._features_rest), dim=1\n        )\n    else:\n        colors = self._features_dc[:, None, :]\n\n    if self.sh_degree &gt; 0:\n        viewdirs = world_means.detach() - c2w[..., :3, 3]  # (N, 3)\n        viewdirs = viewdirs / viewdirs.norm(dim=-1, keepdim=True)\n        rgbs = spherical_harmonics(self.sh_degree, viewdirs, colors)\n        rgbs = torch.clamp(rgbs + 0.5, 0.0, 1.0)\n    else:\n        rgbs = torch.sigmoid(colors[:, 0, :])\n\n    gs_dict = dict(\n        _means=world_means,\n        _opacities=(\n            torch.sigmoid(self._opacities)\n            if apply_activate\n            else self._opacities\n        ),\n        _rgbs=rgbs,\n        _scales=(\n            torch.exp(self._scales) if apply_activate else self._scales\n        ),\n        _quats=self.quat_norm(world_quats),\n        _features_dc=self._features_dc,\n        _features_rest=self._features_rest,\n        sh_degree=self.sh_degree,\n        device=self.device,\n    )\n\n    return GaussianOperator(**gs_dict)\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.layout","title":"embodied_gen.models.layout","text":""},{"location":"api/models.html#embodied_gen.models.layout.LayoutDesigner","title":"LayoutDesigner","text":"<pre><code>LayoutDesigner(gpt_client: GPTclient, system_prompt: str, verbose: bool = False)\n</code></pre> <p>               Bases: <code>object</code></p> <p>A class for querying GPT-based scene layout reasoning and formatting responses.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>str</code> <p>The system prompt for GPT.</p> <code>verbose</code> <code>bool</code> <p>Whether to log responses.</p> <code>gpt_client</code> <code>GPTclient</code> <p>The GPT client instance.</p> <p>Methods:</p> Name Description <code>query</code> <p>Query GPT with a prompt and parameters.</p> <code>format_response</code> <p>Parse and clean JSON response.</p> <code>format_response_repair</code> <p>Repair and parse JSON response.</p> <code>save_output</code> <p>Save output to file.</p> <code>__call__</code> <p>Query and process output.</p> Source code in <code>embodied_gen/models/layout.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    system_prompt: str,\n    verbose: bool = False,\n) -&gt; None:\n    self.prompt = system_prompt.strip()\n    self.verbose = verbose\n    self.gpt_client = gpt_client\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.layout.LayoutDesigner.__call__","title":"__call__","text":"<pre><code>__call__(prompt: str, save_path: str = None, params: dict = None) -&gt; dict | str\n</code></pre> <p>Query GPT and process the output.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>User prompt.</p> required <code>save_path</code> <code>str</code> <p>Path to save output.</p> <code>None</code> <code>params</code> <code>dict</code> <p>GPT parameters.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict | str</code> <p>dict | str: Output data.</p> Source code in <code>embodied_gen/models/layout.py</code> <pre><code>def __call__(\n    self, prompt: str, save_path: str = None, params: dict = None\n) -&gt; dict | str:\n    \"\"\"Query GPT and process the output.\n\n    Args:\n        prompt (str): User prompt.\n        save_path (str, optional): Path to save output.\n        params (dict, optional): GPT parameters.\n\n    Returns:\n        dict | str: Output data.\n    \"\"\"\n    response = self.query(prompt, params=params)\n    output = self.format_response_repair(response)\n    self.save_output(output, save_path) if save_path else None\n\n    return output\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.layout.LayoutDesigner.format_response","title":"format_response","text":"<pre><code>format_response(response: str) -&gt; dict\n</code></pre> <p>Format and parse GPT response as JSON.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>Raw GPT response.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Parsed JSON output.</p> <p>Raises:</p> Type Description <code>JSONDecodeError</code> <p>If parsing fails.</p> Source code in <code>embodied_gen/models/layout.py</code> <pre><code>def format_response(self, response: str) -&gt; dict:\n    \"\"\"Format and parse GPT response as JSON.\n\n    Args:\n        response (str): Raw GPT response.\n\n    Returns:\n        dict: Parsed JSON output.\n\n    Raises:\n        json.JSONDecodeError: If parsing fails.\n    \"\"\"\n    cleaned = re.sub(r\"^```json\\s*|\\s*```$\", \"\", response.strip())\n    try:\n        output = json.loads(cleaned)\n    except json.JSONDecodeError as e:\n        raise json.JSONDecodeError(\n            f\"Error: {e}, failed to parse JSON response: {response}\"\n        )\n\n    return output\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.layout.LayoutDesigner.format_response_repair","title":"format_response_repair","text":"<pre><code>format_response_repair(response: str) -&gt; dict\n</code></pre> <p>Repair and parse possibly broken JSON response.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>Raw GPT response.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Parsed JSON output.</p> Source code in <code>embodied_gen/models/layout.py</code> <pre><code>def format_response_repair(self, response: str) -&gt; dict:\n    \"\"\"Repair and parse possibly broken JSON response.\n\n    Args:\n        response (str): Raw GPT response.\n\n    Returns:\n        dict: Parsed JSON output.\n    \"\"\"\n    return json_repair.loads(response)\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.layout.LayoutDesigner.query","title":"query","text":"<pre><code>query(prompt: str, params: dict = None) -&gt; str\n</code></pre> <p>Query GPT with the system prompt and user prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>User prompt.</p> required <code>params</code> <code>dict</code> <p>GPT parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>GPT response.</p> Source code in <code>embodied_gen/models/layout.py</code> <pre><code>def query(self, prompt: str, params: dict = None) -&gt; str:\n    \"\"\"Query GPT with the system prompt and user prompt.\n\n    Args:\n        prompt (str): User prompt.\n        params (dict, optional): GPT parameters.\n\n    Returns:\n        str: GPT response.\n    \"\"\"\n    full_prompt = self.prompt + f\"\\n\\nInput:\\n\\\"{prompt}\\\"\"\n\n    response = self.gpt_client.query(\n        text_prompt=full_prompt,\n        params=params,\n    )\n\n    if self.verbose:\n        logger.info(f\"Response: {response}\")\n\n    return response\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.layout.LayoutDesigner.save_output","title":"save_output","text":"<pre><code>save_output(output: dict, save_path: str) -&gt; None\n</code></pre> <p>Save output dictionary to a file.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>dict</code> <p>Output data.</p> required <code>save_path</code> <code>str</code> <p>Path to save the file.</p> required Source code in <code>embodied_gen/models/layout.py</code> <pre><code>def save_output(self, output: dict, save_path: str) -&gt; None:\n    \"\"\"Save output dictionary to a file.\n\n    Args:\n        output (dict): Output data.\n        save_path (str): Path to save the file.\n    \"\"\"\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    with open(save_path, 'w') as f:\n        json.dump(output, f, indent=4)\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.layout.build_scene_layout","title":"build_scene_layout","text":"<pre><code>build_scene_layout(task_desc: str, output_path: str = None, gpt_params: dict = None) -&gt; LayoutInfo\n</code></pre> <p>Build a 3D scene layout from a natural language task description.</p> <p>This function uses GPT-based reasoning to generate a structured scene layout, including object hierarchy, spatial relations, and style descriptions.</p> <p>Parameters:</p> Name Type Description Default <code>task_desc</code> <code>str</code> <p>Natural language description of the robotic task.</p> required <code>output_path</code> <code>str</code> <p>Path to save the visualized scene tree.</p> <code>None</code> <code>gpt_params</code> <code>dict</code> <p>Parameters for GPT queries.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LayoutInfo</code> <code>LayoutInfo</code> <p>Structured layout information for the scene.</p> Example <pre><code>from embodied_gen.models.layout import build_scene_layout\nlayout_info = build_scene_layout(\n    task_desc=\"Put the apples on the table on the plate\",\n    output_path=\"outputs/scene_tree.jpg\",\n)\nprint(layout_info)\n</code></pre> Source code in <code>embodied_gen/models/layout.py</code> <pre><code>def build_scene_layout(\n    task_desc: str, output_path: str = None, gpt_params: dict = None\n) -&gt; LayoutInfo:\n    \"\"\"Build a 3D scene layout from a natural language task description.\n\n    This function uses GPT-based reasoning to generate a structured scene layout,\n    including object hierarchy, spatial relations, and style descriptions.\n\n    Args:\n        task_desc (str): Natural language description of the robotic task.\n        output_path (str, optional): Path to save the visualized scene tree.\n        gpt_params (dict, optional): Parameters for GPT queries.\n\n    Returns:\n        LayoutInfo: Structured layout information for the scene.\n\n    Example:\n        ```py\n        from embodied_gen.models.layout import build_scene_layout\n        layout_info = build_scene_layout(\n            task_desc=\"Put the apples on the table on the plate\",\n            output_path=\"outputs/scene_tree.jpg\",\n        )\n        print(layout_info)\n        ```\n    \"\"\"\n    layout_relation = LAYOUT_DISASSEMBLER(task_desc, params=gpt_params)\n    layout_tree = LAYOUT_GRAPHER(layout_relation, params=gpt_params)\n    object_mapping = Scene3DItemEnum.object_mapping(layout_relation)\n    obj_prompt = f'{layout_relation[\"task_desc\"]} {object_mapping}'\n    objs_desc = LAYOUT_DESCRIBER(obj_prompt, params=gpt_params)\n    layout_info = LayoutInfo(\n        layout_tree, layout_relation, objs_desc, object_mapping\n    )\n\n    if output_path is not None:\n        visualizer = SceneTreeVisualizer(layout_info)\n        visualizer.render(save_path=output_path)\n        logger.info(f\"Scene hierarchy tree saved to {output_path}\")\n\n    return layout_info\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.text_model","title":"embodied_gen.models.text_model","text":""},{"location":"api/models.html#embodied_gen.models.text_model.build_text2img_ip_pipeline","title":"build_text2img_ip_pipeline","text":"<pre><code>build_text2img_ip_pipeline(ckpt_dir: str, ref_scale: float, device: str = 'cuda') -&gt; StableDiffusionXLPipelineIP\n</code></pre> <p>Builds a Stable Diffusion XL pipeline with IP-Adapter for text-to-image generation.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_dir</code> <code>str</code> <p>Directory containing model checkpoints.</p> required <code>ref_scale</code> <code>float</code> <p>Reference scale for IP-Adapter.</p> required <code>device</code> <code>str</code> <p>Device for inference.</p> <code>'cuda'</code> <p>Returns:</p> Name Type Description <code>StableDiffusionXLPipelineIP</code> <code>StableDiffusionXLPipeline</code> <p>Configured pipeline.</p> Example <pre><code>from embodied_gen.models.text_model import build_text2img_ip_pipeline\npipe = build_text2img_ip_pipeline(\"weights/Kolors\", ref_scale=0.3)\n</code></pre> Source code in <code>embodied_gen/models/text_model.py</code> <pre><code>def build_text2img_ip_pipeline(\n    ckpt_dir: str,\n    ref_scale: float,\n    device: str = \"cuda\",\n) -&gt; StableDiffusionXLPipelineIP:\n    \"\"\"Builds a Stable Diffusion XL pipeline with IP-Adapter for text-to-image generation.\n\n    Args:\n        ckpt_dir (str): Directory containing model checkpoints.\n        ref_scale (float): Reference scale for IP-Adapter.\n        device (str, optional): Device for inference.\n\n    Returns:\n        StableDiffusionXLPipelineIP: Configured pipeline.\n\n    Example:\n        ```py\n        from embodied_gen.models.text_model import build_text2img_ip_pipeline\n        pipe = build_text2img_ip_pipeline(\"weights/Kolors\", ref_scale=0.3)\n        ```\n    \"\"\"\n    download_kolors_weights(ckpt_dir)\n\n    text_encoder = ChatGLMModel.from_pretrained(\n        f\"{ckpt_dir}/text_encoder\", torch_dtype=torch.float16\n    ).half()\n    tokenizer = ChatGLMTokenizer.from_pretrained(f\"{ckpt_dir}/text_encoder\")\n    vae = AutoencoderKL.from_pretrained(\n        f\"{ckpt_dir}/vae\", revision=None\n    ).half()\n    scheduler = EulerDiscreteScheduler.from_pretrained(f\"{ckpt_dir}/scheduler\")\n    unet = UNet2DConditionModelIP.from_pretrained(\n        f\"{ckpt_dir}/unet\", revision=None\n    ).half()\n    image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n        f\"{ckpt_dir}/../Kolors-IP-Adapter-Plus/image_encoder\",\n        ignore_mismatched_sizes=True,\n    ).to(dtype=torch.float16)\n    clip_image_processor = CLIPImageProcessor(size=336, crop_size=336)\n\n    pipe = StableDiffusionXLPipelineIP(\n        vae=vae,\n        text_encoder=text_encoder,\n        tokenizer=tokenizer,\n        unet=unet,\n        scheduler=scheduler,\n        image_encoder=image_encoder,\n        feature_extractor=clip_image_processor,\n        force_zeros_for_empty_prompt=False,\n    )\n\n    if hasattr(pipe.unet, \"encoder_hid_proj\"):\n        pipe.unet.text_encoder_hid_proj = pipe.unet.encoder_hid_proj\n\n    pipe.load_ip_adapter(\n        f\"{ckpt_dir}/../Kolors-IP-Adapter-Plus\",\n        subfolder=\"\",\n        weight_name=[\"ip_adapter_plus_general.bin\"],\n    )\n    pipe.set_ip_adapter_scale([ref_scale])\n\n    pipe = pipe.to(device)\n    pipe.image_encoder = pipe.image_encoder.to(device)\n    pipe.enable_model_cpu_offload()\n    # pipe.enable_xformers_memory_efficient_attention()\n    # pipe.enable_vae_slicing()\n\n    return pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.text_model.build_text2img_pipeline","title":"build_text2img_pipeline","text":"<pre><code>build_text2img_pipeline(ckpt_dir: str, device: str = 'cuda') -&gt; StableDiffusionXLPipeline\n</code></pre> <p>Builds a Stable Diffusion XL pipeline for text-to-image generation.</p> <p>Parameters:</p> Name Type Description Default <code>ckpt_dir</code> <code>str</code> <p>Directory containing model checkpoints.</p> required <code>device</code> <code>str</code> <p>Device for inference.</p> <code>'cuda'</code> <p>Returns:</p> Name Type Description <code>StableDiffusionXLPipeline</code> <code>StableDiffusionXLPipeline</code> <p>Configured pipeline.</p> Example <pre><code>from embodied_gen.models.text_model import build_text2img_pipeline\npipe = build_text2img_pipeline(\"weights/Kolors\")\n</code></pre> Source code in <code>embodied_gen/models/text_model.py</code> <pre><code>def build_text2img_pipeline(\n    ckpt_dir: str,\n    device: str = \"cuda\",\n) -&gt; StableDiffusionXLPipeline:\n    \"\"\"Builds a Stable Diffusion XL pipeline for text-to-image generation.\n\n    Args:\n        ckpt_dir (str): Directory containing model checkpoints.\n        device (str, optional): Device for inference.\n\n    Returns:\n        StableDiffusionXLPipeline: Configured pipeline.\n\n    Example:\n        ```py\n        from embodied_gen.models.text_model import build_text2img_pipeline\n        pipe = build_text2img_pipeline(\"weights/Kolors\")\n        ```\n    \"\"\"\n    download_kolors_weights(ckpt_dir)\n\n    text_encoder = ChatGLMModel.from_pretrained(\n        f\"{ckpt_dir}/text_encoder\", torch_dtype=torch.float16\n    ).half()\n    tokenizer = ChatGLMTokenizer.from_pretrained(f\"{ckpt_dir}/text_encoder\")\n    vae = AutoencoderKL.from_pretrained(\n        f\"{ckpt_dir}/vae\", revision=None\n    ).half()\n    scheduler = EulerDiscreteScheduler.from_pretrained(f\"{ckpt_dir}/scheduler\")\n    unet = UNet2DConditionModel.from_pretrained(\n        f\"{ckpt_dir}/unet\", revision=None\n    ).half()\n    pipe = StableDiffusionXLPipeline(\n        vae=vae,\n        text_encoder=text_encoder,\n        tokenizer=tokenizer,\n        unet=unet,\n        scheduler=scheduler,\n        force_zeros_for_empty_prompt=False,\n    )\n    pipe = pipe.to(device)\n    pipe.enable_model_cpu_offload()\n    pipe.enable_xformers_memory_efficient_attention()\n\n    return pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.text_model.download_kolors_weights","title":"download_kolors_weights","text":"<pre><code>download_kolors_weights(local_dir: str = 'weights/Kolors') -&gt; None\n</code></pre> <p>Downloads Kolors model weights from HuggingFace.</p> <p>Parameters:</p> Name Type Description Default <code>local_dir</code> <code>str</code> <p>Local directory to store weights.</p> <code>'weights/Kolors'</code> Source code in <code>embodied_gen/models/text_model.py</code> <pre><code>def download_kolors_weights(local_dir: str = \"weights/Kolors\") -&gt; None:\n    \"\"\"Downloads Kolors model weights from HuggingFace.\n\n    Args:\n        local_dir (str, optional): Local directory to store weights.\n    \"\"\"\n    logger.info(f\"Download kolors weights from huggingface...\")\n    os.makedirs(local_dir, exist_ok=True)\n    subprocess.run(\n        [\n            \"huggingface-cli\",\n            \"download\",\n            \"--resume-download\",\n            \"Kwai-Kolors/Kolors\",\n            \"--local-dir\",\n            local_dir,\n        ],\n        check=True,\n    )\n\n    ip_adapter_path = f\"{local_dir}/../Kolors-IP-Adapter-Plus\"\n    subprocess.run(\n        [\n            \"huggingface-cli\",\n            \"download\",\n            \"--resume-download\",\n            \"Kwai-Kolors/Kolors-IP-Adapter-Plus\",\n            \"--local-dir\",\n            ip_adapter_path,\n        ],\n        check=True,\n    )\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.text_model.text2img_gen","title":"text2img_gen","text":"<pre><code>text2img_gen(prompt: str, n_sample: int, guidance_scale: float, pipeline: StableDiffusionXLPipeline | StableDiffusionXLPipeline, ip_image: Image | str = None, image_wh: tuple[int, int] = [1024, 1024], infer_step: int = 50, ip_image_size: int = 512, seed: int = None) -&gt; list[Image.Image]\n</code></pre> <p>Generates images from text prompts using a Stable Diffusion XL pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt for image generation.</p> required <code>n_sample</code> <code>int</code> <p>Number of images to generate.</p> required <code>guidance_scale</code> <code>float</code> <p>Guidance scale for diffusion.</p> required <code>pipeline</code> <code>StableDiffusionXLPipeline | StableDiffusionXLPipeline</code> <p>Pipeline instance.</p> required <code>ip_image</code> <code>Image | str</code> <p>Reference image for IP-Adapter.</p> <code>None</code> <code>image_wh</code> <code>tuple[int, int]</code> <p>Output image size (width, height).</p> <code>[1024, 1024]</code> <code>infer_step</code> <code>int</code> <p>Number of inference steps.</p> <code>50</code> <code>ip_image_size</code> <code>int</code> <p>Size for IP-Adapter image.</p> <code>512</code> <code>seed</code> <code>int</code> <p>Random seed.</p> <code>None</code> <p>Returns:</p> Type Description <code>list[Image]</code> <p>list[Image.Image]: List of generated images.</p> Example <pre><code>from embodied_gen.models.text_model import text2img_gen\nimages = text2img_gen(prompt=\"banana\", n_sample=3, guidance_scale=7.5)\nimages[0].save(\"banana.png\")\n</code></pre> Source code in <code>embodied_gen/models/text_model.py</code> <pre><code>def text2img_gen(\n    prompt: str,\n    n_sample: int,\n    guidance_scale: float,\n    pipeline: StableDiffusionXLPipeline | StableDiffusionXLPipelineIP,\n    ip_image: Image.Image | str = None,\n    image_wh: tuple[int, int] = [1024, 1024],\n    infer_step: int = 50,\n    ip_image_size: int = 512,\n    seed: int = None,\n) -&gt; list[Image.Image]:\n    \"\"\"Generates images from text prompts using a Stable Diffusion XL pipeline.\n\n    Args:\n        prompt (str): Text prompt for image generation.\n        n_sample (int): Number of images to generate.\n        guidance_scale (float): Guidance scale for diffusion.\n        pipeline (StableDiffusionXLPipeline | StableDiffusionXLPipelineIP): Pipeline instance.\n        ip_image (Image.Image | str, optional): Reference image for IP-Adapter.\n        image_wh (tuple[int, int], optional): Output image size (width, height).\n        infer_step (int, optional): Number of inference steps.\n        ip_image_size (int, optional): Size for IP-Adapter image.\n        seed (int, optional): Random seed.\n\n    Returns:\n        list[Image.Image]: List of generated images.\n\n    Example:\n        ```py\n        from embodied_gen.models.text_model import text2img_gen\n        images = text2img_gen(prompt=\"banana\", n_sample=3, guidance_scale=7.5)\n        images[0].save(\"banana.png\")\n        ```\n    \"\"\"\n    prompt = PROMPT_KAPPEND.format(object=prompt.strip())\n    logger.info(f\"Processing prompt: {prompt}\")\n\n    generator = None\n    if seed is not None:\n        generator = torch.Generator(pipeline.device).manual_seed(seed)\n        torch.manual_seed(seed)\n        np.random.seed(seed)\n        random.seed(seed)\n\n    kwargs = dict(\n        prompt=prompt,\n        height=image_wh[1],\n        width=image_wh[0],\n        num_inference_steps=infer_step,\n        guidance_scale=guidance_scale,\n        num_images_per_prompt=n_sample,\n        generator=generator,\n    )\n    if ip_image is not None:\n        if isinstance(ip_image, str):\n            ip_image = Image.open(ip_image)\n        ip_image = ip_image.resize((ip_image_size, ip_image_size))\n        kwargs.update(ip_adapter_image=[ip_image])\n\n    return pipeline(**kwargs).images\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.sr_model","title":"embodied_gen.models.sr_model","text":""},{"location":"api/models.html#embodied_gen.models.sr_model.ImageRealESRGAN","title":"ImageRealESRGAN","text":"<pre><code>ImageRealESRGAN(outscale: int, model_path: str = None)\n</code></pre> <p>A wrapper for Real-ESRGAN-based image super-resolution.</p> <p>This class uses the RealESRGAN model to perform image upscaling, typically by a factor of 4.</p> <p>Attributes:</p> Name Type Description <code>outscale</code> <code>int</code> <p>The output image scale factor (e.g., 2, 4).</p> <code>model_path</code> <code>str</code> <p>Path to the pre-trained model weights.</p> Example <pre><code>from embodied_gen.models.sr_model import ImageRealESRGAN\nfrom PIL import Image\n\nsr_model = ImageRealESRGAN(outscale=4)\nimg = Image.open(\"input.png\")\nupscaled = sr_model(img)\nupscaled.save(\"output.png\")\n</code></pre> <p>Initializes the RealESRGAN upscaler.</p> <p>Parameters:</p> Name Type Description Default <code>outscale</code> <code>int</code> <p>Output scale factor.</p> required <code>model_path</code> <code>str</code> <p>Path to model weights.</p> <code>None</code> Source code in <code>embodied_gen/models/sr_model.py</code> <pre><code>def __init__(self, outscale: int, model_path: str = None) -&gt; None:\n    \"\"\"Initializes the RealESRGAN upscaler.\n\n    Args:\n        outscale (int): Output scale factor.\n        model_path (str, optional): Path to model weights.\n    \"\"\"\n    # monkey patch to support torchvision&gt;=0.16\n    import torchvision\n    from packaging import version\n\n    if version.parse(torchvision.__version__) &gt; version.parse(\"0.16\"):\n        import sys\n        import types\n\n        import torchvision.transforms.functional as TF\n\n        functional_tensor = types.ModuleType(\n            \"torchvision.transforms.functional_tensor\"\n        )\n        functional_tensor.rgb_to_grayscale = TF.rgb_to_grayscale\n        sys.modules[\"torchvision.transforms.functional_tensor\"] = (\n            functional_tensor\n        )\n\n    self.outscale = outscale\n    self.upsampler = None\n\n    if model_path is None:\n        suffix = \"super_resolution\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        model_path = os.path.join(\n            model_path, suffix, \"RealESRGAN_x4plus.pth\"\n        )\n\n    self.model_path = model_path\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.sr_model.ImageRealESRGAN.__call__","title":"__call__","text":"<pre><code>__call__(image: Union[Image, ndarray]) -&gt; Image.Image\n</code></pre> <p>Performs super-resolution on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Image, ndarray]</code> <p>Input image.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Upscaled image.</p> Source code in <code>embodied_gen/models/sr_model.py</code> <pre><code>@spaces.GPU\ndef __call__(self, image: Union[Image.Image, np.ndarray]) -&gt; Image.Image:\n    \"\"\"Performs super-resolution on the input image.\n\n    Args:\n        image (Union[Image.Image, np.ndarray]): Input image.\n\n    Returns:\n        Image.Image: Upscaled image.\n    \"\"\"\n    self._lazy_init()\n\n    if isinstance(image, Image.Image):\n        image = np.array(image)\n\n    with torch.no_grad():\n        output, _ = self.upsampler.enhance(image, outscale=self.outscale)\n\n    return Image.fromarray(output)\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.sr_model.ImageStableSR","title":"ImageStableSR","text":"<pre><code>ImageStableSR(model_path: str = 'stabilityai/stable-diffusion-x4-upscaler', device='cuda')\n</code></pre> <p>Super-resolution image upscaler using Stable Diffusion x4 upscaling model.</p> <p>This class wraps the StabilityAI Stable Diffusion x4 upscaler for high-quality image super-resolution.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path or HuggingFace repo for the model.</p> <code>'stabilityai/stable-diffusion-x4-upscaler'</code> <code>device</code> <code>str</code> <p>Device for inference.</p> <code>'cuda'</code> Example <pre><code>from embodied_gen.models.sr_model import ImageStableSR\nfrom PIL import Image\n\nsr_model = ImageStableSR()\nimg = Image.open(\"input.png\")\nupscaled = sr_model(img)\nupscaled.save(\"output.png\")\n</code></pre> <p>Initializes the Stable Diffusion x4 upscaler.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Model path or repo.</p> <code>'stabilityai/stable-diffusion-x4-upscaler'</code> <code>device</code> <code>str</code> <p>Device for inference.</p> <code>'cuda'</code> Source code in <code>embodied_gen/models/sr_model.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"stabilityai/stable-diffusion-x4-upscaler\",\n    device=\"cuda\",\n) -&gt; None:\n    \"\"\"Initializes the Stable Diffusion x4 upscaler.\n\n    Args:\n        model_path (str, optional): Model path or repo.\n        device (str, optional): Device for inference.\n    \"\"\"\n    from diffusers import StableDiffusionUpscalePipeline\n\n    self.up_pipeline_x4 = StableDiffusionUpscalePipeline.from_pretrained(\n        model_path,\n        torch_dtype=torch.float16,\n    ).to(device)\n    self.up_pipeline_x4.set_progress_bar_config(disable=True)\n    self.up_pipeline_x4.enable_model_cpu_offload()\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.sr_model.ImageStableSR.__call__","title":"__call__","text":"<pre><code>__call__(image: Union[Image, ndarray], prompt: str = '', infer_step: int = 20) -&gt; Image.Image\n</code></pre> <p>Performs super-resolution on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Image, ndarray]</code> <p>Input image.</p> required <code>prompt</code> <code>str</code> <p>Text prompt for upscaling.</p> <code>''</code> <code>infer_step</code> <code>int</code> <p>Number of inference steps.</p> <code>20</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Upscaled image.</p> Source code in <code>embodied_gen/models/sr_model.py</code> <pre><code>@spaces.GPU\ndef __call__(\n    self,\n    image: Union[Image.Image, np.ndarray],\n    prompt: str = \"\",\n    infer_step: int = 20,\n) -&gt; Image.Image:\n    \"\"\"Performs super-resolution on the input image.\n\n    Args:\n        image (Union[Image.Image, np.ndarray]): Input image.\n        prompt (str, optional): Text prompt for upscaling.\n        infer_step (int, optional): Number of inference steps.\n\n    Returns:\n        Image.Image: Upscaled image.\n    \"\"\"\n    if isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n\n    image = image.convert(\"RGB\")\n\n    with torch.no_grad():\n        upscaled_image = self.up_pipeline_x4(\n            image=image,\n            prompt=[prompt],\n            num_inference_steps=infer_step,\n        ).images[0]\n\n    return upscaled_image\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model","title":"embodied_gen.models.segment_model","text":""},{"location":"api/models.html#embodied_gen.models.segment_model.BMGG14Remover","title":"BMGG14Remover","text":"<pre><code>BMGG14Remover()\n</code></pre> <p>               Bases: <code>object</code></p> <p>Removes background using the RMBG-1.4 segmentation model.</p> Example <pre><code>from embodied_gen.models.segment_model import BMGG14Remover\nremover = BMGG14Remover()\nresult = remover(\"input.jpg\", \"output.png\")\n</code></pre> <p>Initializes the BMGG14Remover.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initializes the BMGG14Remover.\"\"\"\n    self.model = pipeline(\n        \"image-segmentation\",\n        model=\"briaai/RMBG-1.4\",\n        trust_remote_code=True,\n    )\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.BMGG14Remover.__call__","title":"__call__","text":"<pre><code>__call__(image: Union[str, Image, ndarray], save_path: str = None) -&gt; Image.Image\n</code></pre> <p>Removes background from an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Image, ndarray]</code> <p>Input image.</p> required <code>save_path</code> <code>str</code> <p>Path to save the output image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Image with background removed.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __call__(\n    self, image: Union[str, Image.Image, np.ndarray], save_path: str = None\n) -&gt; Image.Image:\n    \"\"\"Removes background from an image.\n\n    Args:\n        image (Union[str, Image.Image, np.ndarray]): Input image.\n        save_path (str, optional): Path to save the output image.\n\n    Returns:\n        Image.Image: Image with background removed.\n    \"\"\"\n    if isinstance(image, str):\n        image = Image.open(image)\n    elif isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n\n    image = resize_pil(image)\n    output_image = self.model(image)\n\n    if save_path is not None:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        output_image.save(save_path)\n\n    return output_image\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.RembgRemover","title":"RembgRemover","text":"<pre><code>RembgRemover()\n</code></pre> <p>               Bases: <code>object</code></p> <p>Removes background from images using the rembg library.</p> Example <pre><code>from embodied_gen.models.segment_model import RembgRemover\nremover = RembgRemover()\nresult = remover(\"input.jpg\", \"output.png\")\n</code></pre> <p>Initializes the RembgRemover.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the RembgRemover.\"\"\"\n    self.rembg_session = rembg.new_session(\"u2net\")\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.RembgRemover.__call__","title":"__call__","text":"<pre><code>__call__(image: Union[str, Image, ndarray], save_path: str = None) -&gt; Image.Image\n</code></pre> <p>Removes background from an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Image, ndarray]</code> <p>Input image.</p> required <code>save_path</code> <code>str</code> <p>Path to save the output image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Image with background removed (RGBA).</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __call__(\n    self, image: Union[str, Image.Image, np.ndarray], save_path: str = None\n) -&gt; Image.Image:\n    \"\"\"Removes background from an image.\n\n    Args:\n        image (Union[str, Image.Image, np.ndarray]): Input image.\n        save_path (str, optional): Path to save the output image.\n\n    Returns:\n        Image.Image: Image with background removed (RGBA).\n    \"\"\"\n    if isinstance(image, str):\n        image = Image.open(image)\n    elif isinstance(image, np.ndarray):\n        image = Image.fromarray(image)\n\n    image = resize_pil(image)\n    output_image = rembg.remove(image, session=self.rembg_session)\n\n    if save_path is not None:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        output_image.save(save_path)\n\n    return output_image\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.SAMPredictor","title":"SAMPredictor","text":"<pre><code>SAMPredictor(checkpoint: str = None, model_type: str = 'vit_h', binary_thresh: float = 0.1, device: str = 'cuda')\n</code></pre> <p>               Bases: <code>object</code></p> <p>Loads SAM models and predicts segmentation masks from user points.</p> <p>Parameters:</p> Name Type Description Default <code>checkpoint</code> <code>str</code> <p>Path to model checkpoint.</p> <code>None</code> <code>model_type</code> <code>str</code> <p>SAM model type.</p> <code>'vit_h'</code> <code>binary_thresh</code> <code>float</code> <p>Threshold for binary mask.</p> <code>0.1</code> <code>device</code> <code>str</code> <p>Device for inference.</p> <code>'cuda'</code> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __init__(\n    self,\n    checkpoint: str = None,\n    model_type: str = \"vit_h\",\n    binary_thresh: float = 0.1,\n    device: str = \"cuda\",\n):\n    self.device = device\n    self.model_type = model_type\n\n    if checkpoint is None:\n        suffix = \"sam\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        checkpoint = os.path.join(\n            model_path, suffix, \"sam_vit_h_4b8939.pth\"\n        )\n\n    self.predictor = self._load_sam_model(checkpoint)\n    self.binary_thresh = binary_thresh\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.SAMPredictor.__call__","title":"__call__","text":"<pre><code>__call__(image: Union[str, Image, ndarray], selected_points: list[list[int]]) -&gt; Image.Image\n</code></pre> <p>Segments image using selected points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Image, ndarray]</code> <p>Input image.</p> required <code>selected_points</code> <code>list[list[int]]</code> <p>List of points and labels.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Segmented RGBA image.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __call__(\n    self,\n    image: Union[str, Image.Image, np.ndarray],\n    selected_points: list[list[int]],\n) -&gt; Image.Image:\n    \"\"\"Segments image using selected points.\n\n    Args:\n        image (Union[str, Image.Image, np.ndarray]): Input image.\n        selected_points (list[list[int]]): List of points and labels.\n\n    Returns:\n        Image.Image: Segmented RGBA image.\n    \"\"\"\n    image = self.preprocess_image(image)\n    self.predictor.set_image(image)\n    masks = self.generate_masks(image, selected_points)\n\n    return self.get_segmented_image(image, masks)\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.SAMPredictor.generate_masks","title":"generate_masks","text":"<pre><code>generate_masks(image: ndarray, selected_points: list[list[int]]) -&gt; np.ndarray\n</code></pre> <p>Generates segmentation masks from selected points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image array.</p> required <code>selected_points</code> <code>list[list[int]]</code> <p>List of points and labels.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>list[tuple[np.ndarray, str]]: List of masks and names.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def generate_masks(\n    self,\n    image: np.ndarray,\n    selected_points: list[list[int]],\n) -&gt; np.ndarray:\n    \"\"\"Generates segmentation masks from selected points.\n\n    Args:\n        image (np.ndarray): Input image array.\n        selected_points (list[list[int]]): List of points and labels.\n\n    Returns:\n        list[tuple[np.ndarray, str]]: List of masks and names.\n    \"\"\"\n    if len(selected_points) == 0:\n        return []\n\n    points = (\n        torch.Tensor([p for p, _ in selected_points])\n        .to(self.predictor.device)\n        .unsqueeze(1)\n    )\n\n    labels = (\n        torch.Tensor([int(l) for _, l in selected_points])\n        .to(self.predictor.device)\n        .unsqueeze(1)\n    )\n\n    transformed_points = self.predictor.transform.apply_coords_torch(\n        points, image.shape[:2]\n    )\n\n    masks, scores, _ = self.predictor.predict_torch(\n        point_coords=transformed_points,\n        point_labels=labels,\n        multimask_output=True,\n    )\n    valid_mask = masks[:, torch.argmax(scores, dim=1)]\n    masks_pos = valid_mask[labels[:, 0] == 1, 0].cpu().detach().numpy()\n    masks_neg = valid_mask[labels[:, 0] == 0, 0].cpu().detach().numpy()\n    if len(masks_neg) == 0:\n        masks_neg = np.zeros_like(masks_pos)\n    if len(masks_pos) == 0:\n        masks_pos = np.zeros_like(masks_neg)\n    masks_neg = masks_neg.max(axis=0, keepdims=True)\n    masks_pos = masks_pos.max(axis=0, keepdims=True)\n    valid_mask = (masks_pos.astype(int) - masks_neg.astype(int)).clip(0, 1)\n\n    binary_mask = (valid_mask &gt; self.binary_thresh).astype(np.int32)\n\n    return [(mask, f\"mask_{i}\") for i, mask in enumerate(binary_mask)]\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.SAMPredictor.get_segmented_image","title":"get_segmented_image","text":"<pre><code>get_segmented_image(image: ndarray, masks: list[tuple[ndarray, str]]) -&gt; Image.Image\n</code></pre> <p>Combines masks and returns segmented image with alpha channel.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Input image array.</p> required <code>masks</code> <code>list[tuple[ndarray, str]]</code> <p>List of masks.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Segmented RGBA image.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def get_segmented_image(\n    self, image: np.ndarray, masks: list[tuple[np.ndarray, str]]\n) -&gt; Image.Image:\n    \"\"\"Combines masks and returns segmented image with alpha channel.\n\n    Args:\n        image (np.ndarray): Input image array.\n        masks (list[tuple[np.ndarray, str]]): List of masks.\n\n    Returns:\n        Image.Image: Segmented RGBA image.\n    \"\"\"\n    seg_image = Image.fromarray(image, mode=\"RGB\")\n    alpha_channel = np.zeros(\n        (seg_image.height, seg_image.width), dtype=np.uint8\n    )\n    for mask, _ in masks:\n        # Use the maximum to combine multiple masks\n        alpha_channel = np.maximum(alpha_channel, mask)\n\n    alpha_channel = np.clip(alpha_channel, 0, 1)\n    alpha_channel = (alpha_channel * 255).astype(np.uint8)\n    alpha_image = Image.fromarray(alpha_channel, mode=\"L\")\n    r, g, b = seg_image.split()\n    seg_image = Image.merge(\"RGBA\", (r, g, b, alpha_image))\n\n    return seg_image\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.SAMPredictor.preprocess_image","title":"preprocess_image","text":"<pre><code>preprocess_image(image: Image) -&gt; np.ndarray\n</code></pre> <p>Preprocesses input image for SAM prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input image.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Preprocessed image array.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def preprocess_image(self, image: Image.Image) -&gt; np.ndarray:\n    \"\"\"Preprocesses input image for SAM prediction.\n\n    Args:\n        image (Image.Image): Input image.\n\n    Returns:\n        np.ndarray: Preprocessed image array.\n    \"\"\"\n    if isinstance(image, str):\n        image = Image.open(image)\n    elif isinstance(image, np.ndarray):\n        image = Image.fromarray(image).convert(\"RGB\")\n\n    image = resize_pil(image)\n    image = np.array(image.convert(\"RGB\"))\n\n    return image\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.SAMRemover","title":"SAMRemover","text":"<pre><code>SAMRemover(checkpoint: str = None, model_type: str = 'vit_h', area_ratio: float = 15)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Loads SAM models and performs background removal on images.</p> <p>Attributes:</p> Name Type Description <code>checkpoint</code> <code>str</code> <p>Path to the model checkpoint.</p> <code>model_type</code> <code>str</code> <p>Type of the SAM model to load.</p> <code>area_ratio</code> <code>float</code> <p>Area ratio for filtering small connected components.</p> Example <pre><code>from embodied_gen.models.segment_model import SAMRemover\nremover = SAMRemover(model_type=\"vit_h\")\nresult = remover(\"input.jpg\", \"output.png\")\n</code></pre> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __init__(\n    self,\n    checkpoint: str = None,\n    model_type: str = \"vit_h\",\n    area_ratio: float = 15,\n):\n    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    self.model_type = model_type\n    self.area_ratio = area_ratio\n\n    if checkpoint is None:\n        suffix = \"sam\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        checkpoint = os.path.join(\n            model_path, suffix, \"sam_vit_h_4b8939.pth\"\n        )\n\n    self.mask_generator = self._load_sam_model(checkpoint)\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.SAMRemover.__call__","title":"__call__","text":"<pre><code>__call__(image: Union[str, Image, ndarray], save_path: str = None) -&gt; Image.Image\n</code></pre> <p>Removes the background from an image using the SAM model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Image, ndarray]</code> <p>Input image.</p> required <code>save_path</code> <code>str</code> <p>Path to save the output image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Image with background removed (RGBA).</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __call__(\n    self, image: Union[str, Image.Image, np.ndarray], save_path: str = None\n) -&gt; Image.Image:\n    \"\"\"Removes the background from an image using the SAM model.\n\n    Args:\n        image (Union[str, Image.Image, np.ndarray]): Input image.\n        save_path (str, optional): Path to save the output image.\n\n    Returns:\n        Image.Image: Image with background removed (RGBA).\n    \"\"\"\n    # Convert input to numpy array\n    if isinstance(image, str):\n        image = Image.open(image)\n    elif isinstance(image, np.ndarray):\n        image = Image.fromarray(image).convert(\"RGB\")\n    image = resize_pil(image)\n    image = np.array(image.convert(\"RGB\"))\n\n    # Generate masks\n    masks = self.mask_generator.generate(image)\n    masks = sorted(masks, key=lambda x: x[\"area\"], reverse=True)\n\n    if not masks:\n        logger.warning(\n            \"Segmentation failed: No mask generated, return raw image.\"\n        )\n        output_image = Image.fromarray(image, mode=\"RGB\")\n    else:\n        # Use the largest mask\n        best_mask = masks[0][\"segmentation\"]\n        mask = (best_mask * 255).astype(np.uint8)\n        mask = filter_small_connected_components(\n            mask, area_ratio=self.area_ratio\n        )\n        # Apply the mask to remove the background\n        background_removed = cv2.bitwise_and(image, image, mask=mask)\n        output_image = np.dstack((background_removed, mask))\n        output_image = Image.fromarray(output_image, mode=\"RGBA\")\n\n    if save_path is not None:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        output_image.save(save_path)\n\n    return output_image\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.get_segmented_image_by_agent","title":"get_segmented_image_by_agent","text":"<pre><code>get_segmented_image_by_agent(image: Image, sam_remover: SAMRemover, rbg_remover: RembgRemover, seg_checker: ImageSegChecker = None, save_path: str = None, mode: Literal['loose', 'strict'] = 'loose') -&gt; Image.Image\n</code></pre> <p>Segments an image using SAM and rembg, with quality checking.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input image.</p> required <code>sam_remover</code> <code>SAMRemover</code> <p>SAM-based remover.</p> required <code>rbg_remover</code> <code>RembgRemover</code> <p>rembg-based remover.</p> required <code>seg_checker</code> <code>ImageSegChecker</code> <p>Quality checker.</p> <code>None</code> <code>save_path</code> <code>str</code> <p>Path to save the output image.</p> <code>None</code> <code>mode</code> <code>Literal['loose', 'strict']</code> <p>Segmentation mode.</p> <code>'loose'</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Segmented RGBA image.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def get_segmented_image_by_agent(\n    image: Image.Image,\n    sam_remover: SAMRemover,\n    rbg_remover: RembgRemover,\n    seg_checker: ImageSegChecker = None,\n    save_path: str = None,\n    mode: Literal[\"loose\", \"strict\"] = \"loose\",\n) -&gt; Image.Image:\n    \"\"\"Segments an image using SAM and rembg, with quality checking.\n\n    Args:\n        image (Image.Image): Input image.\n        sam_remover (SAMRemover): SAM-based remover.\n        rbg_remover (RembgRemover): rembg-based remover.\n        seg_checker (ImageSegChecker, optional): Quality checker.\n        save_path (str, optional): Path to save the output image.\n        mode (Literal[\"loose\", \"strict\"], optional): Segmentation mode.\n\n    Returns:\n        Image.Image: Segmented RGBA image.\n    \"\"\"\n\n    def _is_valid_seg(raw_img: Image.Image, seg_img: Image.Image) -&gt; bool:\n        if seg_checker is None:\n            return True\n        return raw_img.mode == \"RGBA\" and seg_checker([raw_img, seg_img])[0]\n\n    out_sam = f\"{save_path}_sam.png\" if save_path else None\n    out_sam_inv = f\"{save_path}_sam_inv.png\" if save_path else None\n    out_rbg = f\"{save_path}_rbg.png\" if save_path else None\n\n    seg_image = sam_remover(image, out_sam)\n    seg_image = seg_image.convert(\"RGBA\")\n    _, _, _, alpha = seg_image.split()\n    seg_image_inv = invert_rgba_pil(image.convert(\"RGB\"), alpha, out_sam_inv)\n    seg_image_rbg = rbg_remover(image, out_rbg)\n\n    final_image = None\n    if _is_valid_seg(image, seg_image):\n        final_image = seg_image\n    elif _is_valid_seg(image, seg_image_inv):\n        final_image = seg_image_inv\n    elif _is_valid_seg(image, seg_image_rbg):\n        logger.warning(f\"Failed to segment by `SAM`, retry with `rembg`.\")\n        final_image = seg_image_rbg\n    else:\n        if mode == \"strict\":\n            raise RuntimeError(\n                f\"Failed to segment by `SAM` or `rembg`, abort.\"\n            )\n        logger.warning(\"Failed to segment by SAM or rembg, use raw image.\")\n        final_image = image.convert(\"RGBA\")\n\n    if save_path:\n        final_image.save(save_path)\n\n    final_image = trellis_preprocess(final_image)\n\n    return final_image\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.segment_model.invert_rgba_pil","title":"invert_rgba_pil","text":"<pre><code>invert_rgba_pil(image: Image, mask: Image, save_path: str = None) -&gt; Image.Image\n</code></pre> <p>Inverts the alpha channel of an RGBA image using a mask.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input RGB image.</p> required <code>mask</code> <code>Image</code> <p>Mask image for alpha inversion.</p> required <code>save_path</code> <code>str</code> <p>Path to save the output image.</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: RGBA image with inverted alpha.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def invert_rgba_pil(\n    image: Image.Image, mask: Image.Image, save_path: str = None\n) -&gt; Image.Image:\n    \"\"\"Inverts the alpha channel of an RGBA image using a mask.\n\n    Args:\n        image (Image.Image): Input RGB image.\n        mask (Image.Image): Mask image for alpha inversion.\n        save_path (str, optional): Path to save the output image.\n\n    Returns:\n        Image.Image: RGBA image with inverted alpha.\n    \"\"\"\n    mask = (255 - np.array(mask))[..., None]\n    image_array = np.concatenate([np.array(image), mask], axis=-1)\n    inverted_image = Image.fromarray(image_array, \"RGBA\")\n\n    if save_path is not None:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        inverted_image.save(save_path)\n\n    return inverted_image\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model","title":"embodied_gen.models.image_comm_model","text":""},{"location":"api/models.html#embodied_gen.models.image_comm_model.BasePipelineLoader","title":"BasePipelineLoader","text":"<pre><code>BasePipelineLoader(device='cuda')\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for loading Hugging Face image generation pipelines.</p> <p>Attributes:</p> Name Type Description <code>device</code> <code>str</code> <p>Device to load the pipeline on.</p> <p>Methods:</p> Name Description <code>load</code> <p>Loads and returns the pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, device=\"cuda\"):\n    self.device = device\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.BasePipelineLoader.load","title":"load  <code>abstractmethod</code>","text":"<pre><code>load()\n</code></pre> <p>Load and return the pipeline instance.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>@abstractmethod\ndef load(self):\n    \"\"\"Load and return the pipeline instance.\"\"\"\n    pass\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.BasePipelineRunner","title":"BasePipelineRunner","text":"<pre><code>BasePipelineRunner(pipe)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for running image generation pipelines.</p> <p>Attributes:</p> Name Type Description <code>pipe</code> <p>The loaded pipeline.</p> <p>Methods:</p> Name Description <code>run</code> <p>Runs the pipeline with a prompt.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, pipe):\n    self.pipe = pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.BasePipelineRunner.run","title":"run  <code>abstractmethod</code>","text":"<pre><code>run(prompt: str, **kwargs) -&gt; Image.Image\n</code></pre> <p>Run the pipeline with the given prompt.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt for image generation.</p> required <code>**kwargs</code> <p>Additional pipeline arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Generated image(s).</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>@abstractmethod\ndef run(self, prompt: str, **kwargs) -&gt; Image.Image:\n    \"\"\"Run the pipeline with the given prompt.\n\n    Args:\n        prompt (str): Text prompt for image generation.\n        **kwargs: Additional pipeline arguments.\n\n    Returns:\n        Image.Image: Generated image(s).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.ChromaLoader","title":"ChromaLoader","text":"<pre><code>ChromaLoader(device='cuda')\n</code></pre> <p>               Bases: <code>BasePipelineLoader</code></p> <p>Loader for Chroma pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, device=\"cuda\"):\n    self.device = device\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.ChromaLoader.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load the Chroma pipeline.</p> <p>Returns:</p> Name Type Description <code>ChromaPipeline</code> <p>Loaded pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def load(self):\n    \"\"\"Load the Chroma pipeline.\n\n    Returns:\n        ChromaPipeline: Loaded pipeline.\n    \"\"\"\n    return ChromaPipeline.from_pretrained(\n        \"lodestones/Chroma\", torch_dtype=torch.bfloat16\n    ).to(self.device)\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.ChromaRunner","title":"ChromaRunner","text":"<pre><code>ChromaRunner(pipe)\n</code></pre> <p>               Bases: <code>BasePipelineRunner</code></p> <p>Runner for Chroma pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, pipe):\n    self.pipe = pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.ChromaRunner.run","title":"run","text":"<pre><code>run(prompt: str, negative_prompt=None, **kwargs) -&gt; Image.Image\n</code></pre> <p>Generate images using Chroma pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt.</p> required <code>negative_prompt</code> <code>str</code> <p>Negative prompt.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Generated image(s).</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def run(self, prompt: str, negative_prompt=None, **kwargs) -&gt; Image.Image:\n    \"\"\"Generate images using Chroma pipeline.\n\n    Args:\n        prompt (str): Text prompt.\n        negative_prompt (str, optional): Negative prompt.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Image.Image: Generated image(s).\n    \"\"\"\n    return self.pipe(\n        prompt=prompt, negative_prompt=negative_prompt, **kwargs\n    ).images\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.CosmosLoader","title":"CosmosLoader","text":"<pre><code>CosmosLoader(model_id='nvidia/Cosmos-Predict2-2B-Text2Image', local_dir='weights/cosmos2', device='cuda')\n</code></pre> <p>               Bases: <code>BasePipelineLoader</code></p> <p>Loader for Cosmos2 text-to-image pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(\n    self,\n    model_id=\"nvidia/Cosmos-Predict2-2B-Text2Image\",\n    local_dir=\"weights/cosmos2\",\n    device=\"cuda\",\n):\n    super().__init__(device)\n    self.model_id = model_id\n    self.local_dir = local_dir\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.CosmosLoader.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load the Cosmos2 text-to-image pipeline.</p> <p>Returns:</p> Name Type Description <code>Cosmos2TextToImagePipeline</code> <p>Loaded pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def load(self):\n    \"\"\"Load the Cosmos2 text-to-image pipeline.\n\n    Returns:\n        Cosmos2TextToImagePipeline: Loaded pipeline.\n    \"\"\"\n    self._patch()\n    snapshot_download(\n        repo_id=self.model_id,\n        local_dir=self.local_dir,\n        local_dir_use_symlinks=False,\n        resume_download=True,\n    )\n\n    config = PipelineQuantizationConfig(\n        quant_backend=\"bitsandbytes_4bit\",\n        quant_kwargs={\n            \"load_in_4bit\": True,\n            \"bnb_4bit_quant_type\": \"nf4\",\n            \"bnb_4bit_compute_dtype\": torch.bfloat16,\n            \"bnb_4bit_use_double_quant\": True,\n        },\n        components_to_quantize=[\"text_encoder\", \"transformer\", \"unet\"],\n    )\n\n    pipe = Cosmos2TextToImagePipeline.from_pretrained(\n        self.model_id,\n        torch_dtype=torch.bfloat16,\n        quantization_config=config,\n        use_safetensors=True,\n        safety_checker=None,\n        requires_safety_checker=False,\n    ).to(self.device)\n    return pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.CosmosRunner","title":"CosmosRunner","text":"<pre><code>CosmosRunner(pipe)\n</code></pre> <p>               Bases: <code>BasePipelineRunner</code></p> <p>Runner for Cosmos2 text-to-image pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, pipe):\n    self.pipe = pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.CosmosRunner.run","title":"run","text":"<pre><code>run(prompt: str, negative_prompt=None, **kwargs) -&gt; Image.Image\n</code></pre> <p>Generate images using Cosmos2 pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt.</p> required <code>negative_prompt</code> <code>str</code> <p>Negative prompt.</p> <code>None</code> <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Generated image(s).</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def run(self, prompt: str, negative_prompt=None, **kwargs) -&gt; Image.Image:\n    \"\"\"Generate images using Cosmos2 pipeline.\n\n    Args:\n        prompt (str): Text prompt.\n        negative_prompt (str, optional): Negative prompt.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Image.Image: Generated image(s).\n    \"\"\"\n    return self.pipe(\n        prompt=prompt, negative_prompt=negative_prompt, **kwargs\n    ).images\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.FluxLoader","title":"FluxLoader","text":"<pre><code>FluxLoader(device='cuda')\n</code></pre> <p>               Bases: <code>BasePipelineLoader</code></p> <p>Loader for Flux pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, device=\"cuda\"):\n    self.device = device\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.FluxLoader.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load the Flux pipeline.</p> <p>Returns:</p> Name Type Description <code>FluxPipeline</code> <p>Loaded pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def load(self):\n    \"\"\"Load the Flux pipeline.\n\n    Returns:\n        FluxPipeline: Loaded pipeline.\n    \"\"\"\n    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n    pipe = FluxPipeline.from_pretrained(\n        \"black-forest-labs/FLUX.1-schnell\", torch_dtype=torch.bfloat16\n    )\n    pipe.enable_model_cpu_offload()\n    pipe.enable_xformers_memory_efficient_attention()\n    pipe.enable_attention_slicing()\n    return pipe.to(self.device)\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.FluxRunner","title":"FluxRunner","text":"<pre><code>FluxRunner(pipe)\n</code></pre> <p>               Bases: <code>BasePipelineRunner</code></p> <p>Runner for Flux pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, pipe):\n    self.pipe = pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.FluxRunner.run","title":"run","text":"<pre><code>run(prompt: str, **kwargs) -&gt; Image.Image\n</code></pre> <p>Generate images using Flux pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt.</p> required <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Generated image(s).</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def run(self, prompt: str, **kwargs) -&gt; Image.Image:\n    \"\"\"Generate images using Flux pipeline.\n\n    Args:\n        prompt (str): Text prompt.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Image.Image: Generated image(s).\n    \"\"\"\n    return self.pipe(prompt=prompt, **kwargs).images\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.KolorsLoader","title":"KolorsLoader","text":"<pre><code>KolorsLoader(device='cuda')\n</code></pre> <p>               Bases: <code>BasePipelineLoader</code></p> <p>Loader for Kolors pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, device=\"cuda\"):\n    self.device = device\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.KolorsLoader.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load the Kolors pipeline.</p> <p>Returns:</p> Name Type Description <code>KolorsPipeline</code> <p>Loaded pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def load(self):\n    \"\"\"Load the Kolors pipeline.\n\n    Returns:\n        KolorsPipeline: Loaded pipeline.\n    \"\"\"\n    pipe = KolorsPipeline.from_pretrained(\n        \"Kwai-Kolors/Kolors-diffusers\",\n        torch_dtype=torch.float16,\n        variant=\"fp16\",\n    ).to(self.device)\n    pipe.enable_model_cpu_offload()\n    pipe.enable_xformers_memory_efficient_attention()\n    pipe.scheduler = DPMSolverMultistepScheduler.from_config(\n        pipe.scheduler.config, use_karras_sigmas=True\n    )\n    return pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.KolorsRunner","title":"KolorsRunner","text":"<pre><code>KolorsRunner(pipe)\n</code></pre> <p>               Bases: <code>BasePipelineRunner</code></p> <p>Runner for Kolors pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, pipe):\n    self.pipe = pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.KolorsRunner.run","title":"run","text":"<pre><code>run(prompt: str, **kwargs) -&gt; Image.Image\n</code></pre> <p>Generate images using Kolors pipeline.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt.</p> required <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Generated image(s).</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def run(self, prompt: str, **kwargs) -&gt; Image.Image:\n    \"\"\"Generate images using Kolors pipeline.\n\n    Args:\n        prompt (str): Text prompt.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Image.Image: Generated image(s).\n    \"\"\"\n    return self.pipe(prompt=prompt, **kwargs).images\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.SD35Loader","title":"SD35Loader","text":"<pre><code>SD35Loader(device='cuda')\n</code></pre> <p>               Bases: <code>BasePipelineLoader</code></p> <p>Loader for Stable Diffusion 3.5 medium pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, device=\"cuda\"):\n    self.device = device\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.SD35Loader.load","title":"load","text":"<pre><code>load()\n</code></pre> <p>Load the Stable Diffusion 3.5 medium pipeline.</p> <p>Returns:</p> Name Type Description <code>StableDiffusion3Pipeline</code> <p>Loaded pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def load(self):\n    \"\"\"Load the Stable Diffusion 3.5 medium pipeline.\n\n    Returns:\n        StableDiffusion3Pipeline: Loaded pipeline.\n    \"\"\"\n    pipe = StableDiffusion3Pipeline.from_pretrained(\n        \"stabilityai/stable-diffusion-3.5-medium\",\n        torch_dtype=torch.float16,\n    )\n    pipe = pipe.to(self.device)\n    pipe.enable_model_cpu_offload()\n    pipe.enable_xformers_memory_efficient_attention()\n    pipe.enable_attention_slicing()\n    return pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.SD35Runner","title":"SD35Runner","text":"<pre><code>SD35Runner(pipe)\n</code></pre> <p>               Bases: <code>BasePipelineRunner</code></p> <p>Runner for Stable Diffusion 3.5 medium pipeline.</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def __init__(self, pipe):\n    self.pipe = pipe\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.SD35Runner.run","title":"run","text":"<pre><code>run(prompt: str, **kwargs) -&gt; Image.Image\n</code></pre> <p>Generate images using Stable Diffusion 3.5 medium.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>str</code> <p>Text prompt.</p> required <code>**kwargs</code> <p>Additional arguments.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Generated image(s).</p> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def run(self, prompt: str, **kwargs) -&gt; Image.Image:\n    \"\"\"Generate images using Stable Diffusion 3.5 medium.\n\n    Args:\n        prompt (str): Text prompt.\n        **kwargs: Additional arguments.\n\n    Returns:\n        Image.Image: Generated image(s).\n    \"\"\"\n    return self.pipe(prompt=prompt, **kwargs).images\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.image_comm_model.build_hf_image_pipeline","title":"build_hf_image_pipeline","text":"<pre><code>build_hf_image_pipeline(name: str, device='cuda') -&gt; BasePipelineRunner\n</code></pre> <p>Build a Hugging Face image generation pipeline runner by name.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the pipeline (e.g., \"sd35\", \"cosmos\").</p> required <code>device</code> <code>str</code> <p>Device to load the pipeline on.</p> <code>'cuda'</code> <p>Returns:</p> Name Type Description <code>BasePipelineRunner</code> <code>BasePipelineRunner</code> <p>Pipeline runner instance.</p> Example <pre><code>from embodied_gen.models.image_comm_model import build_hf_image_pipeline\nrunner = build_hf_image_pipeline(\"sd35\")\nimages = runner.run(prompt=\"A robot holding a sign that says 'Hello'\")\n</code></pre> Source code in <code>embodied_gen/models/image_comm_model.py</code> <pre><code>def build_hf_image_pipeline(name: str, device=\"cuda\") -&gt; BasePipelineRunner:\n    \"\"\"Build a Hugging Face image generation pipeline runner by name.\n\n    Args:\n        name (str): Name of the pipeline (e.g., \"sd35\", \"cosmos\").\n        device (str): Device to load the pipeline on.\n\n    Returns:\n        BasePipelineRunner: Pipeline runner instance.\n\n    Example:\n        ```py\n        from embodied_gen.models.image_comm_model import build_hf_image_pipeline\n        runner = build_hf_image_pipeline(\"sd35\")\n        images = runner.run(prompt=\"A robot holding a sign that says 'Hello'\")\n        ```\n    \"\"\"\n    if name not in PIPELINE_REGISTRY:\n        raise ValueError(f\"Unsupported model: {name}\")\n    loader_cls, runner_cls = PIPELINE_REGISTRY[name]\n    pipe = loader_cls(device=device).load()\n\n    return runner_cls(pipe)\n</code></pre>"},{"location":"api/models.html#embodied_gen.models.delight_model","title":"embodied_gen.models.delight_model","text":""},{"location":"api/models.html#embodied_gen.models.delight_model.DelightingModel","title":"DelightingModel","text":"<pre><code>DelightingModel(model_path: str = None, num_infer_step: int = 50, mask_erosion_size: int = 3, image_guide_scale: float = 1.5, text_guide_scale: float = 1.0, device: str = 'cuda', seed: int = 0)\n</code></pre> <p>               Bases: <code>object</code></p> <p>A model to remove the lighting in image space.</p> <p>This model is encapsulated based on the Hunyuan3D-Delight model from <code>https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0</code> # noqa</p> <p>Attributes:</p> Name Type Description <code>image_guide_scale</code> <code>float</code> <p>Weight of image guidance in diffusion process.</p> <code>text_guide_scale</code> <code>float</code> <p>Weight of text (prompt) guidance in diffusion process.</p> <code>num_infer_step</code> <code>int</code> <p>Number of inference steps for diffusion model.</p> <code>mask_erosion_size</code> <code>int</code> <p>Size of erosion kernel for alpha mask cleanup.</p> <code>device</code> <code>str</code> <p>Device used for inference, e.g., 'cuda' or 'cpu'.</p> <code>seed</code> <code>int</code> <p>Random seed for diffusion model reproducibility.</p> <code>model_path</code> <code>str</code> <p>Filesystem path to pretrained model weights.</p> <code>pipeline</code> <p>Lazy-loaded diffusion pipeline instance.</p> Source code in <code>embodied_gen/models/delight_model.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = None,\n    num_infer_step: int = 50,\n    mask_erosion_size: int = 3,\n    image_guide_scale: float = 1.5,\n    text_guide_scale: float = 1.0,\n    device: str = \"cuda\",\n    seed: int = 0,\n) -&gt; None:\n    self.image_guide_scale = image_guide_scale\n    self.text_guide_scale = text_guide_scale\n    self.num_infer_step = num_infer_step\n    self.mask_erosion_size = mask_erosion_size\n    self.kernel = np.ones(\n        (self.mask_erosion_size, self.mask_erosion_size), np.uint8\n    )\n    self.seed = seed\n    self.device = device\n    self.pipeline = None  # lazy load model adapt to @spaces.GPU\n\n    if model_path is None:\n        suffix = \"hunyuan3d-delight-v2-0\"\n        model_path = snapshot_download(\n            repo_id=\"tencent/Hunyuan3D-2\", allow_patterns=f\"{suffix}/*\"\n        )\n        model_path = os.path.join(model_path, suffix)\n\n    self.model_path = model_path\n</code></pre>"},{"location":"api/trainer.html","title":"Trainer API","text":"<p>This section covers the training pipelines for various models.</p>"},{"location":"api/trainer.html#embodied_gen.trainer.gsplat_trainer","title":"embodied_gen.trainer.gsplat_trainer","text":""},{"location":"api/trainer.html#embodied_gen.trainer.gsplat_trainer.Runner","title":"Runner","text":"<pre><code>Runner(local_rank: int, world_rank, world_size: int, cfg: GsplatTrainConfig)\n</code></pre> <p>Engine for training and testing from gsplat example.</p> <p>Code from https://github.com/nerfstudio-project/gsplat/blob/main/examples/simple_trainer.py</p> Source code in <code>embodied_gen/trainer/gsplat_trainer.py</code> <pre><code>def __init__(\n    self,\n    local_rank: int,\n    world_rank,\n    world_size: int,\n    cfg: GsplatTrainConfig,\n) -&gt; None:\n    set_random_seed(42 + local_rank)\n\n    self.cfg = cfg\n    self.world_rank = world_rank\n    self.local_rank = local_rank\n    self.world_size = world_size\n    self.device = f\"cuda:{local_rank}\"\n\n    # Where to dump results.\n    os.makedirs(cfg.result_dir, exist_ok=True)\n\n    # Setup output directories.\n    self.ckpt_dir = f\"{cfg.result_dir}/ckpts\"\n    os.makedirs(self.ckpt_dir, exist_ok=True)\n    self.stats_dir = f\"{cfg.result_dir}/stats\"\n    os.makedirs(self.stats_dir, exist_ok=True)\n    self.render_dir = f\"{cfg.result_dir}/renders\"\n    os.makedirs(self.render_dir, exist_ok=True)\n    self.ply_dir = f\"{cfg.result_dir}/ply\"\n    os.makedirs(self.ply_dir, exist_ok=True)\n\n    # Tensorboard\n    self.writer = SummaryWriter(log_dir=f\"{cfg.result_dir}/tb\")\n    self.trainset = PanoGSplatDataset(cfg.data_dir, split=\"train\")\n    self.valset = PanoGSplatDataset(\n        cfg.data_dir, split=\"train\", max_sample_num=6\n    )\n    self.testset = PanoGSplatDataset(cfg.data_dir, split=\"eval\")\n    self.scene_scale = cfg.scene_scale\n\n    # Model\n    self.splats, self.optimizers = create_splats_with_optimizers(\n        self.trainset.points,\n        self.trainset.points_rgb,\n        init_num_pts=cfg.init_num_pts,\n        init_extent=cfg.init_extent,\n        init_opacity=cfg.init_opa,\n        init_scale=cfg.init_scale,\n        means_lr=cfg.means_lr,\n        scales_lr=cfg.scales_lr,\n        opacities_lr=cfg.opacities_lr,\n        quats_lr=cfg.quats_lr,\n        sh0_lr=cfg.sh0_lr,\n        shN_lr=cfg.shN_lr,\n        scene_scale=self.scene_scale,\n        sh_degree=cfg.sh_degree,\n        sparse_grad=cfg.sparse_grad,\n        visible_adam=cfg.visible_adam,\n        batch_size=cfg.batch_size,\n        feature_dim=None,\n        device=self.device,\n        world_rank=world_rank,\n        world_size=world_size,\n    )\n    print(\"Model initialized. Number of GS:\", len(self.splats[\"means\"]))\n\n    # Densification Strategy\n    self.cfg.strategy.check_sanity(self.splats, self.optimizers)\n\n    if isinstance(self.cfg.strategy, DefaultStrategy):\n        self.strategy_state = self.cfg.strategy.initialize_state(\n            scene_scale=self.scene_scale\n        )\n    elif isinstance(self.cfg.strategy, MCMCStrategy):\n        self.strategy_state = self.cfg.strategy.initialize_state()\n    else:\n        assert_never(self.cfg.strategy)\n\n    # Losses &amp; Metrics.\n    self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(\n        self.device\n    )\n    self.psnr = PeakSignalNoiseRatio(data_range=1.0).to(self.device)\n\n    if cfg.lpips_net == \"alex\":\n        self.lpips = LearnedPerceptualImagePatchSimilarity(\n            net_type=\"alex\", normalize=True\n        ).to(self.device)\n    elif cfg.lpips_net == \"vgg\":\n        # The 3DGS official repo uses lpips vgg, which is equivalent with the following:\n        self.lpips = LearnedPerceptualImagePatchSimilarity(\n            net_type=\"vgg\", normalize=False\n        ).to(self.device)\n    else:\n        raise ValueError(f\"Unknown LPIPS network: {cfg.lpips_net}\")\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.gsplat_trainer.Runner.eval","title":"eval","text":"<pre><code>eval(step: int, stage: str = 'val', canvas_h: int = 512, canvas_w: int = 1024)\n</code></pre> <p>Entry for evaluation.</p> Source code in <code>embodied_gen/trainer/gsplat_trainer.py</code> <pre><code>@torch.no_grad()\ndef eval(\n    self,\n    step: int,\n    stage: str = \"val\",\n    canvas_h: int = 512,\n    canvas_w: int = 1024,\n):\n    \"\"\"Entry for evaluation.\"\"\"\n    print(\"Running evaluation...\")\n    cfg = self.cfg\n    device = self.device\n    world_rank = self.world_rank\n\n    valloader = torch.utils.data.DataLoader(\n        self.valset, batch_size=1, shuffle=False, num_workers=1\n    )\n    ellipse_time = 0\n    metrics = defaultdict(list)\n    for i, data in enumerate(valloader):\n        camtoworlds = data[\"camtoworld\"].to(device)\n        Ks = data[\"K\"].to(device)\n        pixels = data[\"image\"].to(device) / 255.0\n        height, width = pixels.shape[1:3]\n        masks = data[\"mask\"].to(device) if \"mask\" in data else None\n\n        pixels = pixels.permute(0, 3, 1, 2)  # NHWC -&gt; NCHW\n        pixels = F.interpolate(pixels, size=(canvas_h, canvas_w // 2))\n\n        torch.cuda.synchronize()\n        tic = time.time()\n        colors, _, _ = self.rasterize_splats(\n            camtoworlds=camtoworlds,\n            Ks=Ks,\n            width=width,\n            height=height,\n            sh_degree=cfg.sh_degree,\n            near_plane=cfg.near_plane,\n            far_plane=cfg.far_plane,\n            masks=masks,\n        )  # [1, H, W, 3]\n        torch.cuda.synchronize()\n        ellipse_time += max(time.time() - tic, 1e-10)\n\n        colors = colors.permute(0, 3, 1, 2)  # NHWC -&gt; NCHW\n        colors = F.interpolate(colors, size=(canvas_h, canvas_w // 2))\n        colors = torch.clamp(colors, 0.0, 1.0)\n        canvas_list = [pixels, colors]\n\n        if world_rank == 0:\n            canvas = torch.cat(canvas_list, dim=2).squeeze(0)\n            canvas = canvas.permute(1, 2, 0)  # CHW -&gt; HWC\n            canvas = (canvas * 255).to(torch.uint8).cpu().numpy()\n            cv2.imwrite(\n                f\"{self.render_dir}/{stage}_step{step}_{i:04d}.png\",\n                canvas[..., ::-1],\n            )\n            metrics[\"psnr\"].append(self.psnr(colors, pixels))\n            metrics[\"ssim\"].append(self.ssim(colors, pixels))\n            metrics[\"lpips\"].append(self.lpips(colors, pixels))\n\n    if world_rank == 0:\n        ellipse_time /= len(valloader)\n\n        stats = {\n            k: torch.stack(v).mean().item() for k, v in metrics.items()\n        }\n        stats.update(\n            {\n                \"ellipse_time\": ellipse_time,\n                \"num_GS\": len(self.splats[\"means\"]),\n            }\n        )\n        print(\n            f\"PSNR: {stats['psnr']:.3f}, SSIM: {stats['ssim']:.4f}, LPIPS: {stats['lpips']:.3f} \"\n            f\"Time: {stats['ellipse_time']:.3f}s/image \"\n            f\"Number of GS: {stats['num_GS']}\"\n        )\n        # save stats as json\n        with open(\n            f\"{self.stats_dir}/{stage}_step{step:04d}.json\", \"w\"\n        ) as f:\n            json.dump(stats, f)\n        # save stats to tensorboard\n        for k, v in stats.items():\n            self.writer.add_scalar(f\"{stage}/{k}\", v, step)\n        self.writer.flush()\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer","title":"embodied_gen.trainer.pono2mesh_trainer","text":""},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline","title":"Pano2MeshSRPipeline","text":"<pre><code>Pano2MeshSRPipeline(config: Pano2MeshSRConfig)\n</code></pre> <p>Pipeline for converting panoramic RGB images into 3D mesh representations.</p> <p>This class integrates depth estimation, inpainting, mesh conversion, multi-view mesh repair, and 3D Gaussian Splatting (3DGS) dataset generation.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Pano2MeshSRConfig</code> <p>Configuration object containing model and pipeline parameters.</p> required Example <pre><code>from embodied_gen.trainer.pono2mesh_trainer import Pano2MeshSRPipeline\nfrom embodied_gen.utils.config import Pano2MeshSRConfig\n\nconfig = Pano2MeshSRConfig()\npipeline = Pano2MeshSRPipeline(config)\npipeline(pano_image='example.png', output_dir='./output')\n</code></pre> <p>Initializes the pipeline with models and camera poses.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Pano2MeshSRConfig</code> <p>Configuration object.</p> required Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def __init__(self, config: Pano2MeshSRConfig) -&gt; None:\n    \"\"\"Initializes the pipeline with models and camera poses.\n\n    Args:\n        config (Pano2MeshSRConfig): Configuration object.\n    \"\"\"\n    self.cfg = config\n    self.device = config.device\n\n    # Init models.\n    self.inpainter = PanoPersFusionInpainter(save_path=None)\n    self.geo_predictor = PanoJointPredictor(save_path=None)\n    self.pano_fusion_distance_predictor = PanoFusionDistancePredictor()\n    self.super_model = ImageRealESRGAN(outscale=self.cfg.upscale_factor)\n\n    # Init poses.\n    cubemap_w2cs = get_cubemap_views_world_to_cam()\n    self.cubemap_w2cs = [p.to(self.device) for p in cubemap_w2cs]\n    self.camera_poses = self.load_camera_poses(self.cfg.trajectory_dir)\n\n    kernel = cv2.getStructuringElement(\n        cv2.MORPH_ELLIPSE, self.cfg.kernel_size\n    )\n    self.kernel = torch.from_numpy(kernel).float().to(self.device)\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.__call__","title":"__call__","text":"<pre><code>__call__(pano_image: Image | str, output_dir: str)\n</code></pre> <p>Runs the pipeline to generate mesh and 3DGS data from a panoramic image.</p> <p>Parameters:</p> Name Type Description Default <code>pano_image</code> <code>Image | str</code> <p>Input panoramic image or path.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save outputs.</p> required <p>Returns:</p> Type Description <p>None</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def __call__(self, pano_image: Image.Image | str, output_dir: str):\n    \"\"\"Runs the pipeline to generate mesh and 3DGS data from a panoramic image.\n\n    Args:\n        pano_image (Image.Image | str): Input panoramic image or path.\n        output_dir (str): Directory to save outputs.\n\n    Returns:\n        None\n    \"\"\"\n    self.init_mesh_params()\n    pano_rgb, pano_depth = self.preprocess_pano(pano_image)\n    self.sup_pool = SupInfoPool()\n    self.sup_pool.register_sup_info(\n        pose=torch.eye(4).to(self.device),\n        mask=torch.ones([self.cfg.pano_h, self.cfg.pano_w]),\n        rgb=pano_rgb.permute(1, 2, 0),\n        distance=pano_depth[..., None],\n    )\n    self.sup_pool.gen_occ_grid(res=256)\n\n    logger.info(\"Init mesh from pano RGBD image...\")\n    depth_edge = self.get_edge_image_by_depth(pano_depth)\n    inpaint_edge_mask = (\n        ~torch.from_numpy(depth_edge).to(self.device).bool()\n    )\n    self.rgbd_to_mesh(pano_rgb, pano_depth, inpaint_edge_mask)\n\n    repair_poses = self.load_inpaint_poses(self.camera_poses)\n    inpainted_panos_w_poses = self.mesh_repair_by_greedy_view_selection(\n        repair_poses, output_dir\n    )\n    torch.cuda.empty_cache()\n    torch.set_default_device(\"cpu\")\n\n    if self.cfg.mesh_file is not None:\n        mesh_path = os.path.join(output_dir, self.cfg.mesh_file)\n        self.save_mesh(mesh_path)\n\n    if self.cfg.gs_data_file is None:\n        return\n\n    logger.info(f\"Dump data for 3DGS training...\")\n    points_rgb = (self.colors.clip(0, 1) * 255).to(torch.uint8)\n    data = {\n        \"points\": self.vertices.permute(1, 0).cpu().numpy(),  # (N, 3)\n        \"points_rgb\": points_rgb.permute(1, 0).cpu().numpy(),  # (N, 3)\n        \"train\": [],\n        \"eval\": [],\n    }\n    image_h = self.cfg.cubemap_h * self.cfg.upscale_factor\n    image_w = self.cfg.cubemap_w * self.cfg.upscale_factor\n    Ks = compute_pinhole_intrinsics(image_w, image_h, self.cfg.fov)\n    for idx, (pano_img, pano_pose) in enumerate(inpainted_panos_w_poses):\n        cubemaps = self.pano_to_cubemap(pano_img)\n        for i in range(len(cubemaps)):\n            cubemap = tensor_to_pil(cubemaps[i])\n            cubemap = self.super_model(cubemap)\n            mesh_pose = self.cubemap_w2cs[i] @ pano_pose\n            c2w = self.mesh_pose_to_gs_pose(mesh_pose)\n            data[\"train\"].append(\n                {\n                    \"camtoworld\": c2w.astype(np.float32),\n                    \"K\": Ks.astype(np.float32),\n                    \"image\": np.array(cubemap),\n                    \"image_h\": image_h,\n                    \"image_w\": image_w,\n                    \"image_id\": len(cubemaps) * idx + i,\n                }\n            )\n\n    # Camera poses for evaluation.\n    for idx in range(len(self.camera_poses)):\n        c2w = self.mesh_pose_to_gs_pose(self.camera_poses[idx])\n        data[\"eval\"].append(\n            {\n                \"camtoworld\": c2w.astype(np.float32),\n                \"K\": Ks.astype(np.float32),\n                \"image_h\": image_h,\n                \"image_w\": image_w,\n                \"image_id\": idx,\n            }\n        )\n\n    data_path = os.path.join(output_dir, self.cfg.gs_data_file)\n    torch.save(data, data_path)\n\n    return\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.get_edge_image_by_depth","title":"get_edge_image_by_depth","text":"<pre><code>get_edge_image_by_depth(depth: Tensor, dilate_iter: int = 1) -&gt; np.ndarray\n</code></pre> <p>Computes edge image from depth map.</p> <p>Parameters:</p> Name Type Description Default <code>depth</code> <code>Tensor</code> <p>Depth map tensor.</p> required <code>dilate_iter</code> <code>int</code> <p>Number of dilation iterations.</p> <code>1</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Edge image.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def get_edge_image_by_depth(\n    self, depth: torch.Tensor, dilate_iter: int = 1\n) -&gt; np.ndarray:\n    \"\"\"Computes edge image from depth map.\n\n    Args:\n        depth (torch.Tensor): Depth map tensor.\n        dilate_iter (int, optional): Number of dilation iterations.\n\n    Returns:\n        np.ndarray: Edge image.\n    \"\"\"\n    if isinstance(depth, torch.Tensor):\n        depth = depth.cpu().detach().numpy()\n\n    gray = (depth / depth.max() * 255).astype(np.uint8)\n    edges = cv2.Canny(gray, 60, 150)\n    if dilate_iter &gt; 0:\n        kernel = np.ones((3, 3), np.uint8)\n        edges = cv2.dilate(edges, kernel, iterations=dilate_iter)\n\n    return edges\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.init_mesh_params","title":"init_mesh_params","text":"<pre><code>init_mesh_params() -&gt; None\n</code></pre> <p>Initializes mesh parameters and inpaint mask.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def init_mesh_params(self) -&gt; None:\n    \"\"\"Initializes mesh parameters and inpaint mask.\"\"\"\n    torch.set_default_device(self.device)\n    self.inpaint_mask = torch.ones(\n        (self.cfg.cubemap_h, self.cfg.cubemap_w), dtype=torch.bool\n    )\n    self.vertices = torch.empty((3, 0), requires_grad=False)\n    self.colors = torch.empty((3, 0), requires_grad=False)\n    self.faces = torch.empty((3, 0), dtype=torch.long, requires_grad=False)\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.inpaint_panorama","title":"inpaint_panorama","text":"<pre><code>inpaint_panorama(idx: int, colors: Tensor, distances: Tensor, pano_mask: Tensor) -&gt; tuple[torch.Tensor]\n</code></pre> <p>Inpaints missing regions in a panorama.</p> <p>Parameters:</p> Name Type Description Default <code>idx</code> <code>int</code> <p>Index of the panorama.</p> required <code>colors</code> <code>Tensor</code> <p>RGB image tensor.</p> required <code>distances</code> <code>Tensor</code> <p>Distance map tensor.</p> required <code>pano_mask</code> <code>Tensor</code> <p>Mask tensor.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor]</code> <p>tuple[torch.Tensor]: Inpainted RGB image, distances, and normals.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def inpaint_panorama(\n    self,\n    idx: int,\n    colors: torch.Tensor,\n    distances: torch.Tensor,\n    pano_mask: torch.Tensor,\n) -&gt; tuple[torch.Tensor]:\n    \"\"\"Inpaints missing regions in a panorama.\n\n    Args:\n        idx (int): Index of the panorama.\n        colors (torch.Tensor): RGB image tensor.\n        distances (torch.Tensor): Distance map tensor.\n        pano_mask (torch.Tensor): Mask tensor.\n\n    Returns:\n        tuple[torch.Tensor]: Inpainted RGB image, distances, and normals.\n    \"\"\"\n    mask = (pano_mask[None, ..., None] &gt; 0.5).float()\n    mask = mask.permute(0, 3, 1, 2)\n    mask = dilation(mask, kernel=self.kernel)\n    mask = mask[0, 0, ..., None]  # hwc\n    inpainted_img = self.inpainter.inpaint(idx, colors, mask)\n    inpainted_img = colors * (1 - mask) + inpainted_img * mask\n    inpainted_distances, inpainted_normals = self.geo_predictor(\n        idx,\n        inpainted_img,\n        distances[..., None],\n        mask=mask,\n        reg_loss_weight=0.0,\n        normal_loss_weight=5e-2,\n        normal_tv_loss_weight=5e-2,\n    )\n\n    return inpainted_img, inpainted_distances.squeeze(), inpainted_normals\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.load_camera_poses","title":"load_camera_poses","text":"<pre><code>load_camera_poses(trajectory_dir: str) -&gt; tuple[np.ndarray, list[torch.Tensor]]\n</code></pre> <p>Loads camera poses from a directory.</p> <p>Parameters:</p> Name Type Description Default <code>trajectory_dir</code> <code>str</code> <p>Directory containing camera pose files.</p> required <p>Returns:</p> Type Description <code>tuple[ndarray, list[Tensor]]</code> <p>tuple[np.ndarray, list[torch.Tensor]]: List of relative camera poses.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def load_camera_poses(\n    self, trajectory_dir: str\n) -&gt; tuple[np.ndarray, list[torch.Tensor]]:\n    \"\"\"Loads camera poses from a directory.\n\n    Args:\n        trajectory_dir (str): Directory containing camera pose files.\n\n    Returns:\n        tuple[np.ndarray, list[torch.Tensor]]: List of relative camera poses.\n    \"\"\"\n    pose_filenames = sorted(\n        [\n            fname\n            for fname in os.listdir(trajectory_dir)\n            if fname.startswith(\"camera_pose\")\n        ]\n    )\n\n    pano_pose_world = None\n    relative_poses = []\n    for idx, filename in enumerate(pose_filenames):\n        pose_path = os.path.join(trajectory_dir, filename)\n        pose_matrix = self.read_camera_pose_file(pose_path)\n\n        if pano_pose_world is None:\n            pano_pose_world = pose_matrix.copy()\n            pano_pose_world[0, 3] += self.cfg.pano_center_offset[0]\n            pano_pose_world[2, 3] += self.cfg.pano_center_offset[1]\n\n        # Use different reference for the first 6 cubemap views\n        reference_pose = pose_matrix if idx &lt; 6 else pano_pose_world\n        relative_matrix = pose_matrix @ np.linalg.inv(reference_pose)\n        relative_matrix[0:2, :] *= -1  # flip_xy\n        relative_matrix = (\n            relative_matrix @ rot_z_world_to_cam(180).cpu().numpy()\n        )\n        relative_matrix[:3, 3] *= self.cfg.pose_scale\n        relative_matrix = torch.tensor(\n            relative_matrix, dtype=torch.float32\n        )\n        relative_poses.append(relative_matrix)\n\n    return relative_poses\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.load_inpaint_poses","title":"load_inpaint_poses","text":"<pre><code>load_inpaint_poses(poses: Tensor) -&gt; dict[int, torch.Tensor]\n</code></pre> <p>Samples and loads poses for inpainting.</p> <p>Parameters:</p> Name Type Description Default <code>poses</code> <code>Tensor</code> <p>Tensor of camera poses.</p> required <p>Returns:</p> Type Description <code>dict[int, Tensor]</code> <p>dict[int, torch.Tensor]: Dictionary mapping indices to pose tensors.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def load_inpaint_poses(\n    self, poses: torch.Tensor\n) -&gt; dict[int, torch.Tensor]:\n    \"\"\"Samples and loads poses for inpainting.\n\n    Args:\n        poses (torch.Tensor): Tensor of camera poses.\n\n    Returns:\n        dict[int, torch.Tensor]: Dictionary mapping indices to pose tensors.\n    \"\"\"\n    inpaint_poses = dict()\n    sampled_views = poses[:: self.cfg.inpaint_frame_stride]\n    init_pose = torch.eye(4)\n    for idx, w2c_tensor in enumerate(sampled_views):\n        w2c = w2c_tensor.cpu().numpy().astype(np.float32)\n        c2w = np.linalg.inv(w2c)\n        pose_tensor = init_pose.clone()\n        pose_tensor[:3, 3] = torch.from_numpy(c2w[:3, 3])\n        pose_tensor[:3, 3] *= -1\n        inpaint_poses[idx] = pose_tensor.to(self.device)\n\n    return inpaint_poses\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.mesh_pose_to_gs_pose","title":"mesh_pose_to_gs_pose","text":"<pre><code>mesh_pose_to_gs_pose(mesh_pose: Tensor) -&gt; np.ndarray\n</code></pre> <p>Converts mesh pose to 3D Gaussian Splatting pose.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_pose</code> <code>Tensor</code> <p>Mesh pose tensor.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Converted pose matrix.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def mesh_pose_to_gs_pose(self, mesh_pose: torch.Tensor) -&gt; np.ndarray:\n    \"\"\"Converts mesh pose to 3D Gaussian Splatting pose.\n\n    Args:\n        mesh_pose (torch.Tensor): Mesh pose tensor.\n\n    Returns:\n        np.ndarray: Converted pose matrix.\n    \"\"\"\n    pose = mesh_pose.clone()\n    pose[0, :] *= -1\n    pose[1, :] *= -1\n\n    Rw2c = pose[:3, :3].cpu().numpy()\n    Tw2c = pose[:3, 3:].cpu().numpy()\n    yz_reverse = np.array([[1, 0, 0], [0, -1, 0], [0, 0, -1]])\n\n    Rc2w = (yz_reverse @ Rw2c).T\n    Tc2w = -(Rc2w @ yz_reverse @ Tw2c)\n    c2w = np.concatenate((Rc2w, Tc2w), axis=1)\n    c2w = np.concatenate((c2w, np.array([[0, 0, 0, 1]])), axis=0)\n\n    return c2w\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.mesh_repair_by_greedy_view_selection","title":"mesh_repair_by_greedy_view_selection","text":"<pre><code>mesh_repair_by_greedy_view_selection(pose_dict: dict[str, Tensor], output_dir: str) -&gt; list\n</code></pre> <p>Repairs mesh by selecting views greedily and inpainting missing regions.</p> <p>Parameters:</p> Name Type Description Default <code>pose_dict</code> <code>dict[str, Tensor]</code> <p>Dictionary of poses for inpainting.</p> required <code>output_dir</code> <code>str</code> <p>Directory to save visualizations.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of inpainted panoramas with poses.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def mesh_repair_by_greedy_view_selection(\n    self, pose_dict: dict[str, torch.Tensor], output_dir: str\n) -&gt; list:\n    \"\"\"Repairs mesh by selecting views greedily and inpainting missing regions.\n\n    Args:\n        pose_dict (dict[str, torch.Tensor]): Dictionary of poses for inpainting.\n        output_dir (str): Directory to save visualizations.\n\n    Returns:\n        list: List of inpainted panoramas with poses.\n    \"\"\"\n    inpainted_panos_w_pose = []\n    while len(pose_dict) &gt; 0:\n        logger.info(f\"Repairing mesh left rounds {len(pose_dict)}\")\n        sampled_views = []\n        for key, pose in pose_dict.items():\n            pano_rgb, pano_distance, pano_mask = self.render_pano(pose)\n            completeness = torch.sum(1 - pano_mask) / (pano_mask.numel())\n            sampled_views.append((key, completeness.item(), pose))\n\n        if len(sampled_views) == 0:\n            break\n\n        # Find inpainting with least view completeness.\n        sampled_views = sorted(sampled_views, key=lambda x: x[1])\n        key, _, pose = sampled_views[len(sampled_views) * 2 // 3]\n        pose_dict.pop(key)\n\n        pano_rgb, pano_distance, pano_mask = self.render_pano(pose)\n\n        colors = pano_rgb.permute(1, 2, 0).clone()\n        distances = pano_distance.unsqueeze(-1).clone()\n        pano_inpaint_mask = pano_mask.clone()\n        init_pose = pose.clone()\n        normals = None\n        if pano_inpaint_mask.min().item() &lt; 0.5:\n            colors, distances, normals = self.inpaint_panorama(\n                idx=key,\n                colors=colors,\n                distances=distances,\n                pano_mask=pano_inpaint_mask,\n            )\n\n            init_pose[0, 3], init_pose[1, 3], init_pose[2, 3] = (\n                -pose[0, 3],\n                pose[2, 3],\n                0,\n            )\n            rays = gen_pano_rays(\n                init_pose, self.cfg.pano_h, self.cfg.pano_w\n            )\n            conflict_mask = self.sup_pool.geo_check(\n                rays, distances.unsqueeze(-1)\n            )  # 0 is conflict, 1 not conflict\n            pano_inpaint_mask *= conflict_mask\n\n        self.rgbd_to_mesh(\n            colors.permute(2, 0, 1),\n            distances,\n            pano_inpaint_mask,\n            world_to_cam=pose,\n        )\n\n        self.sup_pool.register_sup_info(\n            pose=init_pose,\n            mask=pano_inpaint_mask.clone(),\n            rgb=colors,\n            distance=distances.unsqueeze(-1),\n            normal=normals,\n        )\n\n        colors = colors.permute(2, 0, 1).unsqueeze(0)\n        inpainted_panos_w_pose.append([colors, pose])\n\n        if self.cfg.visualize:\n            from embodied_gen.data.utils import DiffrastRender\n\n            tensor_to_pil(pano_rgb.unsqueeze(0)).save(\n                f\"{output_dir}/rendered_pano_{key}.jpg\"\n            )\n            tensor_to_pil(colors).save(\n                f\"{output_dir}/inpainted_pano_{key}.jpg\"\n            )\n            norm_depth = DiffrastRender.normalize_map_by_mask(\n                distances, torch.ones_like(distances)\n            )\n            heatmap = (norm_depth.cpu().numpy() * 255).astype(np.uint8)\n            heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n            Image.fromarray(heatmap).save(\n                f\"{output_dir}/inpainted_depth_{key}.png\"\n            )\n\n    return inpainted_panos_w_pose\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.pano_to_cubemap","title":"pano_to_cubemap","text":"<pre><code>pano_to_cubemap(pano_rgb: Tensor)\n</code></pre> <p>Converts a panoramic RGB image to six cubemap views.</p> <p>Parameters:</p> Name Type Description Default <code>pano_rgb</code> <code>Tensor</code> <p>Panoramic RGB image tensor.</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>List of cubemap RGB tensors.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def pano_to_cubemap(self, pano_rgb: torch.Tensor):\n    \"\"\"Converts a panoramic RGB image to six cubemap views.\n\n    Args:\n        pano_rgb (torch.Tensor): Panoramic RGB image tensor.\n\n    Returns:\n        list: List of cubemap RGB tensors.\n    \"\"\"\n    # Define six canonical cube directions in (pitch, yaw)\n    directions = [\n        (0, 0),\n        (0, 1.5 * np.pi),\n        (0, 1.0 * np.pi),\n        (0, 0.5 * np.pi),\n        (-0.5 * np.pi, 0),\n        (0.5 * np.pi, 0),\n    ]\n\n    cubemaps_rgb = []\n    for pitch, yaw in directions:\n        rgb_view = self.pano_to_perpective(\n            pano_rgb, pitch, yaw, fov=self.cfg.fov\n        )\n        cubemaps_rgb.append(rgb_view.cpu())\n\n    return cubemaps_rgb\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.pano_to_perpective","title":"pano_to_perpective","text":"<pre><code>pano_to_perpective(pano_image: Tensor, pitch: float, yaw: float, fov: float) -&gt; torch.Tensor\n</code></pre> <p>Converts a panoramic image to a perspective view.</p> <p>Parameters:</p> Name Type Description Default <code>pano_image</code> <code>Tensor</code> <p>Panoramic image tensor.</p> required <code>pitch</code> <code>float</code> <p>Pitch angle.</p> required <code>yaw</code> <code>float</code> <p>Yaw angle.</p> required <code>fov</code> <code>float</code> <p>Field of view.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: Perspective image tensor.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def pano_to_perpective(\n    self, pano_image: torch.Tensor, pitch: float, yaw: float, fov: float\n) -&gt; torch.Tensor:\n    \"\"\"Converts a panoramic image to a perspective view.\n\n    Args:\n        pano_image (torch.Tensor): Panoramic image tensor.\n        pitch (float): Pitch angle.\n        yaw (float): Yaw angle.\n        fov (float): Field of view.\n\n    Returns:\n        torch.Tensor: Perspective image tensor.\n    \"\"\"\n    rots = dict(\n        roll=0,\n        pitch=pitch,\n        yaw=yaw,\n    )\n    perspective = equi2pers(\n        equi=pano_image.squeeze(0),\n        rots=rots,\n        height=self.cfg.cubemap_h,\n        width=self.cfg.cubemap_w,\n        fov_x=fov,\n        mode=\"bilinear\",\n    ).unsqueeze(0)\n\n    return perspective\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.preprocess_pano","title":"preprocess_pano","text":"<pre><code>preprocess_pano(image: Image | str) -&gt; tuple[torch.Tensor, torch.Tensor]\n</code></pre> <p>Preprocesses a panoramic image for mesh generation.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image | str</code> <p>Input image or path.</p> required <p>Returns:</p> Type Description <code>tuple[Tensor, Tensor]</code> <p>tuple[torch.Tensor, torch.Tensor]: Preprocessed RGB and depth tensors.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def preprocess_pano(\n    self, image: Image.Image | str\n) -&gt; tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Preprocesses a panoramic image for mesh generation.\n\n    Args:\n        image (Image.Image | str): Input image or path.\n\n    Returns:\n        tuple[torch.Tensor, torch.Tensor]: Preprocessed RGB and depth tensors.\n    \"\"\"\n    if isinstance(image, str):\n        image = Image.open(image)\n\n    image = image.convert(\"RGB\")\n\n    if image.size[0] &lt; image.size[1]:\n        image = image.transpose(Image.TRANSPOSE)\n\n    image = resize_image_with_aspect_ratio(image, self.cfg.pano_w)\n    image_rgb = torch.tensor(np.array(image)).permute(2, 0, 1) / 255\n    image_rgb = image_rgb.to(self.device)\n    image_depth = self.pano_fusion_distance_predictor.predict(\n        image_rgb.permute(1, 2, 0)\n    )\n    image_depth = (\n        image_depth / image_depth.max() * self.cfg.depth_scale_factor\n    )\n\n    return image_rgb, image_depth\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.project","title":"project","text":"<pre><code>project(world_to_cam: Tensor)\n</code></pre> <p>Projects the mesh to an image using the given camera pose.</p> <p>Parameters:</p> Name Type Description Default <code>world_to_cam</code> <code>Tensor</code> <p>World-to-camera transformation matrix.</p> required <p>Returns:</p> Type Description <p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Projected RGB image, inpaint mask, and depth map.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def project(self, world_to_cam: torch.Tensor):\n    \"\"\"Projects the mesh to an image using the given camera pose.\n\n    Args:\n        world_to_cam (torch.Tensor): World-to-camera transformation matrix.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Projected RGB image, inpaint mask, and depth map.\n    \"\"\"\n    (\n        project_image,\n        project_depth,\n        inpaint_mask,\n        _,\n        z_buf,\n        mesh,\n    ) = render_mesh(\n        vertices=self.vertices,\n        faces=self.faces,\n        vertex_features=self.colors,\n        H=self.cfg.cubemap_h,\n        W=self.cfg.cubemap_w,\n        fov_in_degrees=self.cfg.fov,\n        RT=world_to_cam,\n        blur_radius=self.cfg.blur_radius,\n        faces_per_pixel=self.cfg.faces_per_pixel,\n    )\n    project_image = project_image * ~inpaint_mask\n\n    return project_image[:3, ...], inpaint_mask, project_depth\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.read_camera_pose_file","title":"read_camera_pose_file  <code>staticmethod</code>","text":"<pre><code>read_camera_pose_file(filepath: str) -&gt; np.ndarray\n</code></pre> <p>Reads a camera pose file and returns the pose matrix.</p> <p>Parameters:</p> Name Type Description Default <code>filepath</code> <code>str</code> <p>Path to the camera pose file.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 4x4 camera pose matrix.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>@staticmethod\ndef read_camera_pose_file(filepath: str) -&gt; np.ndarray:\n    \"\"\"Reads a camera pose file and returns the pose matrix.\n\n    Args:\n        filepath (str): Path to the camera pose file.\n\n    Returns:\n        np.ndarray: 4x4 camera pose matrix.\n    \"\"\"\n    with open(filepath, \"r\") as f:\n        values = [float(num) for line in f for num in line.split()]\n\n    return np.array(values).reshape(4, 4)\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.render_pano","title":"render_pano","text":"<pre><code>render_pano(pose: Tensor)\n</code></pre> <p>Renders a panorama from the mesh using the given pose.</p> <p>Parameters:</p> Name Type Description Default <code>pose</code> <code>Tensor</code> <p>Camera pose.</p> required <p>Returns:</p> Type Description <p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: RGB panorama, depth map, and mask.</p> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def render_pano(self, pose: torch.Tensor):\n    \"\"\"Renders a panorama from the mesh using the given pose.\n\n    Args:\n        pose (torch.Tensor): Camera pose.\n\n    Returns:\n        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: RGB panorama, depth map, and mask.\n    \"\"\"\n    cubemap_list = []\n    for cubemap_pose in self.cubemap_w2cs:\n        project_pose = cubemap_pose @ pose\n        rgb, inpaint_mask, depth = self.project(project_pose)\n        distance_map = depth_to_distance(depth[None, ...])\n        mask = inpaint_mask[None, ...]\n        cubemap_list.append(torch.cat([rgb, distance_map, mask], dim=0))\n\n    # Set default tensor type for CPU operation in cube2equi\n    with torch.device(\"cpu\"):\n        pano_rgbd = cube2equi(\n            cubemap_list, \"list\", self.cfg.pano_h, self.cfg.pano_w\n        )\n\n    pano_rgb = pano_rgbd[:3, :, :]\n    pano_depth = pano_rgbd[3:4, :, :].squeeze(0)\n    pano_mask = pano_rgbd[4:, :, :].squeeze(0)\n\n    return pano_rgb, pano_depth, pano_mask\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.rgbd_to_mesh","title":"rgbd_to_mesh","text":"<pre><code>rgbd_to_mesh(rgb: Tensor, depth: Tensor, inpaint_mask: Tensor, world_to_cam: Tensor = None, using_distance_map: bool = True) -&gt; None\n</code></pre> <p>Converts RGB-D images to mesh and updates mesh parameters.</p> <p>Parameters:</p> Name Type Description Default <code>rgb</code> <code>Tensor</code> <p>RGB image tensor.</p> required <code>depth</code> <code>Tensor</code> <p>Depth map tensor.</p> required <code>inpaint_mask</code> <code>Tensor</code> <p>Inpaint mask tensor.</p> required <code>world_to_cam</code> <code>Tensor</code> <p>Camera pose.</p> <code>None</code> <code>using_distance_map</code> <code>bool</code> <p>Whether to use distance map.</p> <code>True</code> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def rgbd_to_mesh(\n    self,\n    rgb: torch.Tensor,\n    depth: torch.Tensor,\n    inpaint_mask: torch.Tensor,\n    world_to_cam: torch.Tensor = None,\n    using_distance_map: bool = True,\n) -&gt; None:\n    \"\"\"Converts RGB-D images to mesh and updates mesh parameters.\n\n    Args:\n        rgb (torch.Tensor): RGB image tensor.\n        depth (torch.Tensor): Depth map tensor.\n        inpaint_mask (torch.Tensor): Inpaint mask tensor.\n        world_to_cam (torch.Tensor, optional): Camera pose.\n        using_distance_map (bool, optional): Whether to use distance map.\n    \"\"\"\n    if world_to_cam is None:\n        world_to_cam = torch.eye(4, dtype=torch.float32).to(self.device)\n\n    if inpaint_mask.sum() == 0:\n        return\n\n    vertices, faces, colors = features_to_world_space_mesh(\n        colors=rgb.squeeze(0),\n        depth=depth,\n        fov_in_degrees=self.cfg.fov,\n        world_to_cam=world_to_cam,\n        mask=inpaint_mask,\n        faces=self.faces,\n        vertices=self.vertices,\n        using_distance_map=using_distance_map,\n        edge_threshold=0.05,\n    )\n\n    faces += self.vertices.shape[1]\n    self.vertices = torch.cat([self.vertices, vertices], dim=1)\n    self.colors = torch.cat([self.colors, colors], dim=1)\n    self.faces = torch.cat([self.faces, faces], dim=1)\n</code></pre>"},{"location":"api/trainer.html#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline.save_mesh","title":"save_mesh","text":"<pre><code>save_mesh(output_path: str) -&gt; None\n</code></pre> <p>Saves the mesh to a file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to save the mesh file.</p> required Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def save_mesh(self, output_path: str) -&gt; None:\n    \"\"\"Saves the mesh to a file.\n\n    Args:\n        output_path (str): Path to save the mesh file.\n    \"\"\"\n    vertices_np = self.vertices.T.cpu().numpy()\n    colors_np = self.colors.T.cpu().numpy()\n    faces_np = self.faces.T.cpu().numpy()\n    mesh = trimesh.Trimesh(\n        vertices=vertices_np, faces=faces_np, vertex_colors=colors_np\n    )\n\n    mesh.export(output_path)\n</code></pre>"},{"location":"api/utils.html","title":"Utilities API","text":"<p>General-purpose utility functions, configuration, and helper classes.</p>"},{"location":"api/utils.html#embodied_gen.utils.config","title":"embodied_gen.utils.config","text":""},{"location":"api/utils.html#embodied_gen.utils.log","title":"embodied_gen.utils.log","text":""},{"location":"api/utils.html#embodied_gen.utils.enum","title":"embodied_gen.utils.enum","text":""},{"location":"api/utils.html#embodied_gen.utils.enum.AssetType","title":"AssetType  <code>dataclass</code>","text":"<pre><code>AssetType()\n</code></pre> <p>               Bases: <code>str</code></p> <p>Enumeration for asset types.</p> Supported types <p>MJCF: MuJoCo XML format. USD: Universal Scene Description format. URDF: Unified Robot Description Format. MESH: Mesh file format.</p>"},{"location":"api/utils.html#embodied_gen.utils.enum.LayoutInfo","title":"LayoutInfo  <code>dataclass</code>","text":"<pre><code>LayoutInfo(tree: dict[str, list], relation: dict[str, str | list[str]], objs_desc: dict[str, str] = dict(), objs_mapping: dict[str, str] = dict(), assets: dict[str, str] = dict(), quality: dict[str, str] = dict(), position: dict[str, list[float]] = dict())\n</code></pre> <p>               Bases: <code>DataClassJsonMixin</code></p> <p>Data structure for layout information in a 3D scene.</p> <p>Attributes:</p> Name Type Description <code>tree</code> <code>dict[str, list]</code> <p>Hierarchical structure of scene objects.</p> <code>relation</code> <code>dict[str, str | list[str]]</code> <p>Spatial relations between objects.</p> <code>objs_desc</code> <code>dict[str, str]</code> <p>Descriptions of objects.</p> <code>objs_mapping</code> <code>dict[str, str]</code> <p>Mapping from object names to categories.</p> <code>assets</code> <code>dict[str, str]</code> <p>Asset file paths for objects.</p> <code>quality</code> <code>dict[str, str]</code> <p>Quality information for assets.</p> <code>position</code> <code>dict[str, list[float]]</code> <p>Position coordinates for objects.</p>"},{"location":"api/utils.html#embodied_gen.utils.enum.RenderItems","title":"RenderItems  <code>dataclass</code>","text":"<pre><code>RenderItems()\n</code></pre> <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of render item types for 3D scenes.</p> <p>Attributes:</p> Name Type Description <code>IMAGE</code> <p>Color image.</p> <code>ALPHA</code> <p>Mask image.</p> <code>VIEW_NORMAL</code> <p>View-space normal image.</p> <code>GLOBAL_NORMAL</code> <p>World-space normal image.</p> <code>POSITION_MAP</code> <p>Position map image.</p> <code>DEPTH</code> <p>Depth image.</p> <code>ALBEDO</code> <p>Albedo image.</p> <code>DIFFUSE</code> <p>Diffuse image.</p>"},{"location":"api/utils.html#embodied_gen.utils.enum.RobotItemEnum","title":"RobotItemEnum  <code>dataclass</code>","text":"<pre><code>RobotItemEnum()\n</code></pre> <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of supported robot types.</p> <p>Attributes:</p> Name Type Description <code>FRANKA</code> <p>Franka robot.</p> <code>UR5</code> <p>UR5 robot.</p> <code>PIPER</code> <p>Piper robot.</p>"},{"location":"api/utils.html#embodied_gen.utils.enum.Scene3DItemEnum","title":"Scene3DItemEnum  <code>dataclass</code>","text":"<pre><code>Scene3DItemEnum()\n</code></pre> <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of 3D scene item categories.</p> <p>Attributes:</p> Name Type Description <code>BACKGROUND</code> <p>Background objects.</p> <code>CONTEXT</code> <p>Contextual objects.</p> <code>ROBOT</code> <p>Robot entity.</p> <code>MANIPULATED_OBJS</code> <p>Objects manipulated by the robot.</p> <code>DISTRACTOR_OBJS</code> <p>Distractor objects.</p> <code>OTHERS</code> <p>Other objects.</p> <p>Methods:</p> Name Description <code>object_list</code> <p>Returns a list of objects in the scene.</p> <code>object_mapping</code> <p>Returns a mapping from object to category.</p>"},{"location":"api/utils.html#embodied_gen.utils.enum.Scene3DItemEnum.object_list","title":"object_list  <code>classmethod</code>","text":"<pre><code>object_list(layout_relation: dict) -&gt; list\n</code></pre> <p>Returns a list of objects in the scene.</p> <p>Parameters:</p> Name Type Description Default <code>layout_relation</code> <code>dict</code> <p>Dictionary mapping categories to objects.</p> required <p>Returns:</p> Type Description <code>list</code> <p>List of objects in the scene.</p> Source code in <code>embodied_gen/utils/enum.py</code> <pre><code>@classmethod\ndef object_list(cls, layout_relation: dict) -&gt; list:\n    \"\"\"Returns a list of objects in the scene.\n\n    Args:\n        layout_relation: Dictionary mapping categories to objects.\n\n    Returns:\n        List of objects in the scene.\n    \"\"\"\n    return (\n        [\n            layout_relation[cls.BACKGROUND.value],\n            layout_relation[cls.CONTEXT.value],\n        ]\n        + layout_relation[cls.MANIPULATED_OBJS.value]\n        + layout_relation[cls.DISTRACTOR_OBJS.value]\n    )\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.enum.Scene3DItemEnum.object_mapping","title":"object_mapping  <code>classmethod</code>","text":"<pre><code>object_mapping(layout_relation)\n</code></pre> <p>Returns a mapping from object to category.</p> <p>Parameters:</p> Name Type Description Default <code>layout_relation</code> <p>Dictionary mapping categories to objects.</p> required <p>Returns:</p> Type Description <p>Dictionary mapping object names to their category.</p> Source code in <code>embodied_gen/utils/enum.py</code> <pre><code>@classmethod\ndef object_mapping(cls, layout_relation):\n    \"\"\"Returns a mapping from object to category.\n\n    Args:\n        layout_relation: Dictionary mapping categories to objects.\n\n    Returns:\n        Dictionary mapping object names to their category.\n    \"\"\"\n    relation_mapping = {\n        # layout_relation[cls.ROBOT.value]: cls.ROBOT.value,\n        layout_relation[cls.BACKGROUND.value]: cls.BACKGROUND.value,\n        layout_relation[cls.CONTEXT.value]: cls.CONTEXT.value,\n    }\n    relation_mapping.update(\n        {\n            item: cls.MANIPULATED_OBJS.value\n            for item in layout_relation[cls.MANIPULATED_OBJS.value]\n        }\n    )\n    relation_mapping.update(\n        {\n            item: cls.DISTRACTOR_OBJS.value\n            for item in layout_relation[cls.DISTRACTOR_OBJS.value]\n        }\n    )\n\n    return relation_mapping\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.enum.SimAssetMapper","title":"SimAssetMapper","text":"<p>Maps simulator names to asset types.</p> <p>Provides a mapping from simulator names to their corresponding asset type.</p> Example <pre><code>from embodied_gen.utils.enum import SimAssetMapper\nasset_type = SimAssetMapper[\"isaacsim\"]\nprint(asset_type)  # Output: 'usd'\n</code></pre> <p>Methods:</p> Name Description <code>__class_getitem__</code> <p>Returns the asset type for a given simulator name.</p>"},{"location":"api/utils.html#embodied_gen.utils.enum.SimAssetMapper.__class_getitem__","title":"__class_getitem__  <code>classmethod</code>","text":"<pre><code>__class_getitem__(key: str)\n</code></pre> <p>Returns the asset type for a given simulator name.</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the simulator.</p> required <p>Returns:</p> Type Description <p>AssetType corresponding to the simulator.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the simulator name is not recognized.</p> Source code in <code>embodied_gen/utils/enum.py</code> <pre><code>@classmethod\ndef __class_getitem__(cls, key: str):\n    \"\"\"Returns the asset type for a given simulator name.\n\n    Args:\n        key: Name of the simulator.\n\n    Returns:\n        AssetType corresponding to the simulator.\n\n    Raises:\n        KeyError: If the simulator name is not recognized.\n    \"\"\"\n    key = key.upper()\n    if key.startswith(\"SAPIEN\"):\n        key = \"SAPIEN\"\n    return cls._mapping[key]\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.enum.SpatialRelationEnum","title":"SpatialRelationEnum  <code>dataclass</code>","text":"<pre><code>SpatialRelationEnum()\n</code></pre> <p>               Bases: <code>str</code>, <code>Enum</code></p> <p>Enumeration of spatial relations for objects in a scene.</p> <p>Attributes:</p> Name Type Description <code>ON</code> <p>Objects on a surface (e.g., table).</p> <code>IN</code> <p>Objects in a container or room.</p> <code>INSIDE</code> <p>Objects inside a shelf or rack.</p> <code>FLOOR</code> <p>Objects on the floor.</p>"},{"location":"api/utils.html#embodied_gen.utils.geometry","title":"embodied_gen.utils.geometry","text":""},{"location":"api/utils.html#embodied_gen.utils.geometry.all_corners_inside","title":"all_corners_inside","text":"<pre><code>all_corners_inside(hull: Path, box: list, threshold: int = 3) -&gt; bool\n</code></pre> <p>Checks if at least <code>threshold</code> corners of a box are inside a hull.</p> <p>Parameters:</p> Name Type Description Default <code>hull</code> <code>Path</code> <p>Convex hull path.</p> required <code>box</code> <code>list</code> <p>Box coordinates [x1, x2, y1, y2].</p> required <code>threshold</code> <code>int</code> <p>Minimum corners inside.</p> <code>3</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if enough corners are inside.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def all_corners_inside(hull: Path, box: list, threshold: int = 3) -&gt; bool:\n    \"\"\"Checks if at least `threshold` corners of a box are inside a hull.\n\n    Args:\n        hull (Path): Convex hull path.\n        box (list): Box coordinates [x1, x2, y1, y2].\n        threshold (int, optional): Minimum corners inside.\n\n    Returns:\n        bool: True if enough corners are inside.\n    \"\"\"\n    x1, x2, y1, y2 = box\n    corners = [[x1, y1], [x2, y1], [x1, y2], [x2, y2]]\n\n    num_inside = sum(hull.contains_point(c) for c in corners)\n    return num_inside &gt;= threshold\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.bfs_placement","title":"bfs_placement","text":"<pre><code>bfs_placement(layout_file: str, floor_margin: float = 0, beside_margin: float = 0.1, max_attempts: int = 3000, init_rpy: tuple = (1.5708, 0.0, 0.0), rotate_objs: bool = True, rotate_bg: bool = True, rotate_context: bool = True, limit_reach_range: tuple[float, float] | None = (0.2, 0.85), max_orient_diff: float | None = 60, robot_dim: float = 0.12, seed: int = None) -&gt; LayoutInfo\n</code></pre> <p>Places objects in a scene layout using BFS traversal.</p> <p>Parameters:</p> Name Type Description Default <code>layout_file</code> <code>str</code> <p>Path to layout JSON file generated from <code>layout-cli</code>.</p> required <code>floor_margin</code> <code>float</code> <p>Z-offset for objects placed on the floor.</p> <code>0</code> <code>beside_margin</code> <code>float</code> <p>Minimum margin for objects placed 'beside' their parent, used when 'on' placement fails.</p> <code>0.1</code> <code>max_attempts</code> <code>int</code> <p>Max attempts for a non-overlapping placement.</p> <code>3000</code> <code>init_rpy</code> <code>tuple</code> <p>Initial rotation (rpy).</p> <code>(1.5708, 0.0, 0.0)</code> <code>rotate_objs</code> <code>bool</code> <p>Whether to random rotate objects.</p> <code>True</code> <code>rotate_bg</code> <code>bool</code> <p>Whether to random rotate background.</p> <code>True</code> <code>rotate_context</code> <code>bool</code> <p>Whether to random rotate context asset.</p> <code>True</code> <code>limit_reach_range</code> <code>tuple[float, float] | None</code> <p>If set, enforce a check that manipulated objects are within the robot's reach range, in meter.</p> <code>(0.2, 0.85)</code> <code>max_orient_diff</code> <code>float | None</code> <p>If set, enforce a check that manipulated objects are within the robot's orientation range, in degree.</p> <code>60</code> <code>robot_dim</code> <code>float</code> <p>The approximate robot size.</p> <code>0.12</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible placement.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>LayoutInfo</code> <code>LayoutInfo</code> <p>Layout information with object poses.</p> Example <pre><code>from embodied_gen.utils.geometry import bfs_placement\nlayout = bfs_placement(\"scene_layout.json\", seed=42)\nprint(layout.position)\n</code></pre> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>@with_seed(\"seed\")\ndef bfs_placement(\n    layout_file: str,\n    floor_margin: float = 0,\n    beside_margin: float = 0.1,\n    max_attempts: int = 3000,\n    init_rpy: tuple = (1.5708, 0.0, 0.0),\n    rotate_objs: bool = True,\n    rotate_bg: bool = True,\n    rotate_context: bool = True,\n    limit_reach_range: tuple[float, float] | None = (0.20, 0.85),\n    max_orient_diff: float | None = 60,\n    robot_dim: float = 0.12,\n    seed: int = None,\n) -&gt; LayoutInfo:\n    \"\"\"Places objects in a scene layout using BFS traversal.\n\n    Args:\n        layout_file (str): Path to layout JSON file generated from `layout-cli`.\n        floor_margin (float, optional): Z-offset for objects placed on the floor.\n        beside_margin (float, optional): Minimum margin for objects placed 'beside' their parent, used when 'on' placement fails.\n        max_attempts (int, optional): Max attempts for a non-overlapping placement.\n        init_rpy (tuple, optional): Initial rotation (rpy).\n        rotate_objs (bool, optional): Whether to random rotate objects.\n        rotate_bg (bool, optional): Whether to random rotate background.\n        rotate_context (bool, optional): Whether to random rotate context asset.\n        limit_reach_range (tuple[float, float] | None, optional): If set, enforce a check that manipulated objects are within the robot's reach range, in meter.\n        max_orient_diff (float | None, optional): If set, enforce a check that manipulated objects are within the robot's orientation range, in degree.\n        robot_dim (float, optional): The approximate robot size.\n        seed (int, optional): Random seed for reproducible placement.\n\n    Returns:\n        LayoutInfo: Layout information with object poses.\n\n    Example:\n        ```py\n        from embodied_gen.utils.geometry import bfs_placement\n        layout = bfs_placement(\"scene_layout.json\", seed=42)\n        print(layout.position)\n        ```\n    \"\"\"\n    layout_info = LayoutInfo.from_dict(json.load(open(layout_file, \"r\")))\n    asset_dir = os.path.dirname(layout_file)\n    object_mapping = layout_info.objs_mapping\n    position = {}  # node: [x, y, z, qx, qy, qz, qw]\n    parent_bbox_xy = {}\n    placed_boxes_map = defaultdict(list)\n    mesh_info = defaultdict(dict)\n    robot_node = layout_info.relation[Scene3DItemEnum.ROBOT.value]\n    for node in object_mapping:\n        if object_mapping[node] == Scene3DItemEnum.BACKGROUND.value:\n            bg_quat = (\n                compute_axis_rotation_quat(\n                    axis=\"y\",\n                    angle_rad=np.random.uniform(0, 2 * np.pi),\n                )\n                if rotate_bg\n                else [0, 0, 0, 1]\n            )\n            bg_quat = [round(q, 4) for q in bg_quat]\n            continue\n\n        mesh_path = (\n            f\"{layout_info.assets[node]}/mesh/{node.replace(' ', '_')}.obj\"\n        )\n        mesh_path = os.path.join(asset_dir, mesh_path)\n        mesh_info[node][\"path\"] = mesh_path\n        mesh = trimesh.load(mesh_path)\n        rotation = R.from_euler(\"xyz\", init_rpy, degrees=False)\n        vertices = mesh.vertices @ rotation.as_matrix().T\n        z1 = np.percentile(vertices[:, 2], 1)\n        z2 = np.percentile(vertices[:, 2], 99)\n\n        if object_mapping[node] == Scene3DItemEnum.CONTEXT.value:\n            object_quat = [0, 0, 0, 1]\n            if rotate_context:\n                angle_rad = np.random.uniform(0, 2 * np.pi)\n                object_quat = compute_axis_rotation_quat(\n                    axis=\"z\", angle_rad=angle_rad\n                )\n                rotation = R.from_quat(object_quat).as_matrix()\n                vertices = vertices @ rotation.T\n\n            mesh_info[node][\"surface\"] = compute_convex_hull_path(vertices)\n\n            # Put robot in the CONTEXT edge.\n            x, y = random.choice(mesh_info[node][\"surface\"].vertices)\n            theta = np.arctan2(y, x)\n            quat_initial = Quaternion(axis=[0, 0, 1], angle=theta)\n            quat_extra = Quaternion(axis=[0, 0, 1], angle=np.pi)\n            quat = quat_extra * quat_initial\n            _pose = [x, y, z2 - z1, quat.x, quat.y, quat.z, quat.w]\n            position[robot_node] = [round(v, 4) for v in _pose]\n            node_box = [\n                x - robot_dim / 2,\n                x + robot_dim / 2,\n                y - robot_dim / 2,\n                y + robot_dim / 2,\n            ]\n            placed_boxes_map[node].append(node_box)\n        elif rotate_objs:\n            # For manipulated and distractor objects, apply random rotation\n            angle_rad = np.random.uniform(0, 2 * np.pi)\n            object_quat = compute_axis_rotation_quat(\n                axis=\"z\", angle_rad=angle_rad\n            )\n            rotation = R.from_quat(object_quat).as_matrix()\n            vertices = vertices @ rotation.T\n\n        x1, x2, y1, y2 = compute_xy_bbox(vertices)\n        mesh_info[node][\"pose\"] = [x1, x2, y1, y2, z1, z2, *object_quat]\n        mesh_info[node][\"area\"] = max(1e-5, (x2 - x1) * (y2 - y1))\n\n    root = list(layout_info.tree.keys())[0]\n    queue = deque([((root, None), layout_info.tree.get(root, []))])\n    while queue:\n        (node, relation), children = queue.popleft()\n        if node not in object_mapping:\n            continue\n\n        if object_mapping[node] == Scene3DItemEnum.BACKGROUND.value:\n            position[node] = [0, 0, floor_margin, *bg_quat]\n        else:\n            x1, x2, y1, y2, z1, z2, qx, qy, qz, qw = mesh_info[node][\"pose\"]\n            if object_mapping[node] == Scene3DItemEnum.CONTEXT.value:\n                position[node] = [0, 0, -round(z1, 4), qx, qy, qz, qw]\n                parent_bbox_xy[node] = [x1, x2, y1, y2, z1, z2]\n            elif object_mapping[node] in [\n                Scene3DItemEnum.MANIPULATED_OBJS.value,\n                Scene3DItemEnum.DISTRACTOR_OBJS.value,\n            ]:\n                parent_node = find_parent_node(node, layout_info.tree)\n                parent_pos = position[parent_node]\n                (\n                    p_x1,\n                    p_x2,\n                    p_y1,\n                    p_y2,\n                    p_z1,\n                    p_z2,\n                ) = parent_bbox_xy[parent_node]\n\n                obj_dx = x2 - x1\n                obj_dy = y2 - y1\n                hull_path = mesh_info[parent_node].get(\"surface\")\n                for _ in range(max_attempts):\n                    node_x1 = random.uniform(p_x1, p_x2 - obj_dx)\n                    node_y1 = random.uniform(p_y1, p_y2 - obj_dy)\n                    node_box = [\n                        node_x1,\n                        node_x1 + obj_dx,\n                        node_y1,\n                        node_y1 + obj_dy,\n                    ]\n                    if hull_path and not all_corners_inside(\n                        hull_path, node_box\n                    ):\n                        continue\n                    # Make sure the manipulated object is reachable by robot.\n                    if (\n                        limit_reach_range is not None\n                        and object_mapping[node]\n                        == Scene3DItemEnum.MANIPULATED_OBJS.value\n                    ):\n                        cx = parent_pos[0] + node_box[0] + obj_dx / 2\n                        cy = parent_pos[1] + node_box[2] + obj_dy / 2\n                        cz = parent_pos[2] + p_z2 - z1\n                        robot_pos = position[robot_node][:3]\n                        if not check_reachable(\n                            base_xyz=np.array(robot_pos),\n                            reach_xyz=np.array([cx, cy, cz]),\n                            min_reach=limit_reach_range[0],\n                            max_reach=limit_reach_range[1],\n                        ):\n                            continue\n\n                    # Make sure the manipulated object is inside the robot's orientation.\n                    if (\n                        max_orient_diff is not None\n                        and object_mapping[node]\n                        == Scene3DItemEnum.MANIPULATED_OBJS.value\n                    ):\n                        cx = parent_pos[0] + node_box[0] + obj_dx / 2\n                        cy = parent_pos[1] + node_box[2] + obj_dy / 2\n                        cx2, cy2 = position[robot_node][:2]\n                        v1 = np.array([-cx2, -cy2])\n                        v2 = np.array([cx - cx2, cy - cy2])\n                        dot = np.dot(v1, v2)\n                        norms = np.linalg.norm(v1) * np.linalg.norm(v2)\n                        theta = np.arccos(np.clip(dot / norms, -1.0, 1.0))\n                        theta = np.rad2deg(theta)\n                        if theta &gt; max_orient_diff:\n                            continue\n\n                    if not has_iou_conflict(\n                        node_box, placed_boxes_map[parent_node]\n                    ):\n                        z_offset = 0\n                        break\n                else:\n                    logger.warning(\n                        f\"Cannot place {node} on {parent_node} without overlap\"\n                        f\" after {max_attempts} attempts, place beside {parent_node}.\"\n                    )\n                    for _ in range(max_attempts):\n                        node_x1 = random.choice(\n                            [\n                                random.uniform(\n                                    p_x1 - obj_dx - beside_margin,\n                                    p_x1 - obj_dx,\n                                ),\n                                random.uniform(p_x2, p_x2 + beside_margin),\n                            ]\n                        )\n                        node_y1 = random.choice(\n                            [\n                                random.uniform(\n                                    p_y1 - obj_dy - beside_margin,\n                                    p_y1 - obj_dy,\n                                ),\n                                random.uniform(p_y2, p_y2 + beside_margin),\n                            ]\n                        )\n                        node_box = [\n                            node_x1,\n                            node_x1 + obj_dx,\n                            node_y1,\n                            node_y1 + obj_dy,\n                        ]\n                        z_offset = -(parent_pos[2] + p_z2)\n                        if not has_iou_conflict(\n                            node_box, placed_boxes_map[parent_node]\n                        ):\n                            break\n\n                placed_boxes_map[parent_node].append(node_box)\n\n                abs_cx = parent_pos[0] + node_box[0] + obj_dx / 2\n                abs_cy = parent_pos[1] + node_box[2] + obj_dy / 2\n                abs_cz = parent_pos[2] + p_z2 - z1 + z_offset\n                position[node] = [\n                    round(v, 4)\n                    for v in [abs_cx, abs_cy, abs_cz, qx, qy, qz, qw]\n                ]\n                parent_bbox_xy[node] = [x1, x2, y1, y2, z1, z2]\n\n        sorted_children = sorted(\n            children, key=lambda x: -mesh_info[x[0]].get(\"area\", 0)\n        )\n        for child, rel in sorted_children:\n            queue.append(((child, rel), layout_info.tree.get(child, [])))\n\n    layout_info.position = position\n\n    return layout_info\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.check_reachable","title":"check_reachable","text":"<pre><code>check_reachable(base_xyz: ndarray, reach_xyz: ndarray, min_reach: float = 0.25, max_reach: float = 0.85) -&gt; bool\n</code></pre> <p>Checks if the target point is within the reachable range.</p> <p>Parameters:</p> Name Type Description Default <code>base_xyz</code> <code>ndarray</code> <p>Base position.</p> required <code>reach_xyz</code> <code>ndarray</code> <p>Target position.</p> required <code>min_reach</code> <code>float</code> <p>Minimum reach distance.</p> <code>0.25</code> <code>max_reach</code> <code>float</code> <p>Maximum reach distance.</p> <code>0.85</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if reachable, False otherwise.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def check_reachable(\n    base_xyz: np.ndarray,\n    reach_xyz: np.ndarray,\n    min_reach: float = 0.25,\n    max_reach: float = 0.85,\n) -&gt; bool:\n    \"\"\"Checks if the target point is within the reachable range.\n\n    Args:\n        base_xyz (np.ndarray): Base position.\n        reach_xyz (np.ndarray): Target position.\n        min_reach (float, optional): Minimum reach distance.\n        max_reach (float, optional): Maximum reach distance.\n\n    Returns:\n        bool: True if reachable, False otherwise.\n    \"\"\"\n    distance = np.linalg.norm(reach_xyz - base_xyz)\n\n    return min_reach &lt; distance &lt; max_reach\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.compose_mesh_scene","title":"compose_mesh_scene","text":"<pre><code>compose_mesh_scene(layout_info: LayoutInfo, out_scene_path: str, with_bg: bool = False) -&gt; None\n</code></pre> <p>Composes a mesh scene from layout information and saves to file.</p> <p>Parameters:</p> Name Type Description Default <code>layout_info</code> <code>LayoutInfo</code> <p>Layout information.</p> required <code>out_scene_path</code> <code>str</code> <p>Output scene file path.</p> required <code>with_bg</code> <code>bool</code> <p>Include background mesh.</p> <code>False</code> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def compose_mesh_scene(\n    layout_info: LayoutInfo, out_scene_path: str, with_bg: bool = False\n) -&gt; None:\n    \"\"\"Composes a mesh scene from layout information and saves to file.\n\n    Args:\n        layout_info (LayoutInfo): Layout information.\n        out_scene_path (str): Output scene file path.\n        with_bg (bool, optional): Include background mesh.\n    \"\"\"\n    object_mapping = Scene3DItemEnum.object_mapping(layout_info.relation)\n    scene = trimesh.Scene()\n    for node in layout_info.assets:\n        if object_mapping[node] == Scene3DItemEnum.BACKGROUND.value:\n            mesh_path = f\"{layout_info.assets[node]}/mesh_model.ply\"\n            if not with_bg:\n                continue\n        else:\n            mesh_path = (\n                f\"{layout_info.assets[node]}/mesh/{node.replace(' ', '_')}.obj\"\n            )\n\n        mesh = trimesh.load(mesh_path)\n        offset = np.array(layout_info.position[node])[[0, 2, 1]]\n        mesh.vertices += offset\n        scene.add_geometry(mesh, node_name=node)\n\n    os.makedirs(os.path.dirname(out_scene_path), exist_ok=True)\n    scene.export(out_scene_path)\n    logger.info(f\"Composed interactive 3D layout saved in {out_scene_path}\")\n\n    return\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.compute_axis_rotation_quat","title":"compute_axis_rotation_quat","text":"<pre><code>compute_axis_rotation_quat(axis: Literal['x', 'y', 'z'], angle_rad: float) -&gt; list[float]\n</code></pre> <p>Computes quaternion for rotation around a given axis.</p> <p>Parameters:</p> Name Type Description Default <code>axis</code> <code>Literal['x', 'y', 'z']</code> <p>Axis of rotation.</p> required <code>angle_rad</code> <code>float</code> <p>Rotation angle in radians.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Quaternion [x, y, z, w].</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def compute_axis_rotation_quat(\n    axis: Literal[\"x\", \"y\", \"z\"], angle_rad: float\n) -&gt; list[float]:\n    \"\"\"Computes quaternion for rotation around a given axis.\n\n    Args:\n        axis (Literal[\"x\", \"y\", \"z\"]): Axis of rotation.\n        angle_rad (float): Rotation angle in radians.\n\n    Returns:\n        list[float]: Quaternion [x, y, z, w].\n    \"\"\"\n    if axis.lower() == \"x\":\n        q = Quaternion(axis=[1, 0, 0], angle=angle_rad)\n    elif axis.lower() == \"y\":\n        q = Quaternion(axis=[0, 1, 0], angle=angle_rad)\n    elif axis.lower() == \"z\":\n        q = Quaternion(axis=[0, 0, 1], angle=angle_rad)\n    else:\n        raise ValueError(f\"Unsupported axis '{axis}', must be one of x, y, z\")\n\n    return [q.x, q.y, q.z, q.w]\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.compute_convex_hull_path","title":"compute_convex_hull_path","text":"<pre><code>compute_convex_hull_path(vertices: ndarray, z_threshold: float = 0.05, interp_per_edge: int = 10, margin: float = -0.02, x_axis: int = 0, y_axis: int = 1, z_axis: int = 2) -&gt; Path\n</code></pre> <p>Computes a dense convex hull path for the top surface of a mesh.</p> <p>Parameters:</p> Name Type Description Default <code>vertices</code> <code>ndarray</code> <p>Mesh vertices.</p> required <code>z_threshold</code> <code>float</code> <p>Z threshold for top surface.</p> <code>0.05</code> <code>interp_per_edge</code> <code>int</code> <p>Interpolation points per edge.</p> <code>10</code> <code>margin</code> <code>float</code> <p>Margin for polygon buffer.</p> <code>-0.02</code> <code>x_axis</code> <code>int</code> <p>X axis index.</p> <code>0</code> <code>y_axis</code> <code>int</code> <p>Y axis index.</p> <code>1</code> <code>z_axis</code> <code>int</code> <p>Z axis index.</p> <code>2</code> <p>Returns:</p> Name Type Description <code>Path</code> <code>Path</code> <p>Matplotlib path object for the convex hull.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def compute_convex_hull_path(\n    vertices: np.ndarray,\n    z_threshold: float = 0.05,\n    interp_per_edge: int = 10,\n    margin: float = -0.02,\n    x_axis: int = 0,\n    y_axis: int = 1,\n    z_axis: int = 2,\n) -&gt; Path:\n    \"\"\"Computes a dense convex hull path for the top surface of a mesh.\n\n    Args:\n        vertices (np.ndarray): Mesh vertices.\n        z_threshold (float, optional): Z threshold for top surface.\n        interp_per_edge (int, optional): Interpolation points per edge.\n        margin (float, optional): Margin for polygon buffer.\n        x_axis (int, optional): X axis index.\n        y_axis (int, optional): Y axis index.\n        z_axis (int, optional): Z axis index.\n\n    Returns:\n        Path: Matplotlib path object for the convex hull.\n    \"\"\"\n    top_vertices = vertices[\n        vertices[:, z_axis] &gt; vertices[:, z_axis].max() - z_threshold\n    ]\n    top_xy = top_vertices[:, [x_axis, y_axis]]\n\n    if len(top_xy) &lt; 3:\n        raise ValueError(\"Not enough points to form a convex hull\")\n\n    hull = ConvexHull(top_xy)\n    hull_points = top_xy[hull.vertices]\n\n    polygon = Polygon(hull_points)\n    polygon = polygon.buffer(margin)\n    hull_points = np.array(polygon.exterior.coords)\n\n    dense_points = []\n    for i in range(len(hull_points)):\n        p1 = hull_points[i]\n        p2 = hull_points[(i + 1) % len(hull_points)]\n        for t in np.linspace(0, 1, interp_per_edge, endpoint=False):\n            pt = (1 - t) * p1 + t * p2\n            dense_points.append(pt)\n\n    return Path(np.array(dense_points), closed=True)\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.compute_pinhole_intrinsics","title":"compute_pinhole_intrinsics","text":"<pre><code>compute_pinhole_intrinsics(image_w: int, image_h: int, fov_deg: float) -&gt; np.ndarray\n</code></pre> <p>Computes pinhole camera intrinsic matrix from image size and FOV.</p> <p>Parameters:</p> Name Type Description Default <code>image_w</code> <code>int</code> <p>Image width.</p> required <code>image_h</code> <code>int</code> <p>Image height.</p> required <code>fov_deg</code> <code>float</code> <p>Field of view in degrees.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Intrinsic matrix K.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def compute_pinhole_intrinsics(\n    image_w: int, image_h: int, fov_deg: float\n) -&gt; np.ndarray:\n    \"\"\"Computes pinhole camera intrinsic matrix from image size and FOV.\n\n    Args:\n        image_w (int): Image width.\n        image_h (int): Image height.\n        fov_deg (float): Field of view in degrees.\n\n    Returns:\n        np.ndarray: Intrinsic matrix K.\n    \"\"\"\n    fov_rad = np.deg2rad(fov_deg)\n    fx = image_w / (2 * np.tan(fov_rad / 2))\n    fy = fx  # assuming square pixels\n    cx = image_w / 2\n    cy = image_h / 2\n    K = np.array([[fx, 0, cx], [0, fy, cy], [0, 0, 1]])\n\n    return K\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.compute_xy_bbox","title":"compute_xy_bbox","text":"<pre><code>compute_xy_bbox(vertices: ndarray, col_x: int = 0, col_y: int = 1) -&gt; list[float]\n</code></pre> <p>Computes the bounding box in XY plane for given vertices.</p> <p>Parameters:</p> Name Type Description Default <code>vertices</code> <code>ndarray</code> <p>Vertex coordinates.</p> required <code>col_x</code> <code>int</code> <p>Column index for X.</p> <code>0</code> <code>col_y</code> <code>int</code> <p>Column index for Y.</p> <code>1</code> <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: [min_x, max_x, min_y, max_y]</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def compute_xy_bbox(\n    vertices: np.ndarray, col_x: int = 0, col_y: int = 1\n) -&gt; list[float]:\n    \"\"\"Computes the bounding box in XY plane for given vertices.\n\n    Args:\n        vertices (np.ndarray): Vertex coordinates.\n        col_x (int, optional): Column index for X.\n        col_y (int, optional): Column index for Y.\n\n    Returns:\n        list[float]: [min_x, max_x, min_y, max_y]\n    \"\"\"\n    x_vals = vertices[:, col_x]\n    y_vals = vertices[:, col_y]\n    return x_vals.min(), x_vals.max(), y_vals.min(), y_vals.max()\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.find_parent_node","title":"find_parent_node","text":"<pre><code>find_parent_node(node: str, tree: dict) -&gt; str | None\n</code></pre> <p>Finds the parent node of a given node in a tree.</p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>str</code> <p>Node name.</p> required <code>tree</code> <code>dict</code> <p>Tree structure.</p> required <p>Returns:</p> Type Description <code>str | None</code> <p>str | None: Parent node name or None.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def find_parent_node(node: str, tree: dict) -&gt; str | None:\n    \"\"\"Finds the parent node of a given node in a tree.\n\n    Args:\n        node (str): Node name.\n        tree (dict): Tree structure.\n\n    Returns:\n        str | None: Parent node name or None.\n    \"\"\"\n    for parent, children in tree.items():\n        if any(child[0] == node for child in children):\n            return parent\n    return None\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.has_iou_conflict","title":"has_iou_conflict","text":"<pre><code>has_iou_conflict(new_box: list[float], placed_boxes: list[list[float]], iou_threshold: float = 0.0) -&gt; bool\n</code></pre> <p>Checks for intersection-over-union conflict between boxes.</p> <p>Parameters:</p> Name Type Description Default <code>new_box</code> <code>list[float]</code> <p>New box coordinates.</p> required <code>placed_boxes</code> <code>list[list[float]]</code> <p>List of placed box coordinates.</p> required <code>iou_threshold</code> <code>float</code> <p>IOU threshold.</p> <code>0.0</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if conflict exists, False otherwise.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def has_iou_conflict(\n    new_box: list[float],\n    placed_boxes: list[list[float]],\n    iou_threshold: float = 0.0,\n) -&gt; bool:\n    \"\"\"Checks for intersection-over-union conflict between boxes.\n\n    Args:\n        new_box (list[float]): New box coordinates.\n        placed_boxes (list[list[float]]): List of placed box coordinates.\n        iou_threshold (float, optional): IOU threshold.\n\n    Returns:\n        bool: True if conflict exists, False otherwise.\n    \"\"\"\n    new_min_x, new_max_x, new_min_y, new_max_y = new_box\n    for min_x, max_x, min_y, max_y in placed_boxes:\n        ix1 = max(new_min_x, min_x)\n        iy1 = max(new_min_y, min_y)\n        ix2 = min(new_max_x, max_x)\n        iy2 = min(new_max_y, max_y)\n        inter_area = max(0, ix2 - ix1) * max(0, iy2 - iy1)\n        if inter_area &gt; iou_threshold:\n            return True\n    return False\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.matrix_to_pose","title":"matrix_to_pose","text":"<pre><code>matrix_to_pose(matrix: ndarray) -&gt; list[float]\n</code></pre> <p>Converts a 4x4 transformation matrix to a pose (x, y, z, qx, qy, qz, qw).</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>4x4 transformation matrix.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Pose as [x, y, z, qx, qy, qz, qw].</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def matrix_to_pose(matrix: np.ndarray) -&gt; list[float]:\n    \"\"\"Converts a 4x4 transformation matrix to a pose (x, y, z, qx, qy, qz, qw).\n\n    Args:\n        matrix (np.ndarray): 4x4 transformation matrix.\n\n    Returns:\n        list[float]: Pose as [x, y, z, qx, qy, qz, qw].\n    \"\"\"\n    x, y, z = matrix[:3, 3]\n    rot_mat = matrix[:3, :3]\n    quat = R.from_matrix(rot_mat).as_quat()\n    qx, qy, qz, qw = quat\n\n    return [x, y, z, qx, qy, qz, qw]\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.pose_to_matrix","title":"pose_to_matrix","text":"<pre><code>pose_to_matrix(pose: list[float]) -&gt; np.ndarray\n</code></pre> <p>Converts pose (x, y, z, qx, qy, qz, qw) to a 4x4 transformation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>pose</code> <code>list[float]</code> <p>Pose as [x, y, z, qx, qy, qz, qw].</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: 4x4 transformation matrix.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def pose_to_matrix(pose: list[float]) -&gt; np.ndarray:\n    \"\"\"Converts pose (x, y, z, qx, qy, qz, qw) to a 4x4 transformation matrix.\n\n    Args:\n        pose (list[float]): Pose as [x, y, z, qx, qy, qz, qw].\n\n    Returns:\n        np.ndarray: 4x4 transformation matrix.\n    \"\"\"\n    x, y, z, qx, qy, qz, qw = pose\n    r = R.from_quat([qx, qy, qz, qw])\n    matrix = np.eye(4)\n    matrix[:3, :3] = r.as_matrix()\n    matrix[:3, 3] = [x, y, z]\n\n    return matrix\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.quaternion_multiply","title":"quaternion_multiply","text":"<pre><code>quaternion_multiply(init_quat: list[float], rotate_quat: list[float]) -&gt; list[float]\n</code></pre> <p>Multiplies two quaternions.</p> <p>Parameters:</p> Name Type Description Default <code>init_quat</code> <code>list[float]</code> <p>Initial quaternion [x, y, z, w].</p> required <code>rotate_quat</code> <code>list[float]</code> <p>Rotation quaternion [x, y, z, w].</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>list[float]: Resulting quaternion [x, y, z, w].</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def quaternion_multiply(\n    init_quat: list[float], rotate_quat: list[float]\n) -&gt; list[float]:\n    \"\"\"Multiplies two quaternions.\n\n    Args:\n        init_quat (list[float]): Initial quaternion [x, y, z, w].\n        rotate_quat (list[float]): Rotation quaternion [x, y, z, w].\n\n    Returns:\n        list[float]: Resulting quaternion [x, y, z, w].\n    \"\"\"\n    qx, qy, qz, qw = init_quat\n    q1 = Quaternion(w=qw, x=qx, y=qy, z=qz)\n    qx, qy, qz, qw = rotate_quat\n    q2 = Quaternion(w=qw, x=qx, y=qy, z=qz)\n    quat = q2 * q1\n\n    return [quat.x, quat.y, quat.z, quat.w]\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.geometry.with_seed","title":"with_seed","text":"<pre><code>with_seed(seed_attr_name: str = 'seed')\n</code></pre> <p>Decorator to temporarily set the random seed for reproducibility.</p> <p>Parameters:</p> Name Type Description Default <code>seed_attr_name</code> <code>str</code> <p>Name of the seed argument.</p> <code>'seed'</code> <p>Returns:</p> Name Type Description <code>function</code> <p>Decorator function.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def with_seed(seed_attr_name: str = \"seed\"):\n    \"\"\"Decorator to temporarily set the random seed for reproducibility.\n\n    Args:\n        seed_attr_name (str, optional): Name of the seed argument.\n\n    Returns:\n        function: Decorator function.\n    \"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            seed = kwargs.get(seed_attr_name, None)\n            if seed is not None:\n                py_state = random.getstate()\n                np_state = np.random.get_state()\n                torch_state = torch.get_rng_state()\n\n                random.seed(seed)\n                np.random.seed(seed)\n                torch.manual_seed(seed)\n                try:\n                    result = func(*args, **kwargs)\n                finally:\n                    random.setstate(py_state)\n                    np.random.set_state(np_state)\n                    torch.set_rng_state(torch_state)\n                return result\n            else:\n                return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.gaussian","title":"embodied_gen.utils.gaussian","text":""},{"location":"api/utils.html#embodied_gen.utils.gaussian.export_splats","title":"export_splats","text":"<pre><code>export_splats(means: Tensor, scales: Tensor, quats: Tensor, opacities: Tensor, sh0: Tensor, shN: Tensor, format: Literal['ply'] = 'ply', save_to: Optional[str] = None) -&gt; bytes\n</code></pre> <p>Export a Gaussian Splats model to bytes in PLY file format.</p> Source code in <code>embodied_gen/utils/gaussian.py</code> <pre><code>def export_splats(\n    means: torch.Tensor,\n    scales: torch.Tensor,\n    quats: torch.Tensor,\n    opacities: torch.Tensor,\n    sh0: torch.Tensor,\n    shN: torch.Tensor,\n    format: Literal[\"ply\"] = \"ply\",\n    save_to: Optional[str] = None,\n) -&gt; bytes:\n    \"\"\"Export a Gaussian Splats model to bytes in PLY file format.\"\"\"\n    total_splats = means.shape[0]\n    assert means.shape == (total_splats, 3), \"Means must be of shape (N, 3)\"\n    assert scales.shape == (total_splats, 3), \"Scales must be of shape (N, 3)\"\n    assert quats.shape == (\n        total_splats,\n        4,\n    ), \"Quaternions must be of shape (N, 4)\"\n    assert opacities.shape == (\n        total_splats,\n    ), \"Opacities must be of shape (N,)\"\n    assert sh0.shape == (total_splats, 1, 3), \"sh0 must be of shape (N, 1, 3)\"\n    assert (\n        shN.ndim == 3 and shN.shape[0] == total_splats and shN.shape[2] == 3\n    ), f\"shN must be of shape (N, K, 3), got {shN.shape}\"\n\n    # Reshape spherical harmonics\n    sh0 = sh0.squeeze(1)  # Shape (N, 3)\n    shN = shN.permute(0, 2, 1).reshape(means.shape[0], -1)  # Shape (N, K * 3)\n\n    # Check for NaN or Inf values\n    invalid_mask = (\n        torch.isnan(means).any(dim=1)\n        | torch.isinf(means).any(dim=1)\n        | torch.isnan(scales).any(dim=1)\n        | torch.isinf(scales).any(dim=1)\n        | torch.isnan(quats).any(dim=1)\n        | torch.isinf(quats).any(dim=1)\n        | torch.isnan(opacities).any(dim=0)\n        | torch.isinf(opacities).any(dim=0)\n        | torch.isnan(sh0).any(dim=1)\n        | torch.isinf(sh0).any(dim=1)\n        | torch.isnan(shN).any(dim=1)\n        | torch.isinf(shN).any(dim=1)\n    )\n\n    # Filter out invalid entries\n    valid_mask = ~invalid_mask\n    means = means[valid_mask]\n    scales = scales[valid_mask]\n    quats = quats[valid_mask]\n    opacities = opacities[valid_mask]\n    sh0 = sh0[valid_mask]\n    shN = shN[valid_mask]\n\n    if format == \"ply\":\n        data = splat2ply_bytes(means, scales, quats, opacities, sh0, shN)\n    else:\n        raise ValueError(f\"Unsupported format: {format}\")\n\n    if save_to:\n        with open(save_to, \"wb\") as binary_file:\n            binary_file.write(data)\n\n    return data\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.gaussian.restore_scene_scale_and_position","title":"restore_scene_scale_and_position","text":"<pre><code>restore_scene_scale_and_position(real_height: float, mesh_path: str, gs_path: str) -&gt; None\n</code></pre> <p>Scales a mesh and corresponding GS model to match a given real-world height.</p> <p>Uses the 1st and 99th percentile of mesh Z-axis to estimate height, applies scaling and vertical alignment, and updates both the mesh and GS model.</p> <p>Parameters:</p> Name Type Description Default <code>real_height</code> <code>float</code> <p>Target real-world height among Z axis.</p> required <code>mesh_path</code> <code>str</code> <p>Path to the input mesh file.</p> required <code>gs_path</code> <code>str</code> <p>Path to the Gaussian Splatting model file.</p> required Source code in <code>embodied_gen/utils/gaussian.py</code> <pre><code>def restore_scene_scale_and_position(\n    real_height: float, mesh_path: str, gs_path: str\n) -&gt; None:\n    \"\"\"Scales a mesh and corresponding GS model to match a given real-world height.\n\n    Uses the 1st and 99th percentile of mesh Z-axis to estimate height,\n    applies scaling and vertical alignment, and updates both the mesh and GS model.\n\n    Args:\n        real_height (float): Target real-world height among Z axis.\n        mesh_path (str): Path to the input mesh file.\n        gs_path (str): Path to the Gaussian Splatting model file.\n    \"\"\"\n    mesh = trimesh.load(mesh_path)\n    z_min = np.percentile(mesh.vertices[:, 1], 1)\n    z_max = np.percentile(mesh.vertices[:, 1], 99)\n    height = z_max - z_min\n    scale = real_height / height\n\n    rot = Rotation.from_quat([0, 1, 0, 0])\n    mesh.vertices = rot.apply(mesh.vertices)\n    mesh.vertices[:, 1] -= z_min\n    mesh.vertices *= scale\n    mesh.export(mesh_path)\n\n    gs_model: GaussianOperator = GaussianOperator.load_from_ply(gs_path)\n    gs_model = gs_model.get_gaussians(\n        instance_pose=torch.tensor([0.0, -z_min, 0, 0, 1, 0, 0])\n    )\n    gs_model.rescale(scale)\n    gs_model.save_to_ply(gs_path)\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.gpt_clients","title":"embodied_gen.utils.gpt_clients","text":""},{"location":"api/utils.html#embodied_gen.utils.gpt_clients.GPTclient","title":"GPTclient","text":"<pre><code>GPTclient(endpoint: str, api_key: str, model_name: str = 'yfb-gpt-4o', api_version: str = None, check_connection: bool = True, verbose: bool = False)\n</code></pre> <p>A client to interact with GPT models via OpenAI or Azure API.</p> <p>Supports text and image prompts, connection checking, and configurable parameters.</p> <p>Parameters:</p> Name Type Description Default <code>endpoint</code> <code>str</code> <p>API endpoint URL.</p> required <code>api_key</code> <code>str</code> <p>API key for authentication.</p> required <code>model_name</code> <code>str</code> <p>Model name to use.</p> <code>'yfb-gpt-4o'</code> <code>api_version</code> <code>str</code> <p>API version (for Azure).</p> <code>None</code> <code>check_connection</code> <code>bool</code> <p>Whether to check API connection.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Enable verbose logging.</p> <code>False</code> Example <p><pre><code>export ENDPOINT=\"https://yfb-openai-sweden.openai.azure.com\"\nexport API_KEY=\"xxxxxx\"\nexport API_VERSION=\"2025-03-01-preview\"\nexport MODEL_NAME=\"yfb-gpt-4o-sweden\"\n</code></pre> <pre><code>from embodied_gen.utils.gpt_clients import GPT_CLIENT\n\nresponse = GPT_CLIENT.query(\"Describe the physics of a falling apple.\")\nresponse = GPT_CLIENT.query(\n    text_prompt=\"Describe the content in each image.\"\n    image_base64=[\"path/to/image1.png\", \"path/to/image2.jpg\"],\n)\n</code></pre></p> Source code in <code>embodied_gen/utils/gpt_clients.py</code> <pre><code>def __init__(\n    self,\n    endpoint: str,\n    api_key: str,\n    model_name: str = \"yfb-gpt-4o\",\n    api_version: str = None,\n    check_connection: bool = True,\n    verbose: bool = False,\n):\n    if api_version is not None:\n        self.client = AzureOpenAI(\n            azure_endpoint=endpoint,\n            api_key=api_key,\n            api_version=api_version,\n        )\n    else:\n        self.client = OpenAI(\n            base_url=endpoint,\n            api_key=api_key,\n        )\n\n    self.endpoint = endpoint\n    self.model_name = model_name\n    self.image_formats = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".gif\"}\n    self.verbose = verbose\n    if check_connection:\n        self.check_connection()\n\n    logger.info(f\"Using GPT model: {self.model_name}.\")\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.gpt_clients.GPTclient.check_connection","title":"check_connection","text":"<pre><code>check_connection() -&gt; None\n</code></pre> <p>Checks whether the GPT API connection is working.</p> <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If connection fails.</p> Source code in <code>embodied_gen/utils/gpt_clients.py</code> <pre><code>def check_connection(self) -&gt; None:\n    \"\"\"Checks whether the GPT API connection is working.\n\n    Raises:\n        ConnectionError: If connection fails.\n    \"\"\"\n    try:\n        response = self.completion_with_backoff(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a test system.\"},\n                {\"role\": \"user\", \"content\": \"Hello\"},\n            ],\n            model=self.model_name,\n            temperature=0,\n            max_tokens=100,\n        )\n        content = response.choices[0].message.content\n        logger.info(f\"Connection check success.\")\n    except Exception as e:\n        raise ConnectionError(\n            f\"Failed to connect to GPT API at {self.endpoint}, \"\n            f\"please check setting in `{CONFIG_FILE}` and `README`.\"\n        )\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.gpt_clients.GPTclient.completion_with_backoff","title":"completion_with_backoff","text":"<pre><code>completion_with_backoff(**kwargs)\n</code></pre> <p>Performs a chat completion request with retry/backoff.</p> Source code in <code>embodied_gen/utils/gpt_clients.py</code> <pre><code>@retry(\n    retry=retry_if_not_exception_type(openai.BadRequestError),\n    wait=wait_random_exponential(min=1, max=10),\n    stop=stop_after_attempt(5),\n)\ndef completion_with_backoff(self, **kwargs):\n    \"\"\"Performs a chat completion request with retry/backoff.\"\"\"\n    return self.client.chat.completions.create(**kwargs)\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.gpt_clients.GPTclient.query","title":"query","text":"<pre><code>query(text_prompt: str, image_base64: Optional[list[str | Image]] = None, system_role: Optional[str] = None, params: Optional[dict] = None) -&gt; Optional[str]\n</code></pre> <p>Queries the GPT model with text and optional image prompts.</p> <p>Parameters:</p> Name Type Description Default <code>text_prompt</code> <code>str</code> <p>Main text input.</p> required <code>image_base64</code> <code>Optional[list[str | Image]]</code> <p>List of image base64 strings, file paths, or PIL Images.</p> <code>None</code> <code>system_role</code> <code>Optional[str]</code> <p>System-level instructions.</p> <code>None</code> <code>params</code> <code>Optional[dict]</code> <p>Additional GPT parameters.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: Model response content, or None if error.</p> Source code in <code>embodied_gen/utils/gpt_clients.py</code> <pre><code>def query(\n    self,\n    text_prompt: str,\n    image_base64: Optional[list[str | Image.Image]] = None,\n    system_role: Optional[str] = None,\n    params: Optional[dict] = None,\n) -&gt; Optional[str]:\n    \"\"\"Queries the GPT model with text and optional image prompts.\n\n    Args:\n        text_prompt (str): Main text input.\n        image_base64 (Optional[list[str | Image.Image]], optional): List of image base64 strings, file paths, or PIL Images.\n        system_role (Optional[str], optional): System-level instructions.\n        params (Optional[dict], optional): Additional GPT parameters.\n\n    Returns:\n        Optional[str]: Model response content, or None if error.\n    \"\"\"\n    if system_role is None:\n        system_role = \"You are a highly knowledgeable assistant specializing in physics, engineering, and object properties.\"  # noqa\n\n    content_user = [\n        {\n            \"type\": \"text\",\n            \"text\": text_prompt,\n        },\n    ]\n\n    # Process images if provided\n    if image_base64 is not None:\n        if not isinstance(image_base64, list):\n            image_base64 = [image_base64]\n        # Hardcode tmp because of the openrouter can't input multi images.\n        if \"openrouter\" in self.endpoint:\n            image_base64 = combine_images_to_grid(image_base64)\n        for img in image_base64:\n            if isinstance(img, Image.Image):\n                buffer = BytesIO()\n                img.save(buffer, format=img.format or \"PNG\")\n                buffer.seek(0)\n                image_binary = buffer.read()\n                img = base64.b64encode(image_binary).decode(\"utf-8\")\n            elif (\n                len(os.path.splitext(img)) &gt; 1\n                and os.path.splitext(img)[-1].lower() in self.image_formats\n            ):\n                if not os.path.exists(img):\n                    raise FileNotFoundError(f\"Image file not found: {img}\")\n                with open(img, \"rb\") as f:\n                    img = base64.b64encode(f.read()).decode(\"utf-8\")\n\n            content_user.append(\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/png;base64,{img}\"},\n                }\n            )\n\n    payload = {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_role},\n            {\"role\": \"user\", \"content\": content_user},\n        ],\n        \"temperature\": 0.1,\n        \"max_tokens\": 500,\n        \"top_p\": 0.1,\n        \"frequency_penalty\": 0,\n        \"presence_penalty\": 0,\n        \"stop\": None,\n        \"model\": self.model_name,\n    }\n\n    if params:\n        payload.update(params)\n\n    response = None\n    try:\n        response = self.completion_with_backoff(**payload)\n        response = response.choices[0].message.content\n    except Exception as e:\n        logger.error(f\"Error GPTclint {self.endpoint} API call: {e}\")\n        response = None\n\n    if self.verbose:\n        logger.info(f\"Prompt: {text_prompt}\")\n        logger.info(f\"Response: {response}\")\n\n    return response\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media","title":"embodied_gen.utils.process_media","text":""},{"location":"api/utils.html#embodied_gen.utils.process_media.SceneTreeVisualizer","title":"SceneTreeVisualizer","text":"<pre><code>SceneTreeVisualizer(layout_info: LayoutInfo)\n</code></pre> <p>Visualizes a scene tree layout using networkx and matplotlib.</p> <p>Parameters:</p> Name Type Description Default <code>layout_info</code> <code>LayoutInfo</code> <p>Layout information for the scene.</p> required Example <pre><code>from embodied_gen.utils.process_media import SceneTreeVisualizer\nvisualizer = SceneTreeVisualizer(layout_info)\nvisualizer.render(save_path=\"tree.png\")\n</code></pre> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def __init__(self, layout_info: LayoutInfo) -&gt; None:\n    self.tree = layout_info.tree\n    self.relation = layout_info.relation\n    self.objs_desc = layout_info.objs_desc\n    self.G = nx.DiGraph()\n    self.root = self._find_root()\n    self._build_graph()\n\n    self.role_colors = {\n        Scene3DItemEnum.BACKGROUND.value: \"plum\",\n        Scene3DItemEnum.CONTEXT.value: \"lightblue\",\n        Scene3DItemEnum.ROBOT.value: \"lightcoral\",\n        Scene3DItemEnum.MANIPULATED_OBJS.value: \"lightgreen\",\n        Scene3DItemEnum.DISTRACTOR_OBJS.value: \"lightgray\",\n        Scene3DItemEnum.OTHERS.value: \"orange\",\n    }\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.SceneTreeVisualizer.render","title":"render","text":"<pre><code>render(save_path: str, figsize=(8, 6), dpi=300, title: str = 'Scene 3D Hierarchy Tree')\n</code></pre> <p>Renders the scene tree and saves to file.</p> <p>Parameters:</p> Name Type Description Default <code>save_path</code> <code>str</code> <p>Path to save the rendered image.</p> required <code>figsize</code> <code>tuple</code> <p>Figure size.</p> <code>(8, 6)</code> <code>dpi</code> <code>int</code> <p>Image DPI.</p> <code>300</code> <code>title</code> <code>str</code> <p>Plot image title.</p> <code>'Scene 3D Hierarchy Tree'</code> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def render(\n    self,\n    save_path: str,\n    figsize=(8, 6),\n    dpi=300,\n    title: str = \"Scene 3D Hierarchy Tree\",\n):\n    \"\"\"Renders the scene tree and saves to file.\n\n    Args:\n        save_path (str): Path to save the rendered image.\n        figsize (tuple, optional): Figure size.\n        dpi (int, optional): Image DPI.\n        title (str, optional): Plot image title.\n    \"\"\"\n    node_colors = [\n        self.role_colors[self._get_node_role(n)] for n in self.G.nodes\n    ]\n    pos = self._get_positions(self.root)\n\n    plt.figure(figsize=figsize)\n    nx.draw(\n        self.G,\n        pos,\n        with_labels=True,\n        arrows=False,\n        node_size=2000,\n        node_color=node_colors,\n        font_size=10,\n        font_weight=\"bold\",\n    )\n\n    # Draw edge labels\n    edge_labels = nx.get_edge_attributes(self.G, \"relation\")\n    nx.draw_networkx_edge_labels(\n        self.G,\n        pos,\n        edge_labels=edge_labels,\n        font_size=9,\n        font_color=\"black\",\n    )\n\n    # Draw small description text under each node (if available)\n    for node, (x, y) in pos.items():\n        desc = self.objs_desc.get(node)\n        if desc:\n            wrapped = \"\\n\".join(textwrap.wrap(desc, width=30))\n            plt.text(\n                x,\n                y - 0.006,\n                wrapped,\n                fontsize=6,\n                ha=\"center\",\n                va=\"top\",\n                wrap=True,\n                color=\"black\",\n                bbox=dict(\n                    facecolor=\"dimgray\",\n                    edgecolor=\"darkgray\",\n                    alpha=0.1,\n                    boxstyle=\"round,pad=0.2\",\n                ),\n            )\n\n    plt.title(title, fontsize=12)\n    task_desc = self.relation.get(\"task_desc\", \"\")\n    if task_desc:\n        plt.suptitle(\n            f\"Task Description: {task_desc}\", fontsize=10, y=0.999\n        )\n\n    plt.axis(\"off\")\n\n    legend_handles = [\n        Patch(facecolor=color, edgecolor=\"black\", label=role)\n        for role, color in self.role_colors.items()\n    ]\n    plt.legend(\n        handles=legend_handles,\n        loc=\"lower center\",\n        ncol=3,\n        bbox_to_anchor=(0.5, -0.1),\n        fontsize=9,\n    )\n\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n    plt.savefig(save_path, dpi=dpi, bbox_inches=\"tight\")\n    plt.close()\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.alpha_blend_rgba","title":"alpha_blend_rgba","text":"<pre><code>alpha_blend_rgba(fg_image: Union[str, Image, ndarray], bg_image: Union[str, Image, ndarray]) -&gt; Image.Image\n</code></pre> <p>Alpha blends a foreground RGBA image over a background RGBA image.</p> <p>Parameters:</p> Name Type Description Default <code>fg_image</code> <code>Union[str, Image, ndarray]</code> <p>Foreground image (str, PIL Image, or ndarray).</p> required <code>bg_image</code> <code>Union[str, Image, ndarray]</code> <p>Background image (str, PIL Image, or ndarray).</p> required <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Alpha-blended RGBA image.</p> Example <pre><code>from embodied_gen.utils.process_media import alpha_blend_rgba\nresult = alpha_blend_rgba(\"fg.png\", \"bg.png\")\nresult.save(\"blended.png\")\n</code></pre> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def alpha_blend_rgba(\n    fg_image: Union[str, Image.Image, np.ndarray],\n    bg_image: Union[str, Image.Image, np.ndarray],\n) -&gt; Image.Image:\n    \"\"\"Alpha blends a foreground RGBA image over a background RGBA image.\n\n    Args:\n        fg_image: Foreground image (str, PIL Image, or ndarray).\n        bg_image: Background image (str, PIL Image, or ndarray).\n\n    Returns:\n        Image.Image: Alpha-blended RGBA image.\n\n    Example:\n        ```py\n        from embodied_gen.utils.process_media import alpha_blend_rgba\n        result = alpha_blend_rgba(\"fg.png\", \"bg.png\")\n        result.save(\"blended.png\")\n        ```\n    \"\"\"\n    if isinstance(fg_image, str):\n        fg_image = Image.open(fg_image)\n    elif isinstance(fg_image, np.ndarray):\n        fg_image = Image.fromarray(fg_image)\n\n    if isinstance(bg_image, str):\n        bg_image = Image.open(bg_image)\n    elif isinstance(bg_image, np.ndarray):\n        bg_image = Image.fromarray(bg_image)\n\n    if fg_image.size != bg_image.size:\n        raise ValueError(\n            f\"Image sizes not match {fg_image.size} v.s. {bg_image.size}.\"\n        )\n\n    fg = fg_image.convert(\"RGBA\")\n    bg = bg_image.convert(\"RGBA\")\n\n    return Image.alpha_composite(bg, fg)\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.check_object_edge_truncated","title":"check_object_edge_truncated","text":"<pre><code>check_object_edge_truncated(mask: ndarray, edge_threshold: int = 5) -&gt; bool\n</code></pre> <p>Checks if a binary object mask is truncated at the image edges.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>2D binary mask.</p> required <code>edge_threshold</code> <code>int</code> <p>Edge pixel threshold.</p> <code>5</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if object is fully enclosed, False if truncated.</p> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def check_object_edge_truncated(\n    mask: np.ndarray, edge_threshold: int = 5\n) -&gt; bool:\n    \"\"\"Checks if a binary object mask is truncated at the image edges.\n\n    Args:\n        mask (np.ndarray): 2D binary mask.\n        edge_threshold (int, optional): Edge pixel threshold.\n\n    Returns:\n        bool: True if object is fully enclosed, False if truncated.\n    \"\"\"\n    top = mask[:edge_threshold, :].any()\n    bottom = mask[-edge_threshold:, :].any()\n    left = mask[:, :edge_threshold].any()\n    right = mask[:, -edge_threshold:].any()\n\n    return not (top or bottom or left or right)\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.combine_images_to_grid","title":"combine_images_to_grid","text":"<pre><code>combine_images_to_grid(images: list[str | Image], cat_row_col: tuple[int, int] = None, target_wh: tuple[int, int] = (512, 512), image_mode: str = 'RGB') -&gt; list[Image.Image]\n</code></pre> <p>Combines multiple images into a grid.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[str | Image]</code> <p>List of image paths or PIL Images.</p> required <code>cat_row_col</code> <code>tuple[int, int]</code> <p>Grid rows and columns.</p> <code>None</code> <code>target_wh</code> <code>tuple[int, int]</code> <p>Target image size.</p> <code>(512, 512)</code> <code>image_mode</code> <code>str</code> <p>Image mode.</p> <code>'RGB'</code> <p>Returns:</p> Type Description <code>list[Image]</code> <p>list[Image.Image]: List containing the grid image.</p> Example <pre><code>from embodied_gen.utils.process_media import combine_images_to_grid\ngrid = combine_images_to_grid([\"img1.png\", \"img2.png\"])\ngrid[0].save(\"grid.png\")\n</code></pre> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def combine_images_to_grid(\n    images: list[str | Image.Image],\n    cat_row_col: tuple[int, int] = None,\n    target_wh: tuple[int, int] = (512, 512),\n    image_mode: str = \"RGB\",\n) -&gt; list[Image.Image]:\n    \"\"\"Combines multiple images into a grid.\n\n    Args:\n        images (list[str | Image.Image]): List of image paths or PIL Images.\n        cat_row_col (tuple[int, int], optional): Grid rows and columns.\n        target_wh (tuple[int, int], optional): Target image size.\n        image_mode (str, optional): Image mode.\n\n    Returns:\n        list[Image.Image]: List containing the grid image.\n\n    Example:\n        ```py\n        from embodied_gen.utils.process_media import combine_images_to_grid\n        grid = combine_images_to_grid([\"img1.png\", \"img2.png\"])\n        grid[0].save(\"grid.png\")\n        ```\n    \"\"\"\n    n_images = len(images)\n    if n_images == 1:\n        return images\n\n    if cat_row_col is None:\n        n_col = math.ceil(math.sqrt(n_images))\n        n_row = math.ceil(n_images / n_col)\n    else:\n        n_row, n_col = cat_row_col\n\n    images = [\n        Image.open(p).convert(image_mode) if isinstance(p, str) else p\n        for p in images\n    ]\n    images = [img.resize(target_wh) for img in images]\n\n    grid_w, grid_h = n_col * target_wh[0], n_row * target_wh[1]\n    grid = Image.new(image_mode, (grid_w, grid_h), (0, 0, 0))\n\n    for idx, img in enumerate(images):\n        row, col = divmod(idx, n_col)\n        grid.paste(img, (col * target_wh[0], row * target_wh[1]))\n\n    return [grid]\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.filter_image_small_connected_components","title":"filter_image_small_connected_components","text":"<pre><code>filter_image_small_connected_components(image: Union[Image, ndarray], area_ratio: float = 10, connectivity: int = 8) -&gt; np.ndarray\n</code></pre> <p>Removes small connected components from the alpha channel of an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[Image, ndarray]</code> <p>Input image.</p> required <code>area_ratio</code> <code>float</code> <p>Minimum area ratio.</p> <code>10</code> <code>connectivity</code> <code>int</code> <p>Connectivity for labeling.</p> <code>8</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Image with filtered alpha channel.</p> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def filter_image_small_connected_components(\n    image: Union[Image.Image, np.ndarray],\n    area_ratio: float = 10,\n    connectivity: int = 8,\n) -&gt; np.ndarray:\n    \"\"\"Removes small connected components from the alpha channel of an image.\n\n    Args:\n        image (Union[Image.Image, np.ndarray]): Input image.\n        area_ratio (float, optional): Minimum area ratio.\n        connectivity (int, optional): Connectivity for labeling.\n\n    Returns:\n        np.ndarray: Image with filtered alpha channel.\n    \"\"\"\n    if isinstance(image, Image.Image):\n        image = image.convert(\"RGBA\")\n        image = np.array(image)\n\n    mask = image[..., 3]\n    mask = filter_small_connected_components(mask, area_ratio, connectivity)\n    image[..., 3] = mask\n\n    return image\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.filter_small_connected_components","title":"filter_small_connected_components","text":"<pre><code>filter_small_connected_components(mask: Union[Image, ndarray], area_ratio: float, connectivity: int = 8) -&gt; np.ndarray\n</code></pre> <p>Removes small connected components from a binary mask.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>Union[Image, ndarray]</code> <p>Input mask.</p> required <code>area_ratio</code> <code>float</code> <p>Minimum area ratio for components.</p> required <code>connectivity</code> <code>int</code> <p>Connectivity for labeling.</p> <code>8</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Mask with small components removed.</p> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def filter_small_connected_components(\n    mask: Union[Image.Image, np.ndarray],\n    area_ratio: float,\n    connectivity: int = 8,\n) -&gt; np.ndarray:\n    \"\"\"Removes small connected components from a binary mask.\n\n    Args:\n        mask (Union[Image.Image, np.ndarray]): Input mask.\n        area_ratio (float): Minimum area ratio for components.\n        connectivity (int, optional): Connectivity for labeling.\n\n    Returns:\n        np.ndarray: Mask with small components removed.\n    \"\"\"\n    if isinstance(mask, Image.Image):\n        mask = np.array(mask)\n    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(\n        mask,\n        connectivity=connectivity,\n    )\n\n    small_components = np.zeros_like(mask, dtype=np.uint8)\n    mask_area = (mask != 0).sum()\n    min_area = mask_area // area_ratio\n    for label in range(1, num_labels):\n        area = stats[label, cv2.CC_STAT_AREA]\n        if area &lt; min_area:\n            small_components[labels == label] = 255\n\n    mask = cv2.bitwise_and(mask, cv2.bitwise_not(small_components))\n\n    return mask\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.is_image_file","title":"is_image_file","text":"<pre><code>is_image_file(filename: str) -&gt; bool\n</code></pre> <p>Checks if a filename is an image file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>Filename to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if image file, False otherwise.</p> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def is_image_file(filename: str) -&gt; bool:\n    \"\"\"Checks if a filename is an image file.\n\n    Args:\n        filename (str): Filename to check.\n\n    Returns:\n        bool: True if image file, False otherwise.\n    \"\"\"\n    mime_type, _ = mimetypes.guess_type(filename)\n\n    return mime_type is not None and mime_type.startswith(\"image\")\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.load_scene_dict","title":"load_scene_dict","text":"<pre><code>load_scene_dict(file_path: str) -&gt; dict\n</code></pre> <p>Loads a scene description dictionary from a file.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the scene description file.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Mapping from scene ID to description.</p> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def load_scene_dict(file_path: str) -&gt; dict:\n    \"\"\"Loads a scene description dictionary from a file.\n\n    Args:\n        file_path (str): Path to the scene description file.\n\n    Returns:\n        dict: Mapping from scene ID to description.\n    \"\"\"\n    scene_dict = {}\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line or \":\" not in line:\n                continue\n            scene_id, desc = line.split(\":\", 1)\n            scene_dict[scene_id.strip()] = desc.strip()\n\n    return scene_dict\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.merge_images_video","title":"merge_images_video","text":"<pre><code>merge_images_video(color_images, normal_images, output_path) -&gt; None\n</code></pre> <p>Merges color and normal images into a video.</p> <p>Parameters:</p> Name Type Description Default <code>color_images</code> <code>list[ndarray]</code> <p>List of color images.</p> required <code>normal_images</code> <code>list[ndarray]</code> <p>List of normal images.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output video.</p> required Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def merge_images_video(color_images, normal_images, output_path) -&gt; None:\n    \"\"\"Merges color and normal images into a video.\n\n    Args:\n        color_images (list[np.ndarray]): List of color images.\n        normal_images (list[np.ndarray]): List of normal images.\n        output_path (str): Path to save the output video.\n    \"\"\"\n    width = color_images[0].shape[1]\n    combined_video = [\n        np.hstack([rgb_img[:, : width // 2], normal_img[:, width // 2 :]])\n        for rgb_img, normal_img in zip(color_images, normal_images)\n    ]\n    imageio.mimsave(output_path, combined_video, fps=50)\n\n    return\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.merge_video_video","title":"merge_video_video","text":"<pre><code>merge_video_video(video_path1: str, video_path2: str, output_path: str) -&gt; None\n</code></pre> <p>Merges two videos by combining their left and right halves.</p> <p>Parameters:</p> Name Type Description Default <code>video_path1</code> <code>str</code> <p>Path to first video.</p> required <code>video_path2</code> <code>str</code> <p>Path to second video.</p> required <code>output_path</code> <code>str</code> <p>Path to save the merged video.</p> required Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def merge_video_video(\n    video_path1: str, video_path2: str, output_path: str\n) -&gt; None:\n    \"\"\"Merges two videos by combining their left and right halves.\n\n    Args:\n        video_path1 (str): Path to first video.\n        video_path2 (str): Path to second video.\n        output_path (str): Path to save the merged video.\n    \"\"\"\n    clip1 = VideoFileClip(video_path1)\n    clip2 = VideoFileClip(video_path2)\n\n    if clip1.size != clip2.size:\n        raise ValueError(\"The resolutions of the two videos do not match.\")\n\n    width, height = clip1.size\n    clip1_half = clip1.crop(x1=0, y1=0, x2=width // 2, y2=height)\n    clip2_half = clip2.crop(x1=width // 2, y1=0, x2=width, y2=height)\n    final_clip = clips_array([[clip1_half, clip2_half]])\n    final_clip.write_videofile(output_path, codec=\"libx264\")\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.parse_text_prompts","title":"parse_text_prompts","text":"<pre><code>parse_text_prompts(prompts: list[str]) -&gt; list[str]\n</code></pre> <p>Parses text prompts from a list or file.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>list[str]</code> <p>List of prompts or a file path.</p> required <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of parsed prompts.</p> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def parse_text_prompts(prompts: list[str]) -&gt; list[str]:\n    \"\"\"Parses text prompts from a list or file.\n\n    Args:\n        prompts (list[str]): List of prompts or a file path.\n\n    Returns:\n        list[str]: List of parsed prompts.\n    \"\"\"\n    if len(prompts) == 1 and prompts[0].endswith(\".txt\"):\n        with open(prompts[0], \"r\") as f:\n            prompts = [\n                line.strip()\n                for line in f\n                if line.strip() and not line.strip().startswith(\"#\")\n            ]\n    return prompts\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.render_asset3d","title":"render_asset3d","text":"<pre><code>render_asset3d(mesh_path: str, output_root: str, distance: float = 5.0, num_images: int = 1, elevation: list[float] = (0.0,), pbr_light_factor: float = 1.2, return_key: str = 'image_color/*', output_subdir: str = 'renders', gen_color_mp4: bool = False, gen_viewnormal_mp4: bool = False, gen_glonormal_mp4: bool = False, no_index_file: bool = False, with_mtl: bool = True) -&gt; list[str]\n</code></pre> <p>Renders a 3D mesh asset and returns output image paths.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_path</code> <code>str</code> <p>Path to the mesh file.</p> required <code>output_root</code> <code>str</code> <p>Directory to save outputs.</p> required <code>distance</code> <code>float</code> <p>Camera distance.</p> <code>5.0</code> <code>num_images</code> <code>int</code> <p>Number of views to render.</p> <code>1</code> <code>elevation</code> <code>list[float]</code> <p>Camera elevation angles.</p> <code>(0.0,)</code> <code>pbr_light_factor</code> <code>float</code> <p>PBR lighting factor.</p> <code>1.2</code> <code>return_key</code> <code>str</code> <p>Glob pattern for output images.</p> <code>'image_color/*'</code> <code>output_subdir</code> <code>str</code> <p>Subdirectory for outputs.</p> <code>'renders'</code> <code>gen_color_mp4</code> <code>bool</code> <p>Generate color MP4 video.</p> <code>False</code> <code>gen_viewnormal_mp4</code> <code>bool</code> <p>Generate view normal MP4.</p> <code>False</code> <code>gen_glonormal_mp4</code> <code>bool</code> <p>Generate global normal MP4.</p> <code>False</code> <code>no_index_file</code> <code>bool</code> <p>Skip index file saving.</p> <code>False</code> <code>with_mtl</code> <code>bool</code> <p>Use mesh material.</p> <code>True</code> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: List of output image file paths.</p> Example <pre><code>from embodied_gen.utils.process_media import render_asset3d\n\nimage_paths = render_asset3d(\n    mesh_path=\"path_to_mesh.obj\",\n    output_root=\"path_to_save_dir\",\n    num_images=4,\n    elevation=(30, -30),\n    output_subdir=\"renders\",\n    no_index_file=True,\n)\n</code></pre> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>@spaces.GPU\ndef render_asset3d(\n    mesh_path: str,\n    output_root: str,\n    distance: float = 5.0,\n    num_images: int = 1,\n    elevation: list[float] = (0.0,),\n    pbr_light_factor: float = 1.2,\n    return_key: str = \"image_color/*\",\n    output_subdir: str = \"renders\",\n    gen_color_mp4: bool = False,\n    gen_viewnormal_mp4: bool = False,\n    gen_glonormal_mp4: bool = False,\n    no_index_file: bool = False,\n    with_mtl: bool = True,\n) -&gt; list[str]:\n    \"\"\"Renders a 3D mesh asset and returns output image paths.\n\n    Args:\n        mesh_path (str): Path to the mesh file.\n        output_root (str): Directory to save outputs.\n        distance (float, optional): Camera distance.\n        num_images (int, optional): Number of views to render.\n        elevation (list[float], optional): Camera elevation angles.\n        pbr_light_factor (float, optional): PBR lighting factor.\n        return_key (str, optional): Glob pattern for output images.\n        output_subdir (str, optional): Subdirectory for outputs.\n        gen_color_mp4 (bool, optional): Generate color MP4 video.\n        gen_viewnormal_mp4 (bool, optional): Generate view normal MP4.\n        gen_glonormal_mp4 (bool, optional): Generate global normal MP4.\n        no_index_file (bool, optional): Skip index file saving.\n        with_mtl (bool, optional): Use mesh material.\n\n    Returns:\n        list[str]: List of output image file paths.\n\n    Example:\n        ```py\n        from embodied_gen.utils.process_media import render_asset3d\n\n        image_paths = render_asset3d(\n            mesh_path=\"path_to_mesh.obj\",\n            output_root=\"path_to_save_dir\",\n            num_images=4,\n            elevation=(30, -30),\n            output_subdir=\"renders\",\n            no_index_file=True,\n        )\n        ```\n    \"\"\"\n    input_args = dict(\n        mesh_path=mesh_path,\n        output_root=output_root,\n        uuid=output_subdir,\n        distance=distance,\n        num_images=num_images,\n        elevation=elevation,\n        pbr_light_factor=pbr_light_factor,\n        with_mtl=with_mtl,\n        gen_color_mp4=gen_color_mp4,\n        gen_viewnormal_mp4=gen_viewnormal_mp4,\n        gen_glonormal_mp4=gen_glonormal_mp4,\n        no_index_file=no_index_file,\n    )\n\n    try:\n        _ = render_api(**input_args)\n    except Exception as e:\n        logger.error(f\"Error occurred during rendering: {e}.\")\n\n    dst_paths = glob(os.path.join(output_root, output_subdir, return_key))\n\n    return dst_paths\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.process_media.vcat_pil_images","title":"vcat_pil_images","text":"<pre><code>vcat_pil_images(images: list[Image], image_mode: str = 'RGB') -&gt; Image.Image\n</code></pre> <p>Vertically concatenates a list of PIL images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list[Image]</code> <p>List of images.</p> required <code>image_mode</code> <code>str</code> <p>Image mode.</p> <code>'RGB'</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: Vertically concatenated image.</p> Example <pre><code>from embodied_gen.utils.process_media import vcat_pil_images\nimg = vcat_pil_images([Image.open(\"a.png\"), Image.open(\"b.png\")])\nimg.save(\"vcat.png\")\n</code></pre> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def vcat_pil_images(\n    images: list[Image.Image], image_mode: str = \"RGB\"\n) -&gt; Image.Image:\n    \"\"\"Vertically concatenates a list of PIL images.\n\n    Args:\n        images (list[Image.Image]): List of images.\n        image_mode (str, optional): Image mode.\n\n    Returns:\n        Image.Image: Vertically concatenated image.\n\n    Example:\n        ```py\n        from embodied_gen.utils.process_media import vcat_pil_images\n        img = vcat_pil_images([Image.open(\"a.png\"), Image.open(\"b.png\")])\n        img.save(\"vcat.png\")\n        ```\n    \"\"\"\n    widths, heights = zip(*(img.size for img in images))\n    total_height = sum(heights)\n    max_width = max(widths)\n    new_image = Image.new(image_mode, (max_width, total_height))\n    y_offset = 0\n    for image in images:\n        new_image.paste(image, (0, y_offset))\n        y_offset += image.size[1]\n\n    return new_image\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation","title":"embodied_gen.utils.simulation","text":""},{"location":"api/utils.html#embodied_gen.utils.simulation.FrankaPandaGrasper","title":"FrankaPandaGrasper","text":"<pre><code>FrankaPandaGrasper(agent: BaseAgent, control_freq: float, joint_vel_limits: float = 2.0, joint_acc_limits: float = 1.0, finger_length: float = 0.025)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Provides grasp planning and control for Franka Panda robot.</p> <p>Attributes:</p> Name Type Description <code>agent</code> <code>BaseAgent</code> <p>The robot agent.</p> <code>robot</code> <p>The robot instance.</p> <code>control_freq</code> <code>float</code> <p>Control frequency.</p> <code>control_timestep</code> <code>float</code> <p>Control timestep.</p> <code>joint_vel_limits</code> <code>float</code> <p>Joint velocity limits.</p> <code>joint_acc_limits</code> <code>float</code> <p>Joint acceleration limits.</p> <code>finger_length</code> <code>float</code> <p>Length of gripper fingers.</p> <code>planners</code> <p>Motion planners for each environment.</p> <p>Initialize the grasper.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def __init__(\n    self,\n    agent: BaseAgent,\n    control_freq: float,\n    joint_vel_limits: float = 2.0,\n    joint_acc_limits: float = 1.0,\n    finger_length: float = 0.025,\n) -&gt; None:\n    \"\"\"Initialize the grasper.\"\"\"\n    self.agent = agent\n    self.robot = agent.robot\n    self.control_freq = control_freq\n    self.control_timestep = 1 / control_freq\n    self.joint_vel_limits = joint_vel_limits\n    self.joint_acc_limits = joint_acc_limits\n    self.finger_length = finger_length\n    self.planners = self._setup_planner()\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.FrankaPandaGrasper.compute_grasp_action","title":"compute_grasp_action","text":"<pre><code>compute_grasp_action(actor: Entity, reach_target_only: bool = True, offset: tuple[float, float, float] = [0, 0, -0.05], env_idx: int = 0) -&gt; np.ndarray\n</code></pre> <p>Compute grasp actions for a target actor.</p> <p>Parameters:</p> Name Type Description Default <code>actor</code> <code>Entity</code> <p>Target actor to grasp.</p> required <code>reach_target_only</code> <code>bool</code> <p>Only reach the target pose if True.</p> <code>True</code> <code>offset</code> <code>tuple[float, float, float]</code> <p>Offset for reach pose.</p> <code>[0, 0, -0.05]</code> <code>env_idx</code> <code>int</code> <p>Environment index.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of grasp actions.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def compute_grasp_action(\n    self,\n    actor: sapien.pysapien.Entity,\n    reach_target_only: bool = True,\n    offset: tuple[float, float, float] = [0, 0, -0.05],\n    env_idx: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"Compute grasp actions for a target actor.\n\n    Args:\n        actor (sapien.pysapien.Entity): Target actor to grasp.\n        reach_target_only (bool): Only reach the target pose if True.\n        offset (tuple[float, float, float]): Offset for reach pose.\n        env_idx (int): Environment index.\n\n    Returns:\n        np.ndarray: Array of grasp actions.\n    \"\"\"\n    physx_rigid = actor.components[1]\n    mesh = get_component_mesh(physx_rigid, to_world_frame=True)\n    obb = mesh.bounding_box_oriented\n    approaching = np.array([0, 0, -1])\n    tcp_pose = self.agent.tcp.pose[env_idx]\n    target_closing = (\n        tcp_pose.to_transformation_matrix()[0, :3, 1].cpu().numpy()\n    )\n    grasp_info = compute_grasp_info_by_obb(\n        obb,\n        approaching=approaching,\n        target_closing=target_closing,\n        depth=self.finger_length,\n    )\n\n    closing, center = grasp_info[\"closing\"], grasp_info[\"center\"]\n    raw_tcp_pose = tcp_pose.sp\n    grasp_pose = self.agent.build_grasp_pose(approaching, closing, center)\n    reach_pose = grasp_pose * sapien.Pose(p=offset)\n    grasp_pose = grasp_pose * sapien.Pose(p=[0, 0, 0.01])\n    actions = []\n    reach_actions = self.move_to_pose(\n        reach_pose,\n        self.control_timestep,\n        gripper_state=1,\n        env_idx=env_idx,\n    )\n    actions.append(reach_actions)\n\n    if reach_actions is None:\n        logger.warning(\n            f\"Failed to reach the grasp pose for node `{actor.name}`, skipping grasping.\"\n        )\n        return None\n\n    if not reach_target_only:\n        grasp_actions = self.move_to_pose(\n            grasp_pose,\n            self.control_timestep,\n            gripper_state=1,\n            env_idx=env_idx,\n        )\n        actions.append(grasp_actions)\n        close_actions = self.control_gripper(\n            gripper_state=-1,\n            env_idx=env_idx,\n        )\n        actions.append(close_actions)\n        back_actions = self.move_to_pose(\n            raw_tcp_pose,\n            self.control_timestep,\n            gripper_state=-1,\n            env_idx=env_idx,\n        )\n        actions.append(back_actions)\n\n    return np.concatenate(actions, axis=0)\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.FrankaPandaGrasper.control_gripper","title":"control_gripper","text":"<pre><code>control_gripper(gripper_state: Literal[-1, 1], n_step: int = 10) -&gt; np.ndarray\n</code></pre> <p>Generate gripper control actions.</p> <p>Parameters:</p> Name Type Description Default <code>gripper_state</code> <code>Literal[-1, 1]</code> <p>Desired gripper state.</p> required <code>n_step</code> <code>int</code> <p>Number of steps.</p> <code>10</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of gripper actions.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def control_gripper(\n    self,\n    gripper_state: Literal[-1, 1],\n    n_step: int = 10,\n) -&gt; np.ndarray:\n    \"\"\"Generate gripper control actions.\n\n    Args:\n        gripper_state (Literal[-1, 1]): Desired gripper state.\n        n_step (int): Number of steps.\n\n    Returns:\n        np.ndarray: Array of gripper actions.\n    \"\"\"\n    qpos = self.robot.get_qpos()[0, :-2].cpu().numpy()\n    actions = []\n    for _ in range(n_step):\n        action = np.hstack([qpos, gripper_state])[None, ...]\n        actions.append(action)\n\n    return np.concatenate(actions, axis=0)\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.FrankaPandaGrasper.move_to_pose","title":"move_to_pose","text":"<pre><code>move_to_pose(pose: Pose, control_timestep: float, gripper_state: Literal[-1, 1], use_point_cloud: bool = False, n_max_step: int = 100, action_key: str = 'position', env_idx: int = 0) -&gt; np.ndarray\n</code></pre> <p>Plan and execute motion to a target pose.</p> <p>Parameters:</p> Name Type Description Default <code>pose</code> <code>Pose</code> <p>Target pose.</p> required <code>control_timestep</code> <code>float</code> <p>Control timestep.</p> required <code>gripper_state</code> <code>Literal[-1, 1]</code> <p>Desired gripper state.</p> required <code>use_point_cloud</code> <code>bool</code> <p>Use point cloud for planning.</p> <code>False</code> <code>n_max_step</code> <code>int</code> <p>Max number of steps.</p> <code>100</code> <code>action_key</code> <code>str</code> <p>Key for action in result.</p> <code>'position'</code> <code>env_idx</code> <code>int</code> <p>Environment index.</p> <code>0</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Array of actions to reach the pose.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def move_to_pose(\n    self,\n    pose: sapien.Pose,\n    control_timestep: float,\n    gripper_state: Literal[-1, 1],\n    use_point_cloud: bool = False,\n    n_max_step: int = 100,\n    action_key: str = \"position\",\n    env_idx: int = 0,\n) -&gt; np.ndarray:\n    \"\"\"Plan and execute motion to a target pose.\n\n    Args:\n        pose (sapien.Pose): Target pose.\n        control_timestep (float): Control timestep.\n        gripper_state (Literal[-1, 1]): Desired gripper state.\n        use_point_cloud (bool): Use point cloud for planning.\n        n_max_step (int): Max number of steps.\n        action_key (str): Key for action in result.\n        env_idx (int): Environment index.\n\n    Returns:\n        np.ndarray: Array of actions to reach the pose.\n    \"\"\"\n    result = self.planners[env_idx].plan_qpos_to_pose(\n        np.concatenate([pose.p, pose.q]),\n        self.robot.get_qpos().cpu().numpy()[0],\n        time_step=control_timestep,\n        use_point_cloud=use_point_cloud,\n    )\n\n    if result[\"status\"] != \"Success\":\n        result = self.planners[env_idx].plan_screw(\n            np.concatenate([pose.p, pose.q]),\n            self.robot.get_qpos().cpu().numpy()[0],\n            time_step=control_timestep,\n            use_point_cloud=use_point_cloud,\n        )\n\n    if result[\"status\"] != \"Success\":\n        return\n\n    sample_ratio = (len(result[action_key]) // n_max_step) + 1\n    result[action_key] = result[action_key][::sample_ratio]\n\n    n_step = len(result[action_key])\n    actions = []\n    for i in range(n_step):\n        qpos = result[action_key][i]\n        action = np.hstack([qpos, gripper_state])[None, ...]\n        actions.append(action)\n\n    return np.concatenate(actions, axis=0)\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.SapienSceneManager","title":"SapienSceneManager","text":"<pre><code>SapienSceneManager(sim_freq: int, ray_tracing: bool, device: str = 'cuda')\n</code></pre> <p>Manages SAPIEN simulation scenes, cameras, and rendering.</p> <p>This class provides utilities for setting up scenes, adding cameras, stepping simulation, and rendering images.</p> <p>Attributes:</p> Name Type Description <code>sim_freq</code> <code>int</code> <p>Simulation frequency.</p> <code>ray_tracing</code> <code>bool</code> <p>Whether to use ray tracing.</p> <code>device</code> <code>str</code> <p>Device for simulation.</p> <code>renderer</code> <code>SapienRenderer</code> <p>SAPIEN renderer.</p> <code>scene</code> <code>Scene</code> <p>Simulation scene.</p> <code>cameras</code> <code>list</code> <p>List of camera components.</p> <code>actors</code> <code>dict</code> <p>Mapping of actor names to entities.</p> <p>Example see <code>embodied_gen/scripts/simulate_sapien.py</code>.</p> <p>Initialize the scene manager.</p> <p>Parameters:</p> Name Type Description Default <code>sim_freq</code> <code>int</code> <p>Simulation frequency.</p> required <code>ray_tracing</code> <code>bool</code> <p>Enable ray tracing.</p> required <code>device</code> <code>str</code> <p>Device for simulation.</p> <code>'cuda'</code> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def __init__(\n    self, sim_freq: int, ray_tracing: bool, device: str = \"cuda\"\n) -&gt; None:\n    \"\"\"Initialize the scene manager.\n\n    Args:\n        sim_freq (int): Simulation frequency.\n        ray_tracing (bool): Enable ray tracing.\n        device (str): Device for simulation.\n    \"\"\"\n    self.sim_freq = sim_freq\n    self.ray_tracing = ray_tracing\n    self.device = device\n    self.renderer = sapien.SapienRenderer()\n    self.scene = self._setup_scene()\n    self.cameras: list[sapien.render.RenderCameraComponent] = []\n    self.actors: dict[str, sapien.pysapien.Entity] = {}\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.SapienSceneManager.create_camera","title":"create_camera","text":"<pre><code>create_camera(cam_name: str, pose: Pose, image_hw: tuple[int, int], fovy_deg: float) -&gt; sapien.render.RenderCameraComponent\n</code></pre> <p>Create a camera in the scene.</p> <p>Parameters:</p> Name Type Description Default <code>cam_name</code> <code>str</code> <p>Camera name.</p> required <code>pose</code> <code>Pose</code> <p>Camera pose.</p> required <code>image_hw</code> <code>tuple[int, int]</code> <p>Image resolution (height, width).</p> required <code>fovy_deg</code> <code>float</code> <p>Field of view in degrees.</p> required <p>Returns:</p> Type Description <code>RenderCameraComponent</code> <p>sapien.render.RenderCameraComponent: The created camera.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def create_camera(\n    self,\n    cam_name: str,\n    pose: sapien.Pose,\n    image_hw: tuple[int, int],\n    fovy_deg: float,\n) -&gt; sapien.render.RenderCameraComponent:\n    \"\"\"Create a camera in the scene.\n\n    Args:\n        cam_name (str): Camera name.\n        pose (sapien.Pose): Camera pose.\n        image_hw (tuple[int, int]): Image resolution (height, width).\n        fovy_deg (float): Field of view in degrees.\n\n    Returns:\n        sapien.render.RenderCameraComponent: The created camera.\n    \"\"\"\n    cam_actor = self.scene.create_actor_builder().build_kinematic()\n    cam_actor.set_pose(pose)\n    camera = self.scene.add_mounted_camera(\n        name=cam_name,\n        mount=cam_actor,\n        pose=sapien.Pose(p=[0, 0, 0], q=[1, 0, 0, 0]),\n        width=image_hw[1],\n        height=image_hw[0],\n        fovy=np.deg2rad(fovy_deg),\n        near=0.01,\n        far=100,\n    )\n    self.cameras.append(camera)\n\n    return camera\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.SapienSceneManager.initialize_circular_cameras","title":"initialize_circular_cameras","text":"<pre><code>initialize_circular_cameras(num_cameras: int, radius: float, height: float, target_pt: list[float], image_hw: tuple[int, int], fovy_deg: float) -&gt; list[sapien.render.RenderCameraComponent]\n</code></pre> <p>Initialize multiple cameras arranged in a circle.</p> <p>Parameters:</p> Name Type Description Default <code>num_cameras</code> <code>int</code> <p>Number of cameras.</p> required <code>radius</code> <code>float</code> <p>Circle radius.</p> required <code>height</code> <code>float</code> <p>Camera height.</p> required <code>target_pt</code> <code>list[float]</code> <p>Target point to look at.</p> required <code>image_hw</code> <code>tuple[int, int]</code> <p>Image resolution.</p> required <code>fovy_deg</code> <code>float</code> <p>Field of view in degrees.</p> required <p>Returns:</p> Type Description <code>list[RenderCameraComponent]</code> <p>list[sapien.render.RenderCameraComponent]: List of cameras.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def initialize_circular_cameras(\n    self,\n    num_cameras: int,\n    radius: float,\n    height: float,\n    target_pt: list[float],\n    image_hw: tuple[int, int],\n    fovy_deg: float,\n) -&gt; list[sapien.render.RenderCameraComponent]:\n    \"\"\"Initialize multiple cameras arranged in a circle.\n\n    Args:\n        num_cameras (int): Number of cameras.\n        radius (float): Circle radius.\n        height (float): Camera height.\n        target_pt (list[float]): Target point to look at.\n        image_hw (tuple[int, int]): Image resolution.\n        fovy_deg (float): Field of view in degrees.\n\n    Returns:\n        list[sapien.render.RenderCameraComponent]: List of cameras.\n    \"\"\"\n    angle_step = 2 * np.pi / num_cameras\n    world_up_vec = np.array([0.0, 0.0, 1.0])\n    target_pt = np.array(target_pt)\n\n    for i in range(num_cameras):\n        angle = i * angle_step\n        cam_x = radius * np.cos(angle)\n        cam_y = radius * np.sin(angle)\n        cam_z = height\n        eye_pos = [cam_x, cam_y, cam_z]\n\n        forward_vec = target_pt - eye_pos\n        forward_vec = forward_vec / np.linalg.norm(forward_vec)\n        temp_right_vec = np.cross(forward_vec, world_up_vec)\n\n        if np.linalg.norm(temp_right_vec) &lt; 1e-6:\n            temp_right_vec = np.array([1.0, 0.0, 0.0])\n            if np.abs(np.dot(temp_right_vec, forward_vec)) &gt; 0.99:\n                temp_right_vec = np.array([0.0, 1.0, 0.0])\n\n        right_vec = temp_right_vec / np.linalg.norm(temp_right_vec)\n        up_vec = np.cross(right_vec, forward_vec)\n        rotation_matrix = np.array([forward_vec, -right_vec, up_vec]).T\n\n        rot = R.from_matrix(rotation_matrix)\n        scipy_quat = rot.as_quat()  # (x, y, z, w)\n        quat = [\n            scipy_quat[3],\n            scipy_quat[0],\n            scipy_quat[1],\n            scipy_quat[2],\n        ]  # (w, x, y, z)\n\n        self.create_camera(\n            f\"camera_{i}\",\n            sapien.Pose(p=eye_pos, q=quat),\n            image_hw,\n            fovy_deg,\n        )\n\n    return self.cameras\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.SapienSceneManager.step_action","title":"step_action","text":"<pre><code>step_action(agent: BaseAgent, action: Tensor, cameras: list[RenderCameraComponent], render_keys: list[str], sim_steps_per_control: int = 1) -&gt; dict\n</code></pre> <p>Step the simulation and render images from cameras.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>BaseAgent</code> <p>The robot agent.</p> required <code>action</code> <code>Tensor</code> <p>Action to apply.</p> required <code>cameras</code> <code>list</code> <p>List of camera components.</p> required <code>render_keys</code> <code>list[str]</code> <p>Types of images to render.</p> required <code>sim_steps_per_control</code> <code>int</code> <p>Simulation steps per control.</p> <code>1</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of rendered frames per camera.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def step_action(\n    self,\n    agent: BaseAgent,\n    action: torch.Tensor,\n    cameras: list[sapien.render.RenderCameraComponent],\n    render_keys: list[str],\n    sim_steps_per_control: int = 1,\n) -&gt; dict:\n    \"\"\"Step the simulation and render images from cameras.\n\n    Args:\n        agent (BaseAgent): The robot agent.\n        action (torch.Tensor): Action to apply.\n        cameras (list): List of camera components.\n        render_keys (list[str]): Types of images to render.\n        sim_steps_per_control (int): Simulation steps per control.\n\n    Returns:\n        dict: Dictionary of rendered frames per camera.\n    \"\"\"\n    agent.set_action(action)\n    frames = defaultdict(list)\n    for _ in range(sim_steps_per_control):\n        self.scene.step()\n\n    self.scene.update_render()\n    for camera in cameras:\n        camera.take_picture()\n        images = render_images(camera, render_keys=render_keys)\n        frames[camera.name].append(images)\n\n    return frames\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.load_actor_from_urdf","title":"load_actor_from_urdf","text":"<pre><code>load_actor_from_urdf(scene: Scene | ManiSkillScene, file_path: str, pose: Pose | None = None, env_idx: int = None, use_static: bool = False, update_mass: bool = False, scale: float | ndarray = 1.0) -&gt; sapien.pysapien.Entity\n</code></pre> <p>Load an sapien actor from a URDF file and add it to the scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>Scene | ManiSkillScene</code> <p>The simulation scene.</p> required <code>file_path</code> <code>str</code> <p>Path to the URDF file.</p> required <code>pose</code> <code>Pose | None</code> <p>Initial pose of the actor.</p> <code>None</code> <code>env_idx</code> <code>int</code> <p>Environment index for multi-env setup.</p> <code>None</code> <code>use_static</code> <code>bool</code> <p>Whether the actor is static.</p> <code>False</code> <code>update_mass</code> <code>bool</code> <p>Whether to update the actor's mass from URDF.</p> <code>False</code> <code>scale</code> <code>float | ndarray</code> <p>Scale factor for the actor.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Entity</code> <p>sapien.pysapien.Entity: The created actor entity.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def load_actor_from_urdf(\n    scene: sapien.Scene | ManiSkillScene,\n    file_path: str,\n    pose: sapien.Pose | None = None,\n    env_idx: int = None,\n    use_static: bool = False,\n    update_mass: bool = False,\n    scale: float | np.ndarray = 1.0,\n) -&gt; sapien.pysapien.Entity:\n    \"\"\"Load an sapien actor from a URDF file and add it to the scene.\n\n    Args:\n        scene (sapien.Scene | ManiSkillScene): The simulation scene.\n        file_path (str): Path to the URDF file.\n        pose (sapien.Pose | None): Initial pose of the actor.\n        env_idx (int): Environment index for multi-env setup.\n        use_static (bool): Whether the actor is static.\n        update_mass (bool): Whether to update the actor's mass from URDF.\n        scale (float | np.ndarray): Scale factor for the actor.\n\n    Returns:\n        sapien.pysapien.Entity: The created actor entity.\n    \"\"\"\n\n    def _get_local_pose(origin_tag: ET.Element | None) -&gt; sapien.Pose:\n        local_pose = sapien.Pose(p=[0, 0, 0], q=[1, 0, 0, 0])\n        if origin_tag is not None:\n            xyz = list(map(float, origin_tag.get(\"xyz\", \"0 0 0\").split()))\n            rpy = list(map(float, origin_tag.get(\"rpy\", \"0 0 0\").split()))\n            qx, qy, qz, qw = R.from_euler(\"xyz\", rpy, degrees=False).as_quat()\n            local_pose = sapien.Pose(p=xyz, q=[qw, qx, qy, qz])\n\n        return local_pose\n\n    tree = ET.parse(file_path)\n    root = tree.getroot()\n    node_name = root.get(\"name\")\n    file_dir = os.path.dirname(file_path)\n\n    visual_mesh = root.find(\".//visual/geometry/mesh\")\n    visual_file = visual_mesh.get(\"filename\")\n    visual_scale = visual_mesh.get(\"scale\", \"1.0 1.0 1.0\")\n    visual_scale = np.array([float(x) for x in visual_scale.split()])\n    visual_scale *= np.array(scale)\n\n    collision_mesh = root.find(\".//collision/geometry/mesh\")\n    collision_file = collision_mesh.get(\"filename\")\n    collision_scale = collision_mesh.get(\"scale\", \"1.0 1.0 1.0\")\n    collision_scale = np.array([float(x) for x in collision_scale.split()])\n    collision_scale *= np.array(scale)\n\n    visual_pose = _get_local_pose(root.find(\".//visual/origin\"))\n    collision_pose = _get_local_pose(root.find(\".//collision/origin\"))\n\n    visual_file = os.path.join(file_dir, visual_file)\n    collision_file = os.path.join(file_dir, collision_file)\n    static_fric = root.find(\".//collision/gazebo/mu1\").text\n    dynamic_fric = root.find(\".//collision/gazebo/mu2\").text\n\n    material = physx.PhysxMaterial(\n        static_friction=np.clip(float(static_fric), 0.1, 0.7),\n        dynamic_friction=np.clip(float(dynamic_fric), 0.1, 0.6),\n        restitution=0.05,\n    )\n    builder = scene.create_actor_builder()\n\n    body_type = \"static\" if use_static else \"dynamic\"\n    builder.set_physx_body_type(body_type)\n    builder.add_multiple_convex_collisions_from_file(\n        collision_file,\n        material=material,\n        scale=collision_scale,\n        # decomposition=\"coacd\",\n        # decomposition_params=dict(\n        #     threshold=0.05, max_convex_hull=64, verbose=False\n        # ),\n        pose=collision_pose,\n    )\n\n    builder.add_visual_from_file(\n        visual_file,\n        scale=visual_scale,\n        pose=visual_pose,\n    )\n    if pose is None:\n        pose = sapien.Pose(p=[0, 0, 0], q=[1, 0, 0, 0])\n\n    builder.set_initial_pose(pose)\n    if isinstance(scene, ManiSkillScene) and env_idx is not None:\n        builder.set_scene_idxs([env_idx])\n\n    actor = builder.build(\n        name=node_name if env_idx is None else f\"{node_name}-{env_idx}\"\n    )\n\n    if update_mass and hasattr(actor.components[1], \"mass\"):\n        node_mass = float(root.find(\".//inertial/mass\").get(\"value\"))\n        actor.components[1].set_mass(node_mass)\n\n    return actor\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.load_assets_from_layout_file","title":"load_assets_from_layout_file","text":"<pre><code>load_assets_from_layout_file(scene: ManiSkillScene | Scene, layout: str, z_offset: float = 0.0, init_quat: list[float] = [0, 0, 0, 1], env_idx: int = None) -&gt; dict[str, sapien.pysapien.Entity]\n</code></pre> <p>Load assets from an EmbodiedGen layout file and create sapien actors in the scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>ManiSkillScene | Scene</code> <p>The sapien simulation scene.</p> required <code>layout</code> <code>str</code> <p>Path to the embodiedgen layout file.</p> required <code>z_offset</code> <code>float</code> <p>Z offset for non-context objects.</p> <code>0.0</code> <code>init_quat</code> <code>list[float]</code> <p>Initial quaternion for orientation.</p> <code>[0, 0, 0, 1]</code> <code>env_idx</code> <code>int</code> <p>Environment index.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Entity]</code> <p>dict[str, sapien.pysapien.Entity]: Mapping from object names to actor entities.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def load_assets_from_layout_file(\n    scene: ManiSkillScene | sapien.Scene,\n    layout: str,\n    z_offset: float = 0.0,\n    init_quat: list[float] = [0, 0, 0, 1],\n    env_idx: int = None,\n) -&gt; dict[str, sapien.pysapien.Entity]:\n    \"\"\"Load assets from an EmbodiedGen layout file and create sapien actors in the scene.\n\n    Args:\n        scene (ManiSkillScene | sapien.Scene): The sapien simulation scene.\n        layout (str): Path to the embodiedgen layout file.\n        z_offset (float): Z offset for non-context objects.\n        init_quat (list[float]): Initial quaternion for orientation.\n        env_idx (int): Environment index.\n\n    Returns:\n        dict[str, sapien.pysapien.Entity]: Mapping from object names to actor entities.\n    \"\"\"\n    asset_root = os.path.dirname(layout)\n    layout = LayoutInfo.from_dict(json.load(open(layout, \"r\")))\n    actors = dict()\n    for node in layout.assets:\n        file_dir = layout.assets[node]\n        file_name = f\"{node.replace(' ', '_')}.urdf\"\n        urdf_file = os.path.join(asset_root, file_dir, file_name)\n\n        if layout.objs_mapping[node] == Scene3DItemEnum.BACKGROUND.value:\n            continue\n\n        position = layout.position[node].copy()\n        if layout.objs_mapping[node] != Scene3DItemEnum.CONTEXT.value:\n            position[2] += z_offset\n\n        use_static = (\n            layout.relation.get(Scene3DItemEnum.CONTEXT.value, None) == node\n        )\n\n        # Combine initial quaternion with object quaternion\n        x, y, z, qx, qy, qz, qw = position\n        qx, qy, qz, qw = quaternion_multiply([qx, qy, qz, qw], init_quat)\n        actor = load_actor_from_urdf(\n            scene,\n            urdf_file,\n            sapien.Pose(p=[x, y, z], q=[qw, qx, qy, qz]),\n            env_idx,\n            use_static=use_static,\n            update_mass=False,\n        )\n        actors[node] = actor\n\n    return actors\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.load_mani_skill_robot","title":"load_mani_skill_robot","text":"<pre><code>load_mani_skill_robot(scene: Scene | ManiSkillScene, layout: LayoutInfo | str, control_freq: int = 20, robot_init_qpos_noise: float = 0.0, control_mode: str = 'pd_joint_pos', backend_str: tuple[str, str] = ('cpu', 'gpu')) -&gt; BaseAgent\n</code></pre> <p>Load a ManiSkill robot agent into the scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>Scene | ManiSkillScene</code> <p>The simulation scene.</p> required <code>layout</code> <code>LayoutInfo | str</code> <p>Layout info or path to layout file.</p> required <code>control_freq</code> <code>int</code> <p>Control frequency.</p> <code>20</code> <code>robot_init_qpos_noise</code> <code>float</code> <p>Noise for initial joint positions.</p> <code>0.0</code> <code>control_mode</code> <code>str</code> <p>Robot control mode.</p> <code>'pd_joint_pos'</code> <code>backend_str</code> <code>tuple[str, str]</code> <p>Simulation/render backend.</p> <code>('cpu', 'gpu')</code> <p>Returns:</p> Name Type Description <code>BaseAgent</code> <code>BaseAgent</code> <p>The loaded robot agent.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def load_mani_skill_robot(\n    scene: sapien.Scene | ManiSkillScene,\n    layout: LayoutInfo | str,\n    control_freq: int = 20,\n    robot_init_qpos_noise: float = 0.0,\n    control_mode: str = \"pd_joint_pos\",\n    backend_str: tuple[str, str] = (\"cpu\", \"gpu\"),\n) -&gt; BaseAgent:\n    \"\"\"Load a ManiSkill robot agent into the scene.\n\n    Args:\n        scene (sapien.Scene | ManiSkillScene): The simulation scene.\n        layout (LayoutInfo | str): Layout info or path to layout file.\n        control_freq (int): Control frequency.\n        robot_init_qpos_noise (float): Noise for initial joint positions.\n        control_mode (str): Robot control mode.\n        backend_str (tuple[str, str]): Simulation/render backend.\n\n    Returns:\n        BaseAgent: The loaded robot agent.\n    \"\"\"\n    from mani_skill.agents import REGISTERED_AGENTS\n    from mani_skill.envs.scene import ManiSkillScene\n    from mani_skill.envs.utils.system.backend import (\n        parse_sim_and_render_backend,\n    )\n\n    if isinstance(layout, str) and layout.endswith(\".json\"):\n        layout = LayoutInfo.from_dict(json.load(open(layout, \"r\")))\n\n    robot_name = layout.relation[Scene3DItemEnum.ROBOT.value]\n    x, y, z, qx, qy, qz, qw = layout.position[robot_name]\n    delta_z = 0.002  # Add small offset to avoid collision.\n    pose = sapien.Pose([x, y, z + delta_z], [qw, qx, qy, qz])\n\n    if robot_name not in REGISTERED_AGENTS:\n        logger.warning(\n            f\"Robot `{robot_name}` not registered, chosen from {REGISTERED_AGENTS.keys()}, use `panda` instead.\"\n        )\n        robot_name = \"panda\"\n\n    ROBOT_CLS = REGISTERED_AGENTS[robot_name].agent_cls\n    backend = parse_sim_and_render_backend(*backend_str)\n    if isinstance(scene, sapien.Scene):\n        scene = ManiSkillScene([scene], device=backend_str[0], backend=backend)\n    robot = ROBOT_CLS(\n        scene=scene,\n        control_freq=control_freq,\n        control_mode=control_mode,\n        initial_pose=pose,\n    )\n\n    # Set robot init joint rad agree(joint0 to joint6 w 2 finger).\n    qpos = np.array(\n        [\n            0.0,\n            np.pi / 8,\n            0,\n            -np.pi * 3 / 8,\n            0,\n            np.pi * 3 / 4,\n            np.pi / 4,\n            0.04,\n            0.04,\n        ]\n    )\n    qpos = (\n        np.random.normal(\n            0, robot_init_qpos_noise, (len(scene.sub_scenes), len(qpos))\n        )\n        + qpos\n    )\n    qpos[:, -2:] = 0.04\n    robot.reset(qpos)\n    robot.init_qpos = robot.robot.qpos\n    robot.controller.controllers[\"gripper\"].reset()\n\n    return robot\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.simulation.render_images","title":"render_images","text":"<pre><code>render_images(camera: RenderCameraComponent, render_keys: list[Literal['Color', 'Segmentation', 'Normal', 'Mask', 'Depth', 'Foreground']] = None) -&gt; dict[str, Image.Image]\n</code></pre> <p>Render images from a given SAPIEN camera.</p> <p>Parameters:</p> Name Type Description Default <code>camera</code> <code>RenderCameraComponent</code> <p>Camera to render from.</p> required <code>render_keys</code> <code>list[str]</code> <p>Types of images to render.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Image]</code> <p>dict[str, Image.Image]: Dictionary of rendered images.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def render_images(\n    camera: sapien.render.RenderCameraComponent,\n    render_keys: list[\n        Literal[\n            \"Color\",\n            \"Segmentation\",\n            \"Normal\",\n            \"Mask\",\n            \"Depth\",\n            \"Foreground\",\n        ]\n    ] = None,\n) -&gt; dict[str, Image.Image]:\n    \"\"\"Render images from a given SAPIEN camera.\n\n    Args:\n        camera (sapien.render.RenderCameraComponent): Camera to render from.\n        render_keys (list[str], optional): Types of images to render.\n\n    Returns:\n        dict[str, Image.Image]: Dictionary of rendered images.\n    \"\"\"\n    if render_keys is None:\n        render_keys = [\n            \"Color\",\n            \"Segmentation\",\n            \"Normal\",\n            \"Mask\",\n            \"Depth\",\n            \"Foreground\",\n        ]\n\n    results: dict[str, Image.Image] = {}\n    if \"Color\" in render_keys:\n        color = camera.get_picture(\"Color\")\n        color_rgb = (np.clip(color[..., :3], 0, 1) * 255).astype(np.uint8)\n        results[\"Color\"] = Image.fromarray(color_rgb)\n\n    if \"Mask\" in render_keys:\n        alpha = (np.clip(color[..., 3], 0, 1) * 255).astype(np.uint8)\n        results[\"Mask\"] = Image.fromarray(alpha)\n\n    if \"Segmentation\" in render_keys:\n        seg_labels = camera.get_picture(\"Segmentation\")\n        label0 = seg_labels[..., 0].astype(np.uint8)\n        seg_color = COLOR_PALETTE[label0]\n        results[\"Segmentation\"] = Image.fromarray(seg_color)\n\n    if \"Foreground\" in render_keys:\n        seg_labels = camera.get_picture(\"Segmentation\")\n        label0 = seg_labels[..., 0]\n        mask = np.where((label0 &gt; 1), 255, 0).astype(np.uint8)\n        color = camera.get_picture(\"Color\")\n        color_rgb = (np.clip(color[..., :3], 0, 1) * 255).astype(np.uint8)\n        foreground = np.concatenate([color_rgb, mask[..., None]], axis=-1)\n        results[\"Foreground\"] = Image.fromarray(foreground)\n\n    if \"Normal\" in render_keys:\n        normal = camera.get_picture(\"Normal\")[..., :3]\n        normal_img = (((normal + 1) / 2) * 255).astype(np.uint8)\n        results[\"Normal\"] = Image.fromarray(normal_img)\n\n    if \"Depth\" in render_keys:\n        position_map = camera.get_picture(\"Position\")\n        depth = -position_map[..., 2]\n        alpha = torch.tensor(color[..., 3], dtype=torch.float32)\n        norm_depth = DiffrastRender.normalize_map_by_mask(\n            torch.tensor(depth), alpha\n        )\n        depth_img = (norm_depth * 255).to(torch.uint8).numpy()\n        results[\"Depth\"] = Image.fromarray(depth_img)\n\n    return results\n</code></pre>"},{"location":"api/utils.html#embodied_gen.utils.tags","title":"embodied_gen.utils.tags","text":""},{"location":"api/utils.html#embodied_gen.utils.trender","title":"embodied_gen.utils.trender","text":""},{"location":"api/utils.html#embodied_gen.utils.monkey_patches","title":"embodied_gen.utils.monkey_patches","text":""},{"location":"api/validators.html","title":"Validators API","text":"<p>Tools for asset validation, quality control, and conversion.</p>"},{"location":"api/validators.html#embodied_gen.validators.aesthetic_predictor","title":"embodied_gen.validators.aesthetic_predictor","text":""},{"location":"api/validators.html#embodied_gen.validators.aesthetic_predictor.AestheticPredictor","title":"AestheticPredictor","text":"<pre><code>AestheticPredictor(clip_model_dir=None, sac_model_path=None, device='cpu')\n</code></pre> <p>Aesthetic Score Predictor using CLIP and a pre-trained MLP.</p> <p>Checkpoints from <code>https://github.com/christophschuhmann/improved-aesthetic-predictor/tree/main</code>.</p> <p>Parameters:</p> Name Type Description Default <code>clip_model_dir</code> <code>str</code> <p>Path to CLIP model directory.</p> <code>None</code> <code>sac_model_path</code> <code>str</code> <p>Path to SAC model weights.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device for computation (\"cuda\" or \"cpu\").</p> <code>'cpu'</code> Example <pre><code>from embodied_gen.validators.aesthetic_predictor import AestheticPredictor\npredictor = AestheticPredictor(device=\"cuda\")\nscore = predictor.predict(\"image.png\")\nprint(\"Aesthetic score:\", score)\n</code></pre> Source code in <code>embodied_gen/validators/aesthetic_predictor.py</code> <pre><code>def __init__(self, clip_model_dir=None, sac_model_path=None, device=\"cpu\"):\n\n    self.device = device\n\n    if clip_model_dir is None:\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=\"aesthetic/*\"\n        )\n        suffix = \"aesthetic\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        clip_model_dir = os.path.join(model_path, suffix)\n\n    if sac_model_path is None:\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=\"aesthetic/*\"\n        )\n        suffix = \"aesthetic\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        sac_model_path = os.path.join(\n            model_path, suffix, \"sac+logos+ava1-l14-linearMSE.pth\"\n        )\n\n    self.clip_model, self.preprocess = self._load_clip_model(\n        clip_model_dir\n    )\n    self.sac_model = self._load_sac_model(sac_model_path, input_size=768)\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.aesthetic_predictor.AestheticPredictor.normalized","title":"normalized  <code>staticmethod</code>","text":"<pre><code>normalized(a, axis=-1, order=2)\n</code></pre> <p>Normalize the array to unit norm.</p> Source code in <code>embodied_gen/validators/aesthetic_predictor.py</code> <pre><code>@staticmethod\ndef normalized(a, axis=-1, order=2):\n    \"\"\"Normalize the array to unit norm.\"\"\"\n    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n    l2[l2 == 0] = 1\n    return a / np.expand_dims(l2, axis)\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.aesthetic_predictor.AestheticPredictor.predict","title":"predict","text":"<pre><code>predict(image_path)\n</code></pre> <p>Predicts the aesthetic score for a given image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the image file.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Predicted aesthetic score.</p> Source code in <code>embodied_gen/validators/aesthetic_predictor.py</code> <pre><code>def predict(self, image_path):\n    \"\"\"Predicts the aesthetic score for a given image.\n\n    Args:\n        image_path (str): Path to the image file.\n\n    Returns:\n        float: Predicted aesthetic score.\n    \"\"\"\n    if isinstance(image_path, str):\n        pil_image = Image.open(image_path)\n    else:\n        pil_image = image_path\n\n    image = self.preprocess(pil_image).unsqueeze(0).to(self.device)\n\n    with torch.no_grad():\n        # Extract CLIP features\n        image_features = self.clip_model.encode_image(image)\n        # Normalize features\n        normalized_features = self.normalized(\n            image_features.cpu().detach().numpy()\n        )\n        # Predict score\n        prediction = self.sac_model(\n            torch.from_numpy(normalized_features)\n            .type(torch.FloatTensor)\n            .to(self.device)\n        )\n\n    return prediction.item()\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers","title":"embodied_gen.validators.quality_checkers","text":""},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.BaseChecker","title":"BaseChecker","text":"<pre><code>BaseChecker(prompt: str = None, verbose: bool = False)\n</code></pre> <p>Base class for quality checkers using GPT clients.</p> <p>Provides a common interface for querying and validating responses. Subclasses must implement the <code>query</code> method.</p> <p>Attributes:</p> Name Type Description <code>prompt</code> <code>str</code> <p>The prompt used for queries.</p> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(self, prompt: str = None, verbose: bool = False) -&gt; None:\n    self.prompt = prompt\n    self.verbose = verbose\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.BaseChecker.validate","title":"validate  <code>staticmethod</code>","text":"<pre><code>validate(checkers: list[BaseChecker], images_list: list[list[str]]) -&gt; list\n</code></pre> <p>Validates a list of checkers against corresponding image lists.</p> <p>Parameters:</p> Name Type Description Default <code>checkers</code> <code>list[BaseChecker]</code> <p>List of checker instances.</p> required <code>images_list</code> <code>list[list[str]]</code> <p>List of image path lists.</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Validation results with overall outcome.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>@staticmethod\ndef validate(\n    checkers: list[\"BaseChecker\"], images_list: list[list[str]]\n) -&gt; list:\n    \"\"\"Validates a list of checkers against corresponding image lists.\n\n    Args:\n        checkers (list[BaseChecker]): List of checker instances.\n        images_list (list[list[str]]): List of image path lists.\n\n    Returns:\n        list: Validation results with overall outcome.\n    \"\"\"\n    assert len(checkers) == len(images_list)\n    results = []\n    overall_result = True\n    for checker, images in zip(checkers, images_list):\n        qa_flag, qa_info = checker(images)\n        if isinstance(qa_info, str):\n            qa_info = qa_info.replace(\"\\n\", \".\")\n        results.append([checker.__class__.__name__, qa_info])\n        if qa_flag is False:\n            overall_result = False\n\n    results.append([\"overall\", \"YES\" if overall_result else \"NO\"])\n\n    return results\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.ImageAestheticChecker","title":"ImageAestheticChecker","text":"<pre><code>ImageAestheticChecker(clip_model_dir: str = None, sac_model_path: str = None, thresh: float = 4.5, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>Evaluates the aesthetic quality of images using a CLIP-based predictor.</p> <p>Attributes:</p> Name Type Description <code>clip_model_dir</code> <code>str</code> <p>Path to the CLIP model directory.</p> <code>sac_model_path</code> <code>str</code> <p>Path to the aesthetic predictor model weights.</p> <code>thresh</code> <code>float</code> <p>Threshold above which images are considered aesthetically acceptable.</p> <code>verbose</code> <code>bool</code> <p>Whether to print detailed log messages.</p> <code>predictor</code> <code>AestheticPredictor</code> <p>The model used to predict aesthetic scores.</p> Example <pre><code>from embodied_gen.validators.quality_checkers import ImageAestheticChecker\nchecker = ImageAestheticChecker(thresh=4.5)\nflag, score = checker([\"image1.png\", \"image2.png\"])\nprint(\"Aesthetic OK:\", flag, \"Score:\", score)\n</code></pre> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    clip_model_dir: str = None,\n    sac_model_path: str = None,\n    thresh: float = 4.50,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(verbose=verbose)\n    self.clip_model_dir = clip_model_dir\n    self.sac_model_path = sac_model_path\n    self.thresh = thresh\n    self.predictor = AestheticPredictor(clip_model_dir, sac_model_path)\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.ImageSegChecker","title":"ImageSegChecker","text":"<pre><code>ImageSegChecker(gpt_client: GPTclient, prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>A segmentation quality checker for 3D assets using GPT-based reasoning.</p> <p>This class compares an original image with its segmented version to evaluate whether the segmentation successfully isolates the main object with minimal truncation and correct foreground extraction.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>GPT client used for multi-modal image analysis.</p> <code>prompt</code> <code>str</code> <p>The prompt used to guide the GPT model for evaluation.</p> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        Task: Evaluate the quality of object segmentation between two images:\n            the first is the original, the second is the segmented result.\n\n        Criteria:\n        - The main foreground object should be clearly extracted (not the background).\n        - The object must appear realistic, with reasonable geometry and color.\n        - The object should be geometrically complete \u2014 no missing, truncated, or cropped parts.\n        - The object must be centered, with a margin on all sides.\n        - Ignore minor imperfections (e.g., small holes or fine edge artifacts).\n\n        Output Rules:\n        If segmentation is acceptable, respond with \"YES\" (and nothing else).\n        If not acceptable, respond with \"NO\", followed by a brief reason (max 20 words).\n        \"\"\"\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.MeshGeoChecker","title":"MeshGeoChecker","text":"<pre><code>MeshGeoChecker(gpt_client: GPTclient, prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>A geometry quality checker for 3D mesh assets using GPT-based reasoning.</p> <p>This class leverages a multi-modal GPT client to analyze rendered images of a 3D object and determine if its geometry is complete.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>The GPT client used for multi-modal querying.</p> <code>prompt</code> <code>str</code> <p>The prompt sent to the GPT model. If not provided, a default one is used.</p> <code>verbose</code> <code>bool</code> <p>Whether to print debug information during evaluation.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    if self.prompt is None:\n        # Old version for TRELLIS.\n        # self.prompt = \"\"\"\n        # You are an expert in evaluating the geometry quality of generated 3D asset.\n        # You will be given rendered views of a generated 3D asset, type {}, with black background.\n        # Your task is to evaluate the quality of the 3D asset generation,\n        # including geometry, structure, and appearance, based on the rendered views.\n        # Criteria:\n        # - Is the object in the image a single, complete, and well-formed instance,\n        #     without truncation, missing parts, overlapping duplicates, or redundant geometry?\n        # - Minor flaws, asymmetries, or simplifications (e.g., less detail on sides or back,\n        #     soft edges) are acceptable if the object is structurally sound and recognizable.\n        # - Only evaluate geometry. Do not assess texture quality.\n        # - The asset should not contain any unrelated elements, such as\n        #     ground planes, platforms, or background props (e.g., paper, flooring).\n\n        # If all the above criteria are met, return \"YES\". Otherwise, return\n        #     \"NO\" followed by a brief explanation (no more than 20 words).\n\n        # Example:\n        # Images show a yellow cup standing on a flat white plane -&gt; NO\n        # -&gt; Response: NO: extra white surface under the object.\n        # Image shows a chair with simplified back legs and soft edges -&gt; YES\n        # \"\"\"\n\n        self.prompt = \"\"\"\n        You are an expert in evaluating the geometry quality of generated 3D asset.\n        You will be given rendered views of a generated 3D asset, type {}, with black background.\n        Your task is to evaluate the quality of the 3D asset generation,\n        including geometry, structure, and appearance, based on the rendered views.\n        Criteria:\n        - Is the object in the image a single, complete, and well-formed instance,\n            without truncation, missing parts, overlapping duplicates, or redundant geometry?\n        - Minor flaws, asymmetries, or simplifications (e.g., less detail on sides or back,\n            soft edges) are acceptable if the object is structurally sound and recognizable.\n        - Only evaluate geometry. Do not assess texture quality.\n\n        If all the above criteria are met, return \"YES\" only. Otherwise, return\n            \"NO\" followed by a brief explanation (no more than 20 words).\n\n        Example:\n        Image shows a chair with one leg missing -&gt; NO: the chair missing leg.\n        Image shows a geometrically complete cup -&gt; YES\n        \"\"\"\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.PanoHeightEstimator","title":"PanoHeightEstimator","text":"<pre><code>PanoHeightEstimator(gpt_client: GPTclient, default_value: float = 3.5)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Estimate the real ceiling height of an indoor space from a 360\u00b0 panoramic image.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>The GPT client used to perform image-based reasoning and return height estimates.</p> <code>default_value</code> <code>float</code> <p>The fallback height in meters if parsing the GPT output fails.</p> <code>prompt</code> <code>str</code> <p>The textual instruction used to guide the GPT model for height estimation.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    default_value: float = 3.5,\n) -&gt; None:\n    self.gpt_client = gpt_client\n    self.default_value = default_value\n    self.prompt = \"\"\"\n    You are an expert in building height estimation and panoramic image analysis.\n    Your task is to analyze a 360\u00b0 indoor panoramic image and estimate the **actual height** of the space in meters.\n\n    Consider the following visual cues:\n    1. Ceiling visibility and reference objects (doors, windows, furniture, appliances).\n    2. Floor features or level differences.\n    3. Room type (e.g., residential, office, commercial).\n    4. Object-to-ceiling proportions (e.g., height of doors relative to ceiling).\n    5. Architectural elements (e.g., chandeliers, shelves, kitchen cabinets).\n\n    Input: A full 360\u00b0 panoramic indoor photo.\n    Output: A single number in meters representing the estimated room height. Only return the number (e.g., `3.2`)\n    \"\"\"\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.PanoImageGenChecker","title":"PanoImageGenChecker","text":"<pre><code>PanoImageGenChecker(gpt_client: GPTclient, prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>A checker class that validates the quality and realism of generated panoramic indoor images.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>A GPT client instance used to query for image validation.</p> <code>prompt</code> <code>str</code> <p>The instruction prompt passed to the GPT model. If None, a default prompt is used.</p> <code>verbose</code> <code>bool</code> <p>Whether to print internal processing information for debugging.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        You are a panoramic image analyzer specializing in indoor room structure validation.\n\n        Given a generated panoramic image, assess if it meets all the criteria:\n        - Floor Space: \u226530 percent of the floor is free of objects or obstructions.\n        - Visual Clarity: Floor, walls, and ceiling are clear, with no distortion, blur, noise.\n        - Structural Continuity: Surfaces form plausible, continuous geometry\n            without breaks, floating parts, or abrupt cuts.\n        - Spatial Completeness: Full 360\u00b0 coverage without missing areas,\n            seams, gaps, or stitching artifacts.\n        Instructions:\n        - If all criteria are met, reply with \"YES\".\n        - Otherwise, reply with \"NO: &lt;brief explanation&gt;\" (max 20 words).\n\n        Respond exactly as:\n        \"YES\"\n        or\n        \"NO: brief explanation.\"\n        \"\"\"\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.PanoImageOccChecker","title":"PanoImageOccChecker","text":"<pre><code>PanoImageOccChecker(gpt_client: GPTclient, box_hw: tuple[int, int], prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>Checks for physical obstacles in the bottom-center region of a panoramic image.</p> <p>This class crops a specified region from the input panoramic image and uses a GPT client to determine whether any physical obstacles there.</p> <p>Parameters:</p> Name Type Description Default <code>gpt_client</code> <code>GPTclient</code> <p>The GPT-based client used for visual reasoning.</p> required <code>box_hw</code> <code>tuple[int, int]</code> <p>The height and width of the crop box.</p> required <code>prompt</code> <code>str</code> <p>Custom prompt for the GPT client. Defaults to a predefined one.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose logs. Defaults to False.</p> <code>False</code> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    box_hw: tuple[int, int],\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    self.box_hw = box_hw\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        This image is a cropped region from the bottom-center of a panoramic view.\n        Please determine whether there is any obstacle present \u2014 such as furniture, tables, or other physical objects.\n        Ignore floor textures, rugs, carpets, shadows, and lighting effects \u2014 they do not count as obstacles.\n        Only consider real, physical objects that could block walking or movement.\n\n        Instructions:\n        - If there is no obstacle, reply: \"YES\".\n        - Otherwise, reply: \"NO: &lt;brief explanation&gt;\" (max 20 words).\n\n        Respond exactly as:\n        \"YES\"\n        or\n        \"NO: brief explanation.\"\n        \"\"\"\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.SemanticConsistChecker","title":"SemanticConsistChecker","text":"<pre><code>SemanticConsistChecker(gpt_client: GPTclient, prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>Checks semantic consistency between text descriptions and segmented images.</p> <p>Uses GPT to evaluate if the image matches the text in object type, geometry, and color.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>GPT client for queries.</p> <code>prompt</code> <code>str</code> <p>Prompt for consistency evaluation.</p> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        You are an expert in image-text consistency assessment.\n        You will be given:\n        - A short text description of an object.\n        - An segmented image of the same object with the background removed.\n\n        Criteria:\n        - The image must visually match the text description in terms of object type, structure, geometry, and color.\n        - The object must appear realistic, with reasonable geometry (e.g., a table must have a stable number\n            of legs with a reasonable distribution. Count the number of legs visible in the image. (strict) For tables,\n            fewer than four legs or if the legs are unevenly distributed, are not allowed. Do not assume\n            hidden legs unless they are clearly visible.)\n        - Geometric completeness is required: the object must not have missing, truncated, or cropped parts.\n        - The image must contain exactly one object. Multiple distinct objects (e.g. multiple pens) are not allowed.\n            A single composite object (e.g., a chair with legs) is acceptable.\n        - The object should be shown from a slightly angled (three-quarter) perspective,\n            not a flat, front-facing view showing only one surface.\n\n        Instructions:\n        - If all criteria are met, return `\"YES\"`.\n        - Otherwise, return \"NO\" with a brief explanation (max 20 words).\n\n        Respond in exactly one of the following formats:\n        YES\n        or\n        NO: brief explanation.\n\n        Input:\n        {}\n        \"\"\"\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.SemanticMatcher","title":"SemanticMatcher","text":"<pre><code>SemanticMatcher(gpt_client: GPTclient, prompt: str = None, verbose: bool = False, seed: int = None)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>Matches query text to semantically similar scene descriptions.</p> <p>Uses GPT to find the most similar scene IDs from a dictionary.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>GPT client for queries.</p> <code>prompt</code> <code>str</code> <p>Prompt for semantic matching.</p> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging.</p> <code>seed</code> <code>int</code> <p>Random seed for selection.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    prompt: str = None,\n    verbose: bool = False,\n    seed: int = None,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    self.seed = seed\n    random.seed(seed)\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        You are an expert in semantic similarity and scene retrieval.\n        You will be given:\n        - A dictionary where each key is a scene ID, and each value is a scene description.\n        - A query text describing a target scene.\n\n        Your task:\n        return_num = 2\n        - Find the &lt;return_num&gt; most semantically similar scene IDs to the query text.\n        - If there are fewer than &lt;return_num&gt; distinct relevant matches, repeat the closest ones to make a list of &lt;return_num&gt;.\n        - Only output the list of &lt;return_num&gt; scene IDs, sorted from most to less similar.\n        - Do NOT use markdown, JSON code blocks, or any formatting syntax, only return a plain list like [\"id1\", ...].\n        - The returned scene ID must exist in the dictionary and be in exactly the same format. For example,\n            if the key in the dictionary is \"scene_0040\", return \"scene_0040\"; if it is \"scene_040\", return \"scene_040\".\n\n        Input example:\n        Dictionary:\n        \"{{\n        \"t_scene_0008\": \"A study room with full bookshelves and a lamp in the corner.\",\n        \"t_scene_019\": \"A child's bedroom with pink walls and a small desk.\",\n        \"t_scene_020\": \"A living room with a wooden floor.\",\n        \"t_scene_021\": \"A living room with toys scattered on the floor.\",\n        ...\n        \"t_scene_office_0001\": \"A very spacious, modern open-plan office with wide desks and no people, panoramic view.\"\n        }}\"\n        Text:\n        \"A traditional indoor room\"\n        Output:\n        '[\"t_scene_office_0001\", ...]'\n\n        Input:\n        Dictionary:\n        {context}\n        Text:\n        {text}\n        Output:\n        &lt;topk_key_list&gt;\n        \"\"\"\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.SemanticMatcher.query","title":"query","text":"<pre><code>query(text: str, context: dict, rand: bool = True, params: dict = None) -&gt; str\n</code></pre> <p>Queries for semantically similar scene IDs.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>Query text.</p> required <code>context</code> <code>dict</code> <p>Dictionary of scene descriptions.</p> required <code>rand</code> <code>bool</code> <p>Whether to randomly select from top matches.</p> <code>True</code> <code>params</code> <code>dict</code> <p>Additional GPT parameters.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Matched scene ID.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def query(\n    self, text: str, context: dict, rand: bool = True, params: dict = None\n) -&gt; str:\n    \"\"\"Queries for semantically similar scene IDs.\n\n    Args:\n        text (str): Query text.\n        context (dict): Dictionary of scene descriptions.\n        rand (bool, optional): Whether to randomly select from top matches.\n        params (dict, optional): Additional GPT parameters.\n\n    Returns:\n        str: Matched scene ID.\n    \"\"\"\n    match_list = self.gpt_client.query(\n        self.prompt.format(context=context, text=text),\n        params=params,\n    )\n    match_list = json_repair.loads(match_list)\n    result = random.choice(match_list) if rand else match_list[0]\n\n    return result\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.quality_checkers.TextGenAlignChecker","title":"TextGenAlignChecker","text":"<pre><code>TextGenAlignChecker(gpt_client: GPTclient, prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>Evaluates alignment between text prompts and generated 3D asset images.</p> <p>Assesses if the rendered images match the text description in category and geometry.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>GPT client for queries.</p> <code>prompt</code> <code>str</code> <p>Prompt for alignment evaluation.</p> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        You are an expert in evaluating the quality of generated 3D assets.\n        You will be given:\n        - A text description of an object: TEXT\n        - Rendered views of the generated 3D asset.\n\n        Your task is to:\n        1. Determine whether the generated 3D asset roughly reflects the object class\n            or a semantically adjacent category described in the text.\n        2. Evaluate the geometry quality of the 3D asset generation based on the rendered views.\n\n        Criteria:\n        - Determine if the generated 3D asset belongs to the text described or a similar category.\n        - Focus on functional similarity: if the object serves the same general\n            purpose (e.g., writing, placing items), it should be accepted.\n        - Is the geometry complete and well-formed, with no missing parts,\n        distortions, visual artifacts, or redundant structures?\n        - Does the number of object instances match the description?\n            There should be only one object unless otherwise specified.\n        - Minor flaws in geometry or texture are acceptable, high tolerance for texture quality defects.\n        - Minor simplifications in geometry or texture (e.g. soft edges, less detail)\n            are acceptable if the object is still recognizable.\n        - The asset should not contain any unrelated elements, such as\n            ground planes, platforms, or background props (e.g., paper, flooring).\n\n        Example:\n        Text: \"yellow cup\"\n        Image: shows a yellow cup standing on a flat white plane -&gt; NO: extra surface under the object.\n\n        Instructions:\n        - If the quality of generated asset is acceptable and faithfully represents the text, return \"YES\".\n        - Otherwise, return \"NO\" followed by a brief explanation (no more than 20 words).\n\n        Respond in exactly one of the following formats:\n        YES\n        or\n        NO: brief explanation\n\n        Input:\n        Text description: {}\n        \"\"\"\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.urdf_convertor","title":"embodied_gen.validators.urdf_convertor","text":""},{"location":"api/validators.html#embodied_gen.validators.urdf_convertor.URDFGenerator","title":"URDFGenerator","text":"<pre><code>URDFGenerator(gpt_client: GPTclient, mesh_file_list: list[str] = ['material_0.png', 'material.mtl'], prompt_template: str = None, attrs_name: list[str] = None, render_dir: str = 'urdf_renders', render_view_num: int = 4, decompose_convex: bool = False, rotate_xyzw: list[float] = (0.7071, 0, 0, 0.7071))\n</code></pre> <p>               Bases: <code>object</code></p> <p>Generates URDF files for 3D assets with physical and semantic attributes.</p> <p>Uses GPT to estimate object properties and generates a URDF file with mesh, friction, mass, and metadata.</p> <p>Parameters:</p> Name Type Description Default <code>gpt_client</code> <code>GPTclient</code> <p>GPT client for attribute estimation.</p> required <code>mesh_file_list</code> <code>list[str]</code> <p>Additional mesh files to copy.</p> <code>['material_0.png', 'material.mtl']</code> <code>prompt_template</code> <code>str</code> <p>Prompt template for GPT queries.</p> <code>None</code> <code>attrs_name</code> <code>list[str]</code> <p>List of attribute names to include.</p> <code>None</code> <code>render_dir</code> <code>str</code> <p>Directory for rendered images.</p> <code>'urdf_renders'</code> <code>render_view_num</code> <code>int</code> <p>Number of views to render.</p> <code>4</code> <code>decompose_convex</code> <code>bool</code> <p>Whether to decompose mesh for collision.</p> <code>False</code> <code>rotate_xyzw</code> <code>list[float]</code> <p>Quaternion for mesh rotation.</p> <code>(0.7071, 0, 0, 0.7071)</code> Example <pre><code>from embodied_gen.validators.urdf_convertor import URDFGenerator\nfrom embodied_gen.utils.gpt_clients import GPT_CLIENT\n\nurdf_gen = URDFGenerator(GPT_CLIENT, render_view_num=4)\nurdf_path = urdf_gen(mesh_path=\"mesh.obj\", output_root=\"output_dir\")\nprint(\"Generated URDF:\", urdf_path)\n</code></pre> Source code in <code>embodied_gen/validators/urdf_convertor.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    mesh_file_list: list[str] = [\"material_0.png\", \"material.mtl\"],\n    prompt_template: str = None,\n    attrs_name: list[str] = None,\n    render_dir: str = \"urdf_renders\",\n    render_view_num: int = 4,\n    decompose_convex: bool = False,\n    rotate_xyzw: list[float] = (0.7071, 0, 0, 0.7071),\n) -&gt; None:\n    if mesh_file_list is None:\n        mesh_file_list = []\n    self.mesh_file_list = mesh_file_list\n    self.output_mesh_dir = \"mesh\"\n    self.output_render_dir = render_dir\n    self.gpt_client = gpt_client\n    self.render_view_num = render_view_num\n    if render_view_num == 4:\n        view_desc = \"This is orthographic projection showing the front, left, right and back views \"  # noqa\n    else:\n        view_desc = \"This is the rendered views \"\n\n    if prompt_template is None:\n        prompt_template = (\n            view_desc\n            + \"\"\"of the 3D object asset,\n        category: {category}.\n        You are an expert in 3D object analysis and physical property estimation.\n        Give the category of this object asset (within 3 words), (if category is\n        already provided, use it directly), accurately describe this 3D object asset (within 15 words),\n        Determine the pose of the object in the first image and estimate the true vertical height\n        (vertical projection) range of the object (in meters), i.e., how tall the object appears from top\n        to bottom in the first image. also weight range (unit: kilogram), the average\n        static friction coefficient of the object relative to rubber and the average dynamic friction\n        coefficient of the object relative to rubber. Return response in format as shown in Output Example.\n\n        Output Example:\n        Category: cup\n        Description: shiny golden cup with floral design\n        Pose: &lt;short_description_within_10_words&gt;\n        Height: 0.10-0.15 m\n        Weight: 0.3-0.6 kg\n        Static friction coefficient: 0.6\n        Dynamic friction coefficient: 0.5\n\n        IMPORTANT: Estimating Vertical Height from the First (Front View) Image and pose estimation based on all views.\n        - The \"vertical height\" refers to the real-world vertical size of the object\n        as projected in the first image, aligned with the image's vertical axis.\n        - For flat objects like plates or disks or book, if their face is visible in the front view,\n        use the diameter as the vertical height. If the edge is visible, use the thickness instead.\n        - This is not necessarily the full length of the object, but how tall it appears\n        in the first image vertically, based on its pose and orientation estimation on all views.\n        - For objects(e.g., spoons, forks, writing instruments etc.) at an angle showing in images,\n            e.g., tilted at 45\u00b0 will appear shorter vertically than when upright.\n        Estimate the vertical projection of their real length based on its pose.\n        For example:\n          - A pen standing upright in the first image (aligned with the image's vertical axis)\n            full body visible in the first image: \u2192 vertical height \u2248 0.14-0.20 m\n          - A pen lying flat in the first image or either the tip or the tail is facing the image\n            (showing thickness or as a circle) \u2192 vertical height \u2248 0.018-0.025 m\n          - Tilted pen in the first image (e.g., ~45\u00b0 angle): vertical height \u2248 0.07-0.12 m\n        - Use the rest views to help determine the object's 3D pose and orientation.\n        Assume the object is in real-world scale and estimate the approximate vertical height\n        based on the pose estimation and how large it appears vertically in the first image.\n        \"\"\"\n        )\n\n    self.prompt_template = prompt_template\n    if attrs_name is None:\n        attrs_name = [\n            \"category\",\n            \"description\",\n            \"min_height\",\n            \"max_height\",\n            \"real_height\",\n            \"min_mass\",\n            \"max_mass\",\n            \"version\",\n            \"generate_time\",\n            \"gs_model\",\n        ]\n    self.attrs_name = attrs_name\n    self.decompose_convex = decompose_convex\n    # Rotate 90 degrees around the X-axis from blender to align with simulators.\n    self.rotate_xyzw = rotate_xyzw\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.urdf_convertor.URDFGenerator.__call__","title":"__call__","text":"<pre><code>__call__(mesh_path: str, output_root: str, text_prompt: str = None, category: str = 'unknown', **kwargs)\n</code></pre> <p>Generates a URDF file for a mesh asset.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_path</code> <code>str</code> <p>Path to mesh file.</p> required <code>output_root</code> <code>str</code> <p>Directory for outputs.</p> required <code>text_prompt</code> <code>str</code> <p>Prompt for GPT.</p> <code>None</code> <code>category</code> <code>str</code> <p>Asset category.</p> <code>'unknown'</code> <code>**kwargs</code> <p>Additional attributes.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <p>Path to generated URDF file.</p> Source code in <code>embodied_gen/validators/urdf_convertor.py</code> <pre><code>def __call__(\n    self,\n    mesh_path: str,\n    output_root: str,\n    text_prompt: str = None,\n    category: str = \"unknown\",\n    **kwargs,\n):\n    \"\"\"Generates a URDF file for a mesh asset.\n\n    Args:\n        mesh_path (str): Path to mesh file.\n        output_root (str): Directory for outputs.\n        text_prompt (str, optional): Prompt for GPT.\n        category (str, optional): Asset category.\n        **kwargs: Additional attributes.\n\n    Returns:\n        str: Path to generated URDF file.\n    \"\"\"\n    if text_prompt is None or len(text_prompt) == 0:\n        text_prompt = self.prompt_template\n        text_prompt = text_prompt.format(category=category.lower())\n\n    image_path = render_asset3d(\n        mesh_path,\n        output_root,\n        num_images=self.render_view_num,\n        output_subdir=self.output_render_dir,\n        no_index_file=True,\n    )\n    # image_path = combine_images_to_grid(image_path)\n    response = self.gpt_client.query(text_prompt, image_path)\n    # logger.info(response)\n    if response is None:\n        asset_attrs = {\n            \"category\": category.lower(),\n            \"description\": category.lower(),\n            \"min_height\": 1,\n            \"max_height\": 1,\n            \"min_mass\": 1,\n            \"max_mass\": 1,\n            \"mu1\": 0.8,\n            \"mu2\": 0.6,\n            \"version\": VERSION,\n            \"generate_time\": datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n        }\n    else:\n        asset_attrs = self.parse_response(response)\n    for key in self.attrs_name:\n        if key in kwargs:\n            asset_attrs[key] = kwargs[key]\n\n    asset_attrs[\"real_height\"] = round(\n        (asset_attrs[\"min_height\"] + asset_attrs[\"max_height\"]) / 2, 4\n    )\n\n    self.estimated_attrs = self.get_estimated_attributes(asset_attrs)\n\n    urdf_path = self.generate_urdf(mesh_path, output_root, asset_attrs)\n\n    logger.info(f\"response: {response}\")\n\n    return urdf_path\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.urdf_convertor.URDFGenerator.add_quality_tag","title":"add_quality_tag  <code>staticmethod</code>","text":"<pre><code>add_quality_tag(urdf_path: str, results: list, output_path: str = None) -&gt; None\n</code></pre> <p>Adds a quality tag to a URDF file.</p> <p>Parameters:</p> Name Type Description Default <code>urdf_path</code> <code>str</code> <p>Path to the URDF file.</p> required <code>results</code> <code>list</code> <p>List of [checker_name, result] pairs.</p> required <code>output_path</code> <code>str</code> <p>Output file path.</p> <code>None</code> Source code in <code>embodied_gen/validators/urdf_convertor.py</code> <pre><code>@staticmethod\ndef add_quality_tag(\n    urdf_path: str, results: list, output_path: str = None\n) -&gt; None:\n    \"\"\"Adds a quality tag to a URDF file.\n\n    Args:\n        urdf_path (str): Path to the URDF file.\n        results (list): List of [checker_name, result] pairs.\n        output_path (str, optional): Output file path.\n    \"\"\"\n    if output_path is None:\n        output_path = urdf_path\n\n    tree = ET.parse(urdf_path)\n    root = tree.getroot()\n    custom_data = ET.SubElement(root, \"custom_data\")\n    quality = ET.SubElement(custom_data, \"quality\")\n    for key, value in results:\n        checker_tag = ET.SubElement(quality, key)\n        checker_tag.text = str(value)\n\n    rough_string = ET.tostring(root, encoding=\"utf-8\")\n    formatted_string = parseString(rough_string).toprettyxml(indent=\"   \")\n    cleaned_string = \"\\n\".join(\n        [line for line in formatted_string.splitlines() if line.strip()]\n    )\n\n    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(cleaned_string)\n\n    logger.info(f\"URDF files saved to {output_path}\")\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.urdf_convertor.URDFGenerator.generate_urdf","title":"generate_urdf","text":"<pre><code>generate_urdf(input_mesh: str, output_dir: str, attr_dict: dict, output_name: str = None) -&gt; str\n</code></pre> <p>Generate a URDF file for a given mesh with specified attributes.</p> <p>Parameters:</p> Name Type Description Default <code>input_mesh</code> <code>str</code> <p>Path to the input mesh file.</p> required <code>output_dir</code> <code>str</code> <p>Directory to store the generated URDF and mesh.</p> required <code>attr_dict</code> <code>dict</code> <p>Dictionary of asset attributes.</p> required <code>output_name</code> <code>str</code> <p>Name for the URDF and robot.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the generated URDF file.</p> Source code in <code>embodied_gen/validators/urdf_convertor.py</code> <pre><code>def generate_urdf(\n    self,\n    input_mesh: str,\n    output_dir: str,\n    attr_dict: dict,\n    output_name: str = None,\n) -&gt; str:\n    \"\"\"Generate a URDF file for a given mesh with specified attributes.\n\n    Args:\n        input_mesh (str): Path to the input mesh file.\n        output_dir (str): Directory to store the generated URDF and mesh.\n        attr_dict (dict): Dictionary of asset attributes.\n        output_name (str, optional): Name for the URDF and robot.\n\n    Returns:\n        str: Path to the generated URDF file.\n    \"\"\"\n\n    # 1. Load and normalize the mesh\n    mesh = trimesh.load(input_mesh)\n    mesh_scale = np.ptp(mesh.vertices, axis=0).max()\n    mesh.vertices /= mesh_scale  # Normalize to [-0.5, 0.5]\n    raw_height = np.ptp(mesh.vertices, axis=0)[1]\n\n    # 2. Scale the mesh to real height\n    real_height = attr_dict[\"real_height\"]\n    scale = round(real_height / raw_height, 6)\n    mesh = mesh.apply_scale(scale)\n\n    # 3. Prepare output directories and save scaled mesh\n    mesh_folder = os.path.join(output_dir, self.output_mesh_dir)\n    os.makedirs(mesh_folder, exist_ok=True)\n\n    obj_name = os.path.basename(input_mesh)\n    mesh_output_path = os.path.join(mesh_folder, obj_name)\n    mesh.export(mesh_output_path)\n\n    # 4. Copy additional mesh files, if any\n    input_dir = os.path.dirname(input_mesh)\n    for file in self.mesh_file_list:\n        src_file = os.path.join(input_dir, file)\n        dest_file = os.path.join(mesh_folder, file)\n        if os.path.isfile(src_file):\n            shutil.copy(src_file, dest_file)\n\n    # 5. Determine output name\n    if output_name is None:\n        output_name = os.path.splitext(obj_name)[0]\n\n    # 6. Load URDF template and update attributes\n    robot = ET.fromstring(URDF_TEMPLATE)\n    robot.set(\"name\", output_name)\n\n    link = robot.find(\"link\")\n    if link is None:\n        raise ValueError(\"URDF template is missing 'link' element.\")\n    link.set(\"name\", output_name)\n\n    if self.rotate_xyzw is not None:\n        rpy = Rotation.from_quat(self.rotate_xyzw).as_euler(\n            \"xyz\", degrees=False\n        )\n        rpy = [str(round(num, 4)) for num in rpy]\n        link.find(\"visual/origin\").set(\"rpy\", \" \".join(rpy))\n        link.find(\"collision/origin\").set(\"rpy\", \" \".join(rpy))\n\n    # Update visual geometry\n    visual = link.find(\"visual/geometry/mesh\")\n    if visual is not None:\n        visual.set(\n            \"filename\", os.path.join(self.output_mesh_dir, obj_name)\n        )\n        visual.set(\"scale\", \"1.0 1.0 1.0\")\n\n    # Update collision geometry\n    collision = link.find(\"collision/geometry/mesh\")\n    if collision is not None:\n        collision_mesh = os.path.join(self.output_mesh_dir, obj_name)\n        if self.decompose_convex:\n            try:\n                d_params = dict(\n                    threshold=0.05, max_convex_hull=100, verbose=False\n                )\n                filename = f\"{os.path.splitext(obj_name)[0]}_collision.obj\"\n                output_path = os.path.join(mesh_folder, filename)\n                decompose_convex_mesh(\n                    mesh_output_path, output_path, **d_params\n                )\n                collision_mesh = f\"{self.output_mesh_dir}/{filename}\"\n            except Exception as e:\n                logger.warning(\n                    f\"Convex decomposition failed for {output_path}, {e}.\"\n                    \"Use original mesh for collision computation.\"\n                )\n        collision.set(\"filename\", collision_mesh)\n        collision.set(\"scale\", \"1.0 1.0 1.0\")\n\n    # Update friction coefficients\n    gazebo = link.find(\"collision/gazebo\")\n    if gazebo is not None:\n        for param, key in zip([\"mu1\", \"mu2\"], [\"mu1\", \"mu2\"]):\n            element = gazebo.find(param)\n            if element is not None:\n                element.text = f\"{attr_dict[key]:.2f}\"\n\n    # Update mass\n    inertial = link.find(\"inertial/mass\")\n    if inertial is not None:\n        mass_value = (attr_dict[\"min_mass\"] + attr_dict[\"max_mass\"]) / 2\n        inertial.set(\"value\", f\"{mass_value:.4f}\")\n\n    # Add extra_info element to the link\n    extra_info = link.find(\"extra_info/scale\")\n    if extra_info is not None:\n        extra_info.text = f\"{scale:.6f}\"\n\n    for key in self.attrs_name:\n        extra_info = link.find(f\"extra_info/{key}\")\n        if extra_info is not None and key in attr_dict:\n            extra_info.text = f\"{attr_dict[key]}\"\n\n    # 7. Write URDF to file\n    os.makedirs(output_dir, exist_ok=True)\n    urdf_path = os.path.join(output_dir, f\"{output_name}.urdf\")\n    tree = ET.ElementTree(robot)\n    tree.write(urdf_path, encoding=\"utf-8\", xml_declaration=True)\n\n    logger.info(f\"URDF file saved to {urdf_path}\")\n\n    return urdf_path\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.urdf_convertor.URDFGenerator.get_attr_from_urdf","title":"get_attr_from_urdf  <code>staticmethod</code>","text":"<pre><code>get_attr_from_urdf(urdf_path: str, attr_root: str = './/link/extra_info', attr_name: str = 'scale') -&gt; float\n</code></pre> <p>Extracts an attribute value from a URDF file.</p> <p>Parameters:</p> Name Type Description Default <code>urdf_path</code> <code>str</code> <p>Path to the URDF file.</p> required <code>attr_root</code> <code>str</code> <p>XML path to attribute root.</p> <code>'.//link/extra_info'</code> <code>attr_name</code> <code>str</code> <p>Attribute name.</p> <code>'scale'</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Attribute value, or None if not found.</p> Source code in <code>embodied_gen/validators/urdf_convertor.py</code> <pre><code>@staticmethod\ndef get_attr_from_urdf(\n    urdf_path: str,\n    attr_root: str = \".//link/extra_info\",\n    attr_name: str = \"scale\",\n) -&gt; float:\n    \"\"\"Extracts an attribute value from a URDF file.\n\n    Args:\n        urdf_path (str): Path to the URDF file.\n        attr_root (str, optional): XML path to attribute root.\n        attr_name (str, optional): Attribute name.\n\n    Returns:\n        float: Attribute value, or None if not found.\n    \"\"\"\n    if not os.path.exists(urdf_path):\n        raise FileNotFoundError(f\"URDF file not found: {urdf_path}\")\n\n    mesh_attr = None\n    tree = ET.parse(urdf_path)\n    root = tree.getroot()\n    extra_info = root.find(attr_root)\n    if extra_info is not None:\n        scale_element = extra_info.find(attr_name)\n        if scale_element is not None:\n            mesh_attr = scale_element.text\n            try:\n                mesh_attr = float(mesh_attr)\n            except ValueError as e:\n                pass\n\n    return mesh_attr\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.urdf_convertor.URDFGenerator.get_estimated_attributes","title":"get_estimated_attributes","text":"<pre><code>get_estimated_attributes(asset_attrs: dict)\n</code></pre> <p>Calculates estimated attributes from asset properties.</p> <p>Parameters:</p> Name Type Description Default <code>asset_attrs</code> <code>dict</code> <p>Asset attributes.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <p>Estimated attributes (height, mass, mu, category).</p> Source code in <code>embodied_gen/validators/urdf_convertor.py</code> <pre><code>def get_estimated_attributes(self, asset_attrs: dict):\n    \"\"\"Calculates estimated attributes from asset properties.\n\n    Args:\n        asset_attrs (dict): Asset attributes.\n\n    Returns:\n        dict: Estimated attributes (height, mass, mu, category).\n    \"\"\"\n    estimated_attrs = {\n        \"height\": round(\n            (asset_attrs[\"min_height\"] + asset_attrs[\"max_height\"]) / 2, 4\n        ),\n        \"mass\": round(\n            (asset_attrs[\"min_mass\"] + asset_attrs[\"max_mass\"]) / 2, 4\n        ),\n        \"mu\": round((asset_attrs[\"mu1\"] + asset_attrs[\"mu2\"]) / 2, 4),\n        \"category\": asset_attrs[\"category\"],\n    }\n\n    return estimated_attrs\n</code></pre>"},{"location":"api/validators.html#embodied_gen.validators.urdf_convertor.URDFGenerator.parse_response","title":"parse_response","text":"<pre><code>parse_response(response: str) -&gt; dict[str, any]\n</code></pre> <p>Parses GPT response to extract asset attributes.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>str</code> <p>GPT response string.</p> required <p>Returns:</p> Type Description <code>dict[str, any]</code> <p>dict[str, any]: Parsed attributes.</p> Source code in <code>embodied_gen/validators/urdf_convertor.py</code> <pre><code>def parse_response(self, response: str) -&gt; dict[str, any]:\n    \"\"\"Parses GPT response to extract asset attributes.\n\n    Args:\n        response (str): GPT response string.\n\n    Returns:\n        dict[str, any]: Parsed attributes.\n    \"\"\"\n    lines = response.split(\"\\n\")\n    lines = [line.strip() for line in lines if line]\n    category = lines[0].split(\": \")[1]\n    description = lines[1].split(\": \")[1]\n    min_height, max_height = map(\n        lambda x: float(x.strip().replace(\",\", \"\").split()[0]),\n        lines[3].split(\": \")[1].split(\"-\"),\n    )\n    min_mass, max_mass = map(\n        lambda x: float(x.strip().replace(\",\", \"\").split()[0]),\n        lines[4].split(\": \")[1].split(\"-\"),\n    )\n    mu1 = float(lines[5].split(\": \")[1].replace(\",\", \"\"))\n    mu2 = float(lines[6].split(\": \")[1].replace(\",\", \"\"))\n\n    return {\n        \"category\": category.lower(),\n        \"description\": description.lower(),\n        \"min_height\": round(min_height, 4),\n        \"max_height\": round(max_height, 4),\n        \"min_mass\": round(min_mass, 4),\n        \"max_mass\": round(max_mass, 4),\n        \"mu1\": round(mu1, 2),\n        \"mu2\": round(mu2, 2),\n        \"version\": VERSION,\n        \"generate_time\": datetime.now().strftime(\"%Y%m%d%H%M%S\"),\n    }\n</code></pre>"},{"location":"services/index.html","title":"Interactive 3D Generation &amp; Visualization Services","text":"<p>EmbodiedGen provides a suite of interactive services that transform images and text into physically plausible, simulator-ready 3D assets. Each service is optimized for visual quality, simulation compatibility, and scalability \u2014 making it easy to create, edit, and explore assets for digital twin, robotic simulation, and AI embodiment scenarios.</p>"},{"location":"services/index.html#prerequisites","title":"\u2699\ufe0f Prerequisites","text":"<p>Prerequisites</p> <p>Make sure to finish the Installation Guide before launching any service. Missing dependencies will cause initialization errors. Model weights are automatically downloaded on first run.</p>"},{"location":"services/index.html#overview-of-available-services","title":"\ud83e\udde9 Overview of Available Services","text":"Service Description \ud83d\uddbc\ufe0f Image to 3D Generate physically plausible 3D asset URDF from single input image, offering high-quality support for digital twin systems. \ud83d\udcdd Text to 3D Generate physically plausible 3D assets from text descriptions for a wide range of geometry and styles. \ud83c\udfa8 Texture Edit Generate visually rich textures for existing 3D meshes. \ud83d\udcf8 Asset Gallery Explore and download EmbodiedGen All-Simulators-Ready Assets."},{"location":"services/index.html#how-to-run-locally","title":"\u2699\ufe0f How to Run Locally","text":"<p>Quick Start</p> <p>Each service can be launched directly as a local Gradio app: <pre><code># Example: Run the Image-to-3D service\npython apps/image_to_3d.py\n</code></pre></p> <p>Models are automatically downloaded on first run. For full CLI usage, please check the corresponding tutorials.</p>"},{"location":"services/index.html#next-steps","title":"\ud83e\udded Next Steps","text":"<ul> <li>\ud83d\udcd8 Tutorials \u2013 Learn how to use EmbodiedGen in generating interactive 3D scenes for embodied intelligence.</li> <li>\ud83e\uddf1 API Reference \u2013 Integrate EmbodiedGen code programmatically.</li> </ul> <p>\ud83d\udca1 EmbodiedGen bridges the gap between AI-driven 3D generation and physically grounded simulation, enabling true embodiment for intelligent agents.</p>"},{"location":"services/image_to_3d.html","title":"\ud83d\uddbc\ufe0f Image-to-3D Service","text":"<p>This service launches a web application to generate physically plausible 3D asset URDF from single input image, offering high-quality support for digital twin systems.</p>"},{"location":"services/image_to_3d.html#run-the-app-service","title":"\u2601\ufe0f Run the App Service","text":"<p>Note</p> <p>Gradio servive is a simplified demonstration. For the full functionality, please refer to img3d-cli.</p> <p>Run the image-to-3D generation service locally. Models are automatically downloaded on first run, please be patient.</p> <pre><code># Run in foreground\npython apps/image_to_3d.py\n\n# Or run in the background\nCUDA_VISIBLE_DEVICES=0 nohup python apps/image_to_3d.py &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre> <p>Support the use of SAM3D or TRELLIS as 3D generation model, modify <code>GRADIO_APP</code> in <code>apps/image_to_3d.py</code> to switch model.</p> <p>Getting Started</p> <ul> <li>Try it directly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"services/text_to_3d.html","title":"\ud83d\udcdd Text-to-3D Service","text":"<p>This service launches a web application to generate physically plausible 3D assets from text descriptions for a wide range of geometry and styles.</p> <p>\"Antique brass key, intricate filigree\"</p> <p>\"Rusty old wrench, peeling paint\"</p> <p>\"Sleek black drone, red sensors\"</p> <p>\"Miniature screwdriver with bright orange handle\"</p> <p>\"European style wooden dressing table\"</p>"},{"location":"services/text_to_3d.html#run-the-app-service","title":"\u2601\ufe0f Run the App Service","text":"<p>Create 3D assets from text descriptions for a wide range of geometry and styles.</p> <p>Note</p> <p>Gradio servive is a simplified demonstration. For the full functionality, please refer to text3d-cli.</p> <p>Text-to-image model based on the Kolors model, supporting Chinese and English prompts. Models downloaded automatically on first run, please be patient.</p> <pre><code># Run in foreground\npython apps/text_to_3d.py\n\n# Or run in the background\nCUDA_VISIBLE_DEVICES=0 nohup python apps/text_to_3d.py &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre> <p>Getting Started</p> <ul> <li>You can also try Text-to-3D instantly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"services/texture_edit.html","title":"\ud83c\udfa8 Texture Generation Service","text":"<p>This service launches a web application to generate visually rich textures for 3D mesh.</p>"},{"location":"services/texture_edit.html#run-the-app-service","title":"\u2601\ufe0f Run the App Service","text":"<p>Note</p> <p>Gradio servive is a simplified demonstration. For the full functionality, please refer to texture-cli.</p> <p>Run the texture generation service locally. Models downloaded automatically on first run, see <code>download_kolors_weights</code>, <code>geo_cond_mv</code>.</p> <pre><code># Run in foreground\npython apps/texture_edit.py\n\n# Or run in the background\nCUDA_VISIBLE_DEVICES=0 nohup python apps/texture_edit.py &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre> <p>Getting Started</p> <ul> <li>Try it directly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"services/visualize_asset.html","title":"\ud83d\udcf8 EmbodiedGen All-Simulators-Ready Assets Gallery","text":"<p>Getting Started</p> <ul> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"tutorials/index.html","title":"Tutorials &amp; Interface Usage","text":"<p>Welcome to the tutorials for <code>EmbodiedGen</code>. <code>EmbodiedGen</code> is a powerful toolset for generating 3D assets, textures, scenes, and interactive layouts ready for simulators and digital twin environments.</p>"},{"location":"tutorials/index.html#prerequisites","title":"\u2699\ufe0f Prerequisites","text":"<p>Prerequisites</p> <p>Make sure to finish the Installation Guide before starting tutorial. Missing dependencies will cause initialization errors. Model weights are automatically downloaded on first run.</p>"},{"location":"tutorials/index.html#image-to-3d","title":"\ud83d\uddbc\ufe0f Image-to-3D","text":"<p>Generate physically plausible 3D assets from a single input image, supporting digital twin and simulation environments.</p>"},{"location":"tutorials/index.html#text-to-3d","title":"\ud83d\udcdd Text-to-3D","text":"<p>Create physically plausible 3D assets from text descriptions, supporting a wide range of geometry, style, and material details.</p> <p>\"small bronze figurine of a lion\"</p> <p>\"A globe with wooden base\"</p> <p>\"wooden table with embroidery\"</p>"},{"location":"tutorials/index.html#texture-generation","title":"\ud83c\udfa8 Texture Generation","text":"<p>Generate high-quality textures for 3D meshes using text prompts, supporting both Chinese and English, to enhance the visual appearance of existing 3D assets.</p>"},{"location":"tutorials/index.html#3d-scene-generation","title":"\ud83c\udf0d 3D Scene Generation","text":"<p>Generate physically consistent and visually coherent 3D environments from text prompts. Typically used as background 3DGS scenes in simulators for efficient and photo-realistic rendering.</p> <p></p>"},{"location":"tutorials/index.html#layout-generation","title":"\ud83c\udfde\ufe0f Layout Generation","text":"<p>Generate diverse, physically realistic, and scalable interactive 3D scenes from natural language task descriptions, while also modeling the robot and manipulable objects.</p>"},{"location":"tutorials/index.html#parallel-simulation","title":"\ud83c\udfce\ufe0f Parallel Simulation","text":"<p>Generate multiple parallel simulation environments with <code>gym.make</code> and record sensor and trajectory data.</p>"},{"location":"tutorials/index.html#use-in-any-simulator","title":"\ud83c\udfae Use in Any Simulator","text":"<p>Seamlessly use EmbodiedGen-generated assets in major simulators like IsaacSim, MuJoCo, Genesis, PyBullet, IsaacGym, and SAPIEN, featuring accurate physical collisions and consistent visual appearance.</p>"},{"location":"tutorials/index.html#real-to-sim-digital-twin-creation","title":"\ud83d\udd27 Real-to-Sim Digital Twin Creation","text":""},{"location":"tutorials/any_simulators.html","title":"\ud83c\udfae Use EmbodiedGen in Any Simulator","text":"<p>Leverage EmbodiedGen-generated assets with accurate physical collisions and consistent visual appearance across major simulation engines \u2014  IsaacSim, MuJoCo, Genesis, PyBullet, IsaacGym, and SAPIEN.</p> <p>Universal Compatibility</p> <p>EmbodiedGen assets follow standardized URDF semantics with physically consistent collision meshes, enabling seamless loading across multiple simulation frameworks \u2014 no manual editing needed.</p>"},{"location":"tutorials/any_simulators.html#supported-simulators","title":"\ud83e\udde9 Supported Simulators","text":"Simulator Conversion Class IsaacSim <code>MeshtoUSDConverter</code> MuJoCo / Genesis <code>MeshtoMJCFConverter</code> SAPIEN / IsaacGym / PyBullet <code>.urdf</code> generated by EmbodiedGen can be used directly <p>Simulator Integration Overview</p> <p>This table summarizes the compatibility of EmbodiedGen assets with various simulators:</p> Simulator Supported Format Notes IsaacSim USD / .usda Use <code>MeshtoUSDConverter</code> to convert mesh to USD format. MuJoCo MJCF (.xml) Use <code>MeshtoMJCFConverter</code> for physics-ready assets. Genesis MJCF (.xml) Same as MuJoCo; fully compatible with Genesis scenes. SAPIEN URDF (.urdf) Can directly load EmbodiedGen <code>.urdf</code> assets. IsaacGym URDF (.urdf) Directly usable. PyBullet URDF (.urdf) Directly usable."},{"location":"tutorials/any_simulators.html#example-conversion-to-target-simulator","title":"\ud83e\uddf1 Example: Conversion to Target Simulator","text":"<pre><code>from embodied_gen.data.asset_converter import cvt_embodiedgen_asset_to_anysim\nfrom embodied_gen.utils.enum import AssetType, SimAssetMapper\nfrom typing import Literal\n\nsimulator_name: Literal[\n    \"isaacsim\",\n    \"isaacgym\",\n    \"genesis\",\n    \"pybullet\",\n    \"sapien3\",\n    \"mujoco\",\n] = \"mujoco\"\n\ndst_asset_path = cvt_embodiedgen_asset_to_anysim(\n    urdf_files=[\n        \"path1_to_embodiedgen_asset/asset.urdf\",\n        \"path2_to_embodiedgen_asset/asset.urdf\",\n    ],\n    target_dirs=[\n        \"path1_to_target_dir/asset.usd\",\n        \"path2_to_target_dir/asset.usd\",\n    ],\n    target_type=SimAssetMapper[simulator_name],\n    source_type=AssetType.MESH,\n    overwrite=True,\n)\n</code></pre> <p>Collision and visualization mesh across simulators, showing consistent geometry and material fidelity.</p>"},{"location":"tutorials/digital_twin.html","title":"Real-to-Sim Digital Twin Creation","text":""},{"location":"tutorials/gym_env.html","title":"Simulation in Parallel Envs","text":"<p>Generate multiple parallel simulation environments with <code>gym.make</code> and record sensor and trajectory data.</p>"},{"location":"tutorials/gym_env.html#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<pre><code>python embodied_gen/scripts/parallel_sim.py \\\n--layout_file \"outputs/layouts_gen/task_0000/layout.json\" \\\n--output_dir \"outputs/parallel_sim/task_0000\" \\\n--num_envs 16\n</code></pre>"},{"location":"tutorials/image_to_3d.html","title":"\ud83d\uddbc\ufe0f Image-to-3D: Physically Plausible 3D Asset Generation","text":"<p>Generate physically plausible 3D assets from a single input image, supporting digital twin and simulation environments.</p>"},{"location":"tutorials/image_to_3d.html#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<p>Support the use of SAM3D or TRELLIS as 3D generation model, modify <code>IMAGE3D_MODEL</code> in <code>embodied_gen/scripts/imageto3d.py</code> to switch model.</p> <pre><code>img3d-cli --image_path apps/assets/example_image/sample_00.jpg \\\napps/assets/example_image/sample_01.jpg \\\n--n_retry 1 --output_root outputs/imageto3d\n</code></pre> <p>You will get the following results:</p> <p>The generated results are organized as follows: <pre><code>outputs/imageto3d/sample_xx/result\n\u251c\u2500\u2500 mesh\n\u2502   \u251c\u2500\u2500 material_0.png\n\u2502   \u251c\u2500\u2500 material.mtl\n\u2502   \u251c\u2500\u2500 sample_xx_collision.ply\n\u2502   \u251c\u2500\u2500 sample_xx.glb\n\u2502   \u251c\u2500\u2500 sample_xx_gs.ply\n\u2502   \u2514\u2500\u2500 sample_xx.obj\n\u251c\u2500\u2500 sample_xx.urdf\n\u2514\u2500\u2500 video.mp4\n</code></pre></p> <ul> <li><code>mesh/</code> \u2192 Geometry and texture files, including visual mesh, collision mesh and 3DGS.</li> <li><code>*.urdf</code> \u2192 Simulator-ready URDF with collision and visual meshes</li> <li><code>video.mp4</code> \u2192 Preview of the generated 3D asset</li> </ul> <p>Getting Started</p> <ul> <li>Try it directly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"tutorials/layout_gen.html","title":"\ud83c\udfde\ufe0f Layout Generation \u2014 Interactive 3D Scenes","text":"<p>Layout Generation enables the generation of diverse, physically realistic, and scalable interactive 3D scenes directly from natural language task descriptions, while also modeling the robot's pose and relationships with manipulable objects. Target objects are randomly placed within the robot's reachable range, making the scenes readily usable for downstream simulation and reinforcement learning tasks in any mainstream simulator.</p> <p>Model Requirement</p> <p>The text-to-image model is based on <code>SD3.5 Medium</code>. Usage requires agreement to the model license.</p>"},{"location":"tutorials/layout_gen.html#prerequisites-prepare-background-3d-scenes","title":"Prerequisites \u2014 Prepare Background 3D Scenes","text":"<p>Before running <code>layout-cli</code>, you need to prepare background 3D scenes. You can either generate your own using the <code>scene3d-cli</code>, or download pre-generated backgrounds for convenience.</p> <p>Each scene takes approximately 30 minutes to generate. For efficiency, we recommend pre-generating and listing them in <code>outputs/bg_scenes/scene_list.txt</code>.</p> <pre><code># Option 1: Download pre-generated backgrounds (~4 GB)\nhf download xinjjj/scene3d-bg --repo-type dataset --local-dir outputs\n\n# Option 2: Download a larger background set (~14 GB)\nhf download xinjjj..RLv2-BG --repo-type dataset --local-dir outputs\n</code></pre>"},{"location":"tutorials/layout_gen.html#generate-interactive-layout-scenes","title":"Generate Interactive Layout Scenes","text":"<p>Use the <code>layout-cli</code> to create interactive 3D scenes based on task descriptions. Each layout generation takes approximately 30 minutes.</p> <pre><code>layout-cli \\\n  --task_descs \"Place the pen in the mug on the desk\" \\\n               \"Put the fruit on the table on the plate\" \\\n  --bg_list \"outputs/bg_scenes/scene_list.txt\" \\\n  --output_root \"outputs/layouts_gen\" \\\n  --insert_robot\n</code></pre> <p>You will get the following results:</p>"},{"location":"tutorials/layout_gen.html#batch-generation","title":"Batch Generation","text":"<p>You can also run multiple tasks via a task list file in the backend.</p> <pre><code>CUDA_VISIBLE_DEVICES=0 nohup layout-cli \\\n  --task_descs \"apps/assets/example_layout/task_list.txt\" \\\n  --bg_list \"outputs/bg_scenes/scene_list.txt\" \\\n  --n_image_retry 4 --n_asset_retry 3 --n_pipe_retry 2 \\\n  --output_root \"outputs/layouts_gens\" \\\n  --insert_robot &gt; layouts_gens.log &amp;\n</code></pre> <p>\ud83d\udca1 Remove <code>--insert_robot</code> if you don\u2019t need robot pose consideration in layout generation.</p>"},{"location":"tutorials/layout_gen.html#layout-randomization","title":"Layout Randomization","text":"<p>Using <code>compose_layout.py</code>, you can recompose the layout of the generated interactive 3D scenes.</p> <pre><code>python embodied_gen/scripts/compose_layout.py \\\n--layout_path \"outputs/layouts_gens/task_0000/layout.json\" \\\n--output_dir \"outputs/layouts_gens/task_0000/recompose\" \\\n--insert_robot\n</code></pre>"},{"location":"tutorials/layout_gen.html#load-interactive-3d-scenes-in-simulators","title":"Load Interactive 3D Scenes in Simulators","text":"<p>We provide <code>sim-cli</code>, that allows users to easily load generated layouts into an interactive 3D simulation using the SAPIEN engine.</p> <pre><code>sim-cli --layout_path \"outputs/layouts_gen/task_0000/layout.json\" \\\n--output_dir \"outputs/layouts_gen/task_0000/sapien_render\" --insert_robot\n</code></pre> <p>Recommended Workflow</p> <ol> <li>Generate or download background scenes using <code>scene3d-cli</code>.</li> <li>Create interactive layouts from task descriptions using <code>layout-cli</code>.</li> <li>Optionally recompose them using <code>compose_layout.py</code>.</li> <li>Load the final layouts into simulators with <code>sim-cli</code>.</li> </ol>"},{"location":"tutorials/scene_gen.html","title":"\ud83c\udf0d 3D Scene Generation","text":"<p>Generate physically consistent and visually coherent 3D environments from text prompts. Typically used as background 3DGS scenes in simulators for efficient and photo-realistic rendering.</p> <p></p>"},{"location":"tutorials/scene_gen.html#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<p>\ud83d\udca1 Run <code>bash install.sh extra</code> to install additional dependencies if you plan to use <code>scene3d-cli</code>.</p> <p>It typically takes ~30 minutes per scene to generate both the colored mesh and 3D Gaussian Splat(3DGS) representation.</p> <pre><code>CUDA_VISIBLE_DEVICES=0 scene3d-cli \\\n  --prompts \"Art studio with easel and canvas\" \\\n  --output_dir outputs/bg_scenes/ \\\n  --seed 0 \\\n  --gs3d.max_steps 4000 \\\n  --disable_pano_check\n</code></pre> <p>The generated results are organized as follows: <pre><code>outputs/bg_scenes/scene_000\n\u251c\u2500\u2500 gs_model.ply\n\u251c\u2500\u2500 gsplat_cfg.yml\n\u251c\u2500\u2500 mesh_model.ply\n\u251c\u2500\u2500 pano_image.png\n\u251c\u2500\u2500 prompt.txt\n\u2514\u2500\u2500 video.mp4\n</code></pre></p> <ul> <li><code>gs_model.ply</code> \u2192 Generated 3D scene in 3D Gaussian Splat representation.</li> <li><code>mesh_model.ply</code> \u2192 Color mesh representation of the generated scene.</li> <li><code>gsplat_cfg.yml</code> \u2192 Configuration file for 3DGS training and rendering parameters.</li> <li><code>pano_image.png</code> \u2192 Generated panoramic view image.</li> <li><code>prompt.txt</code> \u2192 Original scene generation prompt for traceability.</li> <li><code>video.mp4</code> \u2192 Preview RGB and depth preview of the generated 3D scene.</li> </ul> <p>Usage Notes</p> <ul> <li><code>3D Scene Generation</code> produces background 3DGS scenes optimized for efficient rendering in simulation environments. We also provide hybrid rendering examples combining background 3DGS with foreground interactive assets, see the example for details.</li> <li>In Layout Generation, we further demonstrate task-desc-driven interactive 3D scene generation, building complete 3D scenes based on natural language task descriptions. See the Layout Generation Guide.</li> </ul>"},{"location":"tutorials/text_to_3d.html","title":"\ud83d\udcdd Text-to-3D: Generate 3D Assets from Text","text":"<p>Create physically plausible 3D assets from text descriptions, supporting a wide range of geometry, style, and material details.</p>"},{"location":"tutorials/text_to_3d.html#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<p>Basic CLI(recommend)</p> <p>Text-to-image model based on Stable Diffusion 3.5 Medium\uff0c English prompts only. Usage requires agreement to the model license (click \u201cAccept\u201d).</p> <pre><code>text3d-cli \\\n  --prompts \"small bronze figurine of a lion\" \"A globe with wooden base\" \"wooden table with embroidery\" \\\n  --n_image_retry 1 \\\n  --n_asset_retry 1 \\\n  --n_pipe_retry 1 \\\n  --seed_img 0 \\\n  --output_root outputs/textto3d\n</code></pre> <ul> <li><code>--n_image_retry</code>: Number of retries per prompt for text-to-image generation</li> <li><code>--n_asset_retry</code>: Retry attempts for image-to-3D assets generation</li> <li><code>--n_pipe_retry</code>: Pipeline retry for end-to-end 3D asset quality check</li> <li><code>--seed_img</code>: Optional initial seed image for style guidance</li> <li><code>--output_root</code>: Directory to save generated assets</li> </ul> <p>For large-scale 3D asset generation, set <code>--n_image_retry=4</code> <code>--n_asset_retry=3</code> <code>--n_pipe_retry=2</code>, slower but better, via automatic checking and retries. For more diverse results, omit <code>--seed_img</code>.</p> <p>You will get the following results:</p> <p>\"small bronze figurine of a lion\"</p> <p>\"A globe with wooden base\"</p> <p>\"wooden table with embroidery\"</p> <p>Kolors Model CLI (Supports Chinese &amp; English Prompts): <pre><code>bash embodied_gen/scripts/textto3d.sh \\\n    --prompts \"A globe with wooden base and latitude and longitude lines\" \"\u6a59\u8272\u7535\u52a8\u624b\u94bb\uff0c\u6709\u78e8\u635f\u7ec6\u8282\" \\\n    --output_root outputs/textto3d_k\n</code></pre></p> <p>Models with more permissive licenses can be found in <code>embodied_gen/models/image_comm_model.py</code>.</p> <p>The generated results are organized as follows: <pre><code>outputs/textto3d\n\u251c\u2500\u2500 asset3d\n\u2502   \u251c\u2500\u2500 sample3d_xx\n\u2502   \u2502   \u2514\u2500\u2500 result\n\u2502   \u2502       \u251c\u2500\u2500 mesh\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 material_0.png\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 material.mtl\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 sample3d_xx_collision.obj\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 sample3d_xx.glb\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 sample3d_xx_gs.ply\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 sample3d_xx.obj\n\u2502   \u2502       \u251c\u2500\u2500 sample3d_xx.urdf\n\u2502   \u2502       \u2514\u2500\u2500 video.mp4\n\u2514\u2500\u2500 images\n    \u251c\u2500\u2500 sample3d_xx.png\n    \u251c\u2500\u2500 sample3d_xx_raw.png\n</code></pre></p> <ul> <li><code>mesh/</code> \u2192 3D geometry and texture files for the asset, including visual mesh, collision mesh and 3DGS</li> <li><code>*.urdf</code> \u2192 Simulator-ready URDF including collision and visual meshes</li> <li><code>video.mp4</code> \u2192 Preview video of the generated 3D asset</li> <li><code>images/sample3d_xx.png</code> \u2192 Foreground-extracted image used for image-to-3D step</li> <li><code>images/sample3d_xx_raw.png</code> \u2192 Original generated image from the text-to-image step</li> </ul> <p>Getting Started</p> <ul> <li>You can also try Text-to-3D instantly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"tutorials/texture_edit.html","title":"\ud83c\udfa8 Texture Generation: Create Visually Rich Textures for 3D Meshes","text":"<p>Generate high-quality textures for 3D meshes using text prompts, supporting both Chinese and English. This allows you to enhance the visual appearance of existing 3D assets for simulation, visualization, or digital twin applications.</p>"},{"location":"tutorials/texture_edit.html#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<pre><code>texture-cli \\\n  --mesh_path \"apps/assets/example_texture/meshes/robot_text.obj\" \\\n              \"apps/assets/example_texture/meshes/horse.obj\" \\\n  --prompt \"\u4e3e\u7740\u724c\u5b50\u7684\u5199\u5b9e\u98ce\u683c\u673a\u5668\u4eba\uff0c\u5927\u773c\u775b\uff0c\u724c\u5b50\u4e0a\u5199\u7740\u201cHello\u201d\u7684\u6587\u5b57\" \\\n           \"A gray horse head with flying mane and brown eyes\" \\\n  --output_root \"outputs/texture_gen\" \\\n  --seed 0\n</code></pre> <ul> <li><code>--mesh_path</code> \u2192 Path(s) to input 3D mesh files</li> <li><code>--prompt</code> \u2192 Text prompt(s) describing desired texture/style for each mesh</li> <li><code>--output_root</code> \u2192 Directory to save textured meshes and related outputs</li> <li><code>--seed</code> \u2192 Random seed for reproducible texture generation</li> </ul> <p>You will get the following results:</p> <p>Getting Started</p> <ul> <li>Try it directly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"}]}