{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc4b Welcome to EmbodiedGen","text":"<p>EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence.</p> <p></p> <p>EmbodiedGen is a generative engine to create diverse and interactive 3D worlds composed of high-quality 3D assets(mesh &amp; 3DGS) with plausible physics, leveraging generative AI to address the challenges of generalization in embodied intelligence related research.</p>"},{"location":"acknowledgement/","title":"\ud83d\ude4c Acknowledgement","text":"<p>EmbodiedGen builds upon the following amazing projects and models: \ud83c\udf1f Trellis | \ud83c\udf1f Hunyuan-Delight | \ud83c\udf1f Segment Anything | \ud83c\udf1f Rembg | \ud83c\udf1f RMBG-1.4 | \ud83c\udf1f Stable Diffusion x4 | \ud83c\udf1f Real-ESRGAN | \ud83c\udf1f Kolors | \ud83c\udf1f ChatGLM3 | \ud83c\udf1f Aesthetic Score | \ud83c\udf1f Pano2Room | \ud83c\udf1f Diffusion360 | \ud83c\udf1f Kaolin | \ud83c\udf1f diffusers | \ud83c\udf1f gsplat | \ud83c\udf1f QWEN-2.5VL | \ud83c\udf1f GPT4o | \ud83c\udf1f SD3.5 | \ud83c\udf1f ManiSkill</p>"},{"location":"acknowledgement/#citation","title":"\ud83d\udcda Citation","text":"<p>If you use EmbodiedGen in your research or projects, please cite:</p> <pre><code>@misc{wang2025embodiedgengenerative3dworld,\n      title={EmbodiedGen: Towards a Generative 3D World Engine for Embodied Intelligence},\n      author={Xinjie Wang and Liu Liu and Yu Cao and Ruiqi Wu and Wenkang Qin and Dehui Wang and Wei Sui and Zhizhong Su},\n      year={2025},\n      eprint={2506.10600},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO},\n      url={https://arxiv.org/abs/2506.10600},\n}\n</code></pre>"},{"location":"acknowledgement/#license","title":"\u2696\ufe0f License","text":"<p>This project is licensed under the Apache License 2.0. See the <code>LICENSE</code> file for details.</p>"},{"location":"install/","title":"\ud83d\ude80 Installation","text":""},{"location":"install/#setup-environment","title":"\u2705 Setup Environment","text":"<pre><code>git clone https://github.com/HorizonRobotics/EmbodiedGen.git\ncd EmbodiedGen\ngit checkout v0.1.5\ngit submodule update --init --recursive --progress\nconda create -n embodiedgen python=3.10.13 -y # recommended to use a new env.\nconda activate embodiedgen\nbash install.sh basic\n</code></pre>"},{"location":"install/#starting-from-docker","title":"\u2705 Starting from Docker","text":"<p>We provide a pre-built Docker image on Docker Hub with a configured environment for your convenience. For more details, please refer to Docker documentation.</p> <p>Note: Model checkpoints are not included in the image, they will be automatically downloaded on first run. You still need to set up the GPT Agent manually.</p> <pre><code>IMAGE=wangxinjie/embodiedgen:env_v0.1.x\nCONTAINER=EmbodiedGen-docker-${USER}\ndocker pull ${IMAGE}\ndocker run -itd --shm-size=\"64g\" --gpus all --cap-add=SYS_PTRACE --security-opt seccomp=unconfined --privileged --net=host --name ${CONTAINER} ${IMAGE}\ndocker exec -it ${CONTAINER} bash\n</code></pre>"},{"location":"install/#setup-gpt-agent","title":"\u2705 Setup GPT Agent","text":"<p>Update the API key in file: <code>embodied_gen/utils/gpt_config.yaml</code>.</p> <p>You can choose between two backends for the GPT agent:</p> <ul> <li><code>gpt-4o</code> (Recommended) \u2013 Use this if you have access to Azure OpenAI.</li> <li><code>qwen2.5-vl</code> \u2013 An alternative with free usage via OpenRouter, apply a free key here and update <code>api_key</code> in <code>embodied_gen/utils/gpt_config.yaml</code> (50 free requests per day)</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Welcome to the API reference for EmbodiedGen.</p> <p>This section contains detailed documentation for all public modules, classes, and functions. Use the navigation on the left (or the list below) to browse the different components.</p> <ul> <li>Data API: Tools for data processing, conversion, and rendering.</li> <li>Envs API: Simulation environment definitions.</li> <li>Models API: The core generative models (Texture, 3DGS, Layout, etc.).</li> <li>Trainer API: PyTorch-Lightning style trainers for models.</li> <li>Utilities API: Helper functions and configuration.</li> <li>Validators API: Tools for checking and validating assets.</li> </ul>"},{"location":"api/data/","title":"Data API","text":""},{"location":"api/data/#embodied_gen.data.asset_converter","title":"embodied_gen.data.asset_converter","text":""},{"location":"api/data/#embodied_gen.data.asset_converter.AssetConverterBase","title":"AssetConverterBase","text":"<p>               Bases: <code>ABC</code></p> <p>Converter abstract base class.</p>"},{"location":"api/data/#embodied_gen.data.asset_converter.AssetConverterBase.transform_mesh","title":"transform_mesh","text":"<pre><code>transform_mesh(input_mesh: str, output_mesh: str, mesh_origin: Element) -&gt; None\n</code></pre> <p>Apply transform to the mesh based on the origin element in URDF.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def transform_mesh(\n    self, input_mesh: str, output_mesh: str, mesh_origin: ET.Element\n) -&gt; None:\n    \"\"\"Apply transform to the mesh based on the origin element in URDF.\"\"\"\n    mesh = trimesh.load(input_mesh, group_material=False)\n    rpy = list(map(float, mesh_origin.get(\"rpy\").split(\" \")))\n    rotation = Rotation.from_euler(\"xyz\", rpy, degrees=False)\n    offset = list(map(float, mesh_origin.get(\"xyz\").split(\" \")))\n    os.makedirs(os.path.dirname(output_mesh), exist_ok=True)\n\n    if isinstance(mesh, trimesh.Scene):\n        combined = trimesh.Scene()\n        for mesh_part in mesh.geometry.values():\n            mesh_part.vertices = (\n                mesh_part.vertices @ rotation.as_matrix().T\n            ) + offset\n            combined.add_geometry(mesh_part)\n        _ = combined.export(output_mesh)\n    else:\n        mesh.vertices = (mesh.vertices @ rotation.as_matrix().T) + offset\n        _ = mesh.export(output_mesh)\n\n    return\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.AssetConverterFactory","title":"AssetConverterFactory","text":"<p>Factory class for creating asset converters based on target and source types.</p>"},{"location":"api/data/#embodied_gen.data.asset_converter.AssetConverterFactory.create","title":"create  <code>staticmethod</code>","text":"<pre><code>create(target_type: AssetType, source_type: AssetType = 'urdf', **kwargs) -&gt; AssetConverterBase\n</code></pre> <p>Create an asset converter instance based on target and source types.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>@staticmethod\ndef create(\n    target_type: AssetType, source_type: AssetType = \"urdf\", **kwargs\n) -&gt; AssetConverterBase:\n    \"\"\"Create an asset converter instance based on target and source types.\"\"\"\n    if target_type == AssetType.MJCF and source_type == AssetType.MESH:\n        converter = MeshtoMJCFConverter(**kwargs)\n    elif target_type == AssetType.MJCF and source_type == AssetType.URDF:\n        converter = URDFtoMJCFConverter(**kwargs)\n    elif target_type == AssetType.USD and source_type == AssetType.MESH:\n        converter = MeshtoUSDConverter(**kwargs)\n    elif target_type == AssetType.USD and source_type == AssetType.URDF:\n        converter = URDFtoUSDConverter(**kwargs)\n    else:\n        raise ValueError(\n            f\"Unsupported converter type: {source_type} -&gt; {target_type}.\"\n        )\n\n    return converter\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.AssetType","title":"AssetType  <code>dataclass</code>","text":"<pre><code>AssetType()\n</code></pre> <p>               Bases: <code>str</code></p> <p>Asset type enumeration.</p>"},{"location":"api/data/#embodied_gen.data.asset_converter.MeshtoMJCFConverter","title":"MeshtoMJCFConverter","text":"<pre><code>MeshtoMJCFConverter(**kwargs)\n</code></pre> <p>               Bases: <code>AssetConverterBase</code></p> <p>Convert URDF files into MJCF format.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n) -&gt; None:\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.MeshtoMJCFConverter.add_geometry","title":"add_geometry","text":"<pre><code>add_geometry(mujoco_element: Element, link: Element, body: Element, tag: str, input_dir: str, output_dir: str, mesh_name: str, material: Element | None = None, is_collision: bool = False) -&gt; None\n</code></pre> <p>Add geometry to the MJCF body from the URDF link.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def add_geometry(\n    self,\n    mujoco_element: ET.Element,\n    link: ET.Element,\n    body: ET.Element,\n    tag: str,\n    input_dir: str,\n    output_dir: str,\n    mesh_name: str,\n    material: ET.Element | None = None,\n    is_collision: bool = False,\n) -&gt; None:\n    \"\"\"Add geometry to the MJCF body from the URDF link.\"\"\"\n    element = link.find(tag)\n    geometry = element.find(\"geometry\")\n    mesh = geometry.find(\"mesh\")\n    filename = mesh.get(\"filename\")\n    scale = mesh.get(\"scale\", \"1.0 1.0 1.0\")\n    input_mesh = f\"{input_dir}/{filename}\"\n    output_mesh = f\"{output_dir}/{filename}\"\n    self._copy_asset_file(input_mesh, output_mesh)\n\n    mesh_origin = element.find(\"origin\")\n    if mesh_origin is not None:\n        self.transform_mesh(input_mesh, output_mesh, mesh_origin)\n\n    if is_collision:\n        mesh_parts = trimesh.load(\n            output_mesh, group_material=False, force=\"scene\"\n        )\n        mesh_parts = mesh_parts.geometry.values()\n    else:\n        mesh_parts = [trimesh.load(output_mesh, force=\"mesh\")]\n    for idx, mesh_part in enumerate(mesh_parts):\n        if is_collision:\n            idx_mesh_name = f\"{mesh_name}_{idx}\"\n            base, ext = os.path.splitext(filename)\n            idx_filename = f\"{base}_{idx}{ext}\"\n            base_outdir = os.path.dirname(output_mesh)\n            mesh_part.export(os.path.join(base_outdir, '..', idx_filename))\n            geom_attrs = {\n                \"contype\": \"1\",\n                \"conaffinity\": \"1\",\n                \"rgba\": \"1 1 1 0\",\n            }\n        else:\n            idx_mesh_name, idx_filename = mesh_name, filename\n            geom_attrs = {\"contype\": \"0\", \"conaffinity\": \"0\"}\n\n        ET.SubElement(\n            mujoco_element,\n            \"mesh\",\n            name=idx_mesh_name,\n            file=idx_filename,\n            scale=scale,\n        )\n        geom = ET.SubElement(body, \"geom\", type=\"mesh\", mesh=idx_mesh_name)\n        geom.attrib.update(geom_attrs)\n        if material is not None:\n            geom.set(\"material\", material.get(\"name\"))\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.MeshtoMJCFConverter.add_materials","title":"add_materials","text":"<pre><code>add_materials(mujoco_element: Element, link: Element, tag: str, input_dir: str, output_dir: str, name: str, reflectance: float = 0.2) -&gt; ET.Element\n</code></pre> <p>Add materials to the MJCF asset from the URDF link.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def add_materials(\n    self,\n    mujoco_element: ET.Element,\n    link: ET.Element,\n    tag: str,\n    input_dir: str,\n    output_dir: str,\n    name: str,\n    reflectance: float = 0.2,\n) -&gt; ET.Element:\n    \"\"\"Add materials to the MJCF asset from the URDF link.\"\"\"\n    element = link.find(tag)\n    geometry = element.find(\"geometry\")\n    mesh = geometry.find(\"mesh\")\n    filename = mesh.get(\"filename\")\n    dirname = os.path.dirname(filename)\n    material = None\n    for path in glob(f\"{input_dir}/{dirname}/*.png\"):\n        file_name = os.path.basename(path)\n        if \"keep_materials\" in self.kwargs:\n            find_flag = False\n            for keep_key in self.kwargs[\"keep_materials\"]:\n                if keep_key in file_name.lower():\n                    find_flag = True\n            if find_flag is False:\n                continue\n\n        self._copy_asset_file(\n            path,\n            f\"{output_dir}/{dirname}/{file_name}\",\n        )\n        texture_name = f\"texture_{name}_{os.path.splitext(file_name)[0]}\"\n        material = ET.SubElement(\n            mujoco_element,\n            \"material\",\n            name=f\"material_{name}\",\n            texture=texture_name,\n            reflectance=str(reflectance),\n        )\n        ET.SubElement(\n            mujoco_element,\n            \"texture\",\n            name=texture_name,\n            type=\"2d\",\n            file=f\"{dirname}/{file_name}\",\n        )\n\n    return material\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.MeshtoMJCFConverter.convert","title":"convert","text":"<pre><code>convert(urdf_path: str, mjcf_path: str)\n</code></pre> <p>Convert a URDF file to MJCF format.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def convert(self, urdf_path: str, mjcf_path: str):\n    \"\"\"Convert a URDF file to MJCF format.\"\"\"\n    tree = ET.parse(urdf_path)\n    root = tree.getroot()\n\n    mujoco_struct = ET.Element(\"mujoco\")\n    mujoco_struct.set(\"model\", root.get(\"name\"))\n    mujoco_asset = ET.SubElement(mujoco_struct, \"asset\")\n    mujoco_worldbody = ET.SubElement(mujoco_struct, \"worldbody\")\n\n    input_dir = os.path.dirname(urdf_path)\n    output_dir = os.path.dirname(mjcf_path)\n    os.makedirs(output_dir, exist_ok=True)\n    for idx, link in enumerate(root.findall(\"link\")):\n        link_name = link.get(\"name\", \"unnamed_link\")\n        body = ET.SubElement(mujoco_worldbody, \"body\", name=link_name)\n\n        material = self.add_materials(\n            mujoco_asset,\n            link,\n            \"visual\",\n            input_dir,\n            output_dir,\n            name=str(idx),\n        )\n        joint = ET.SubElement(body, \"joint\", attrib={\"type\": \"free\"})\n        self.add_geometry(\n            mujoco_asset,\n            link,\n            body,\n            \"visual\",\n            input_dir,\n            output_dir,\n            f\"visual_mesh_{idx}\",\n            material,\n        )\n        self.add_geometry(\n            mujoco_asset,\n            link,\n            body,\n            \"collision\",\n            input_dir,\n            output_dir,\n            f\"collision_mesh_{idx}\",\n            is_collision=True,\n        )\n\n    tree = ET.ElementTree(mujoco_struct)\n    ET.indent(tree, space=\"  \", level=0)\n\n    tree.write(mjcf_path, encoding=\"utf-8\", xml_declaration=True)\n    logger.info(f\"Successfully converted {urdf_path} \u2192 {mjcf_path}\")\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.MeshtoUSDConverter","title":"MeshtoUSDConverter","text":"<pre><code>MeshtoUSDConverter(force_usd_conversion: bool = True, make_instanceable: bool = False, simulation_app=None, **kwargs)\n</code></pre> <p>               Bases: <code>AssetConverterBase</code></p> <p>Convert Mesh file from URDF into USD format.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __init__(\n    self,\n    force_usd_conversion: bool = True,\n    make_instanceable: bool = False,\n    simulation_app=None,\n    **kwargs,\n):\n    if simulation_app is not None:\n        self.simulation_app = simulation_app\n\n    if \"exit_close\" in kwargs:\n        self.exit_close = kwargs.pop(\"exit_close\")\n    else:\n        self.exit_close = True\n\n    self.usd_parms = dict(\n        force_usd_conversion=force_usd_conversion,\n        make_instanceable=make_instanceable,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.MeshtoUSDConverter.convert","title":"convert","text":"<pre><code>convert(urdf_path: str, output_file: str)\n</code></pre> <p>Convert a URDF file to USD and post-process collision meshes.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def convert(self, urdf_path: str, output_file: str):\n    \"\"\"Convert a URDF file to USD and post-process collision meshes.\"\"\"\n    from isaaclab.sim.converters import MeshConverter, MeshConverterCfg\n    from pxr import PhysxSchema, Sdf, Usd, UsdShade\n\n    tree = ET.parse(urdf_path)\n    root = tree.getroot()\n    mesh_file = root.find(\"link/visual/geometry/mesh\").get(\"filename\")\n    input_mesh = os.path.join(os.path.dirname(urdf_path), mesh_file)\n    output_dir = os.path.abspath(os.path.dirname(output_file))\n    output_mesh = f\"{output_dir}/mesh/{os.path.basename(mesh_file)}\"\n    mesh_origin = root.find(\"link/visual/origin\")\n    if mesh_origin is not None:\n        self.transform_mesh(input_mesh, output_mesh, mesh_origin)\n\n    cfg = MeshConverterCfg(\n        asset_path=output_mesh,\n        usd_dir=output_dir,\n        usd_file_name=os.path.basename(output_file),\n        **self.usd_parms,\n    )\n    urdf_converter = MeshConverter(cfg)\n    usd_path = urdf_converter.usd_path\n    rmtree(os.path.dirname(output_mesh))\n\n    stage = Usd.Stage.Open(usd_path)\n    layer = stage.GetRootLayer()\n    with Usd.EditContext(stage, layer):\n        for prim in stage.Traverse():\n            # Change texture path to relative path.\n            if prim.GetName() == \"material_0\":\n                shader = UsdShade.Shader(prim).GetInput(\"diffuse_texture\")\n                if shader.Get() is not None:\n                    relative_path = shader.Get().path.replace(\n                        f\"{output_dir}/\", \"\"\n                    )\n                    shader.Set(Sdf.AssetPath(relative_path))\n\n            # Add convex decomposition collision and set ShrinkWrap.\n            elif prim.GetName() == \"mesh\":\n                approx_attr = prim.GetAttribute(\"physics:approximation\")\n                if not approx_attr:\n                    approx_attr = prim.CreateAttribute(\n                        \"physics:approximation\", Sdf.ValueTypeNames.Token\n                    )\n                approx_attr.Set(\"convexDecomposition\")\n\n                physx_conv_api = (\n                    PhysxSchema.PhysxConvexDecompositionCollisionAPI.Apply(\n                        prim\n                    )\n                )\n                physx_conv_api.GetShrinkWrapAttr().Set(True)\n\n                api_schemas = prim.GetMetadata(\"apiSchemas\")\n                if api_schemas is None:\n                    api_schemas = Sdf.TokenListOp()\n\n                api_list = list(api_schemas.GetAddedOrExplicitItems())\n                for api in self.DEFAULT_BIND_APIS:\n                    if api not in api_list:\n                        api_list.append(api)\n\n                api_schemas.appendedItems = api_list\n                prim.SetMetadata(\"apiSchemas\", api_schemas)\n\n    layer.Save()\n    logger.info(f\"Successfully converted {urdf_path} \u2192 {usd_path}\")\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.URDFtoMJCFConverter","title":"URDFtoMJCFConverter","text":"<pre><code>URDFtoMJCFConverter(**kwargs)\n</code></pre> <p>               Bases: <code>MeshtoMJCFConverter</code></p> <p>Convert URDF files with joints to MJCF format, handling transformations from joints.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __init__(\n    self,\n    **kwargs,\n) -&gt; None:\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.URDFtoMJCFConverter.convert","title":"convert","text":"<pre><code>convert(urdf_path: str, mjcf_path: str, **kwargs) -&gt; str\n</code></pre> <p>Convert a URDF file with joints to MJCF format.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def convert(self, urdf_path: str, mjcf_path: str, **kwargs) -&gt; str:\n    \"\"\"Convert a URDF file with joints to MJCF format.\"\"\"\n    tree = ET.parse(urdf_path)\n    root = tree.getroot()\n\n    mujoco_struct = ET.Element(\"mujoco\")\n    mujoco_struct.set(\"model\", root.get(\"name\"))\n    mujoco_asset = ET.SubElement(mujoco_struct, \"asset\")\n    mujoco_worldbody = ET.SubElement(mujoco_struct, \"worldbody\")\n\n    input_dir = os.path.dirname(urdf_path)\n    output_dir = os.path.dirname(mjcf_path)\n    os.makedirs(output_dir, exist_ok=True)\n\n    body_dict = {}\n    for idx, link in enumerate(root.findall(\"link\")):\n        link_name = link.get(\"name\", f\"unnamed_link_{idx}\")\n        body = ET.SubElement(mujoco_worldbody, \"body\", name=link_name)\n        body_dict[link_name] = body\n        if link.find(\"visual\") is not None:\n            material = self.add_materials(\n                mujoco_asset,\n                link,\n                \"visual\",\n                input_dir,\n                output_dir,\n                name=str(idx),\n            )\n            self.add_geometry(\n                mujoco_asset,\n                link,\n                body,\n                \"visual\",\n                input_dir,\n                output_dir,\n                f\"visual_mesh_{idx}\",\n                material,\n            )\n        if link.find(\"collision\") is not None:\n            self.add_geometry(\n                mujoco_asset,\n                link,\n                body,\n                \"collision\",\n                input_dir,\n                output_dir,\n                f\"collision_mesh_{idx}\",\n                is_collision=True,\n            )\n\n    # Process joints to set transformations and hierarchy\n    for joint in root.findall(\"joint\"):\n        joint_type = joint.get(\"type\")\n        if joint_type != \"fixed\":\n            logger.warning(\"Only support fixed joints in conversion now.\")\n            continue\n\n        parent_link = joint.find(\"parent\").get(\"link\")\n        child_link = joint.find(\"child\").get(\"link\")\n        origin = joint.find(\"origin\")\n        if parent_link not in body_dict or child_link not in body_dict:\n            logger.warning(\n                f\"Parent or child link not found for joint: {joint.get('name')}\"\n            )\n            continue\n\n        child_body = body_dict[child_link]\n        mujoco_worldbody.remove(child_body)\n        parent_body = body_dict[parent_link]\n        parent_body.append(child_body)\n        if origin is not None:\n            xyz = origin.get(\"xyz\", \"0 0 0\")\n            rpy = origin.get(\"rpy\", \"0 0 0\")\n            child_body.set(\"pos\", xyz)\n            child_body.set(\"euler\", rpy)\n\n    tree = ET.ElementTree(mujoco_struct)\n    ET.indent(tree, space=\"  \", level=0)\n    tree.write(mjcf_path, encoding=\"utf-8\", xml_declaration=True)\n    logger.info(f\"Successfully converted {urdf_path} \u2192 {mjcf_path}\")\n\n    return mjcf_path\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.URDFtoUSDConverter","title":"URDFtoUSDConverter","text":"<pre><code>URDFtoUSDConverter(fix_base: bool = False, merge_fixed_joints: bool = False, make_instanceable: bool = True, force_usd_conversion: bool = True, collision_from_visuals: bool = True, joint_drive=None, rotate_wxyz: tuple[float] | None = None, simulation_app=None, **kwargs)\n</code></pre> <p>               Bases: <code>MeshtoUSDConverter</code></p> <p>Convert URDF files into USD format.</p> <p>Parameters:</p> Name Type Description Default <code>fix_base</code> <code>bool</code> <p>Whether to fix the base link.</p> <code>False</code> <code>merge_fixed_joints</code> <code>bool</code> <p>Whether to merge fixed joints.</p> <code>False</code> <code>make_instanceable</code> <code>bool</code> <p>Whether to make prims instanceable.</p> <code>True</code> <code>force_usd_conversion</code> <code>bool</code> <p>Force conversion to USD.</p> <code>True</code> <code>collision_from_visuals</code> <code>bool</code> <p>Generate collisions from visuals if not provided.</p> <code>True</code> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def __init__(\n    self,\n    fix_base: bool = False,\n    merge_fixed_joints: bool = False,\n    make_instanceable: bool = True,\n    force_usd_conversion: bool = True,\n    collision_from_visuals: bool = True,\n    joint_drive=None,\n    rotate_wxyz: tuple[float] | None = None,\n    simulation_app=None,\n    **kwargs,\n):\n    self.usd_parms = dict(\n        fix_base=fix_base,\n        merge_fixed_joints=merge_fixed_joints,\n        make_instanceable=make_instanceable,\n        force_usd_conversion=force_usd_conversion,\n        collision_from_visuals=collision_from_visuals,\n        joint_drive=joint_drive,\n        **kwargs,\n    )\n    self.rotate_wxyz = rotate_wxyz\n    if simulation_app is not None:\n        self.simulation_app = simulation_app\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.URDFtoUSDConverter.convert","title":"convert","text":"<pre><code>convert(urdf_path: str, output_file: str)\n</code></pre> <p>Convert a URDF file to USD and post-process collision meshes.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def convert(self, urdf_path: str, output_file: str):\n    \"\"\"Convert a URDF file to USD and post-process collision meshes.\"\"\"\n    from isaaclab.sim.converters import UrdfConverter, UrdfConverterCfg\n    from pxr import Gf, PhysxSchema, Sdf, Usd, UsdGeom\n\n    cfg = UrdfConverterCfg(\n        asset_path=urdf_path,\n        usd_dir=os.path.abspath(os.path.dirname(output_file)),\n        usd_file_name=os.path.basename(output_file),\n        **self.usd_parms,\n    )\n\n    urdf_converter = UrdfConverter(cfg)\n    usd_path = urdf_converter.usd_path\n\n    stage = Usd.Stage.Open(usd_path)\n    layer = stage.GetRootLayer()\n    with Usd.EditContext(stage, layer):\n        for prim in stage.Traverse():\n            if prim.GetName() == \"collisions\":\n                approx_attr = prim.GetAttribute(\"physics:approximation\")\n                if not approx_attr:\n                    approx_attr = prim.CreateAttribute(\n                        \"physics:approximation\", Sdf.ValueTypeNames.Token\n                    )\n                approx_attr.Set(\"convexDecomposition\")\n\n                physx_conv_api = (\n                    PhysxSchema.PhysxConvexDecompositionCollisionAPI.Apply(\n                        prim\n                    )\n                )\n                physx_conv_api.GetShrinkWrapAttr().Set(True)\n\n                api_schemas = prim.GetMetadata(\"apiSchemas\")\n                if api_schemas is None:\n                    api_schemas = Sdf.TokenListOp()\n\n                api_list = list(api_schemas.GetAddedOrExplicitItems())\n                for api in self.DEFAULT_BIND_APIS:\n                    if api not in api_list:\n                        api_list.append(api)\n\n                api_schemas.appendedItems = api_list\n                prim.SetMetadata(\"apiSchemas\", api_schemas)\n\n    if self.rotate_wxyz is not None:\n        inner_prim = next(\n            p\n            for p in stage.GetDefaultPrim().GetChildren()\n            if p.IsA(UsdGeom.Xform)\n        )\n        xformable = UsdGeom.Xformable(inner_prim)\n        xformable.ClearXformOpOrder()\n        orient_op = xformable.AddOrientOp(UsdGeom.XformOp.PrecisionDouble)\n        orient_op.Set(Gf.Quatd(*self.rotate_wxyz))\n\n    layer.Save()\n    logger.info(f\"Successfully converted {urdf_path} \u2192 {usd_path}\")\n</code></pre>"},{"location":"api/data/#embodied_gen.data.asset_converter.cvt_embodiedgen_asset_to_anysim","title":"cvt_embodiedgen_asset_to_anysim","text":"<pre><code>cvt_embodiedgen_asset_to_anysim(urdf_files: list[str], target_type: AssetType, source_type: AssetType, overwrite: bool = False, **kwargs) -&gt; dict[str, str]\n</code></pre> <p>Convert URDF files generated by EmbodiedGen into the format required by all simulators.</p> <p>Supported simulators include SAPIEN, Isaac Sim, MuJoCo, Isaac Gym, Genesis, and Pybullet.</p> Example <p>dst_asset_path = cvt_embodiedgen_asset_to_anysim(     urdf_files,     target_type=SimAssetMapper[simulator_name],     source_type=AssetType.MESH, )</p> <p>Parameters:</p> Name Type Description Default <code>urdf_files</code> <code>List[str]</code> <p>List of URDF file paths to be converted.</p> required <code>target_type</code> <code>AssetType</code> <p>The target asset type.</p> required <code>source_type</code> <code>AssetType</code> <p>The source asset type.</p> required <code>overwrite</code> <code>bool</code> <p>Whether to overwrite existing converted files.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments for the converter.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>Dict[str, str]: A dictionary mapping the original URDF file path to the converted asset file path.</p> Source code in <code>embodied_gen/data/asset_converter.py</code> <pre><code>def cvt_embodiedgen_asset_to_anysim(\n    urdf_files: list[str],\n    target_type: AssetType,\n    source_type: AssetType,\n    overwrite: bool = False,\n    **kwargs,\n) -&gt; dict[str, str]:\n    \"\"\"Convert URDF files generated by EmbodiedGen into the format required by all simulators.\n\n    Supported simulators include SAPIEN, Isaac Sim, MuJoCo, Isaac Gym, Genesis, and Pybullet.\n\n    Example:\n        dst_asset_path = cvt_embodiedgen_asset_to_anysim(\n            urdf_files,\n            target_type=SimAssetMapper[simulator_name],\n            source_type=AssetType.MESH,\n        )\n\n    Args:\n        urdf_files (List[str]): List of URDF file paths to be converted.\n        target_type (AssetType): The target asset type.\n        source_type (AssetType): The source asset type.\n        overwrite (bool): Whether to overwrite existing converted files.\n        **kwargs: Additional keyword arguments for the converter.\n\n    Returns:\n        Dict[str, str]: A dictionary mapping the original URDF file path to the converted asset file path.\n    \"\"\"\n\n    if isinstance(urdf_files, str):\n        urdf_files = [urdf_files]\n\n    # If the target type is URDF, no conversion is needed.\n    if target_type == AssetType.URDF:\n        return {key: key for key in urdf_files}\n\n    asset_converter = AssetConverterFactory.create(\n        target_type=target_type,\n        source_type=source_type,\n        **kwargs,\n    )\n    asset_paths = dict()\n\n    with asset_converter:\n        for urdf_file in urdf_files:\n            filename = os.path.basename(urdf_file).replace(\".urdf\", \"\")\n            asset_dir = os.path.dirname(urdf_file)\n            if target_type == AssetType.MJCF:\n                target_file = f\"{asset_dir}/../mjcf/{filename}.xml\"\n            elif target_type == AssetType.USD:\n                target_file = f\"{asset_dir}/../usd/{filename}.usd\"\n            else:\n                raise NotImplementedError(\n                    f\"Target type {target_type} not supported.\"\n                )\n            if not os.path.exists(target_file):\n                asset_converter.convert(urdf_file, target_file)\n\n            asset_paths[urdf_file] = target_file\n\n    return asset_paths\n</code></pre>"},{"location":"api/data/#embodied_gen.data.datasets","title":"embodied_gen.data.datasets","text":""},{"location":"api/data/#embodied_gen.data.datasets.PanoGSplatDataset","title":"PanoGSplatDataset","text":"<pre><code>PanoGSplatDataset(data_dir: str, split: str = Literal['train', 'eval'], data_name: str = 'gs_data.pt', max_sample_num: int = None)\n</code></pre> <p>               Bases: <code>Dataset</code></p> <p>A PyTorch Dataset for loading panorama-based 3D Gaussian Splatting data.</p> <p>This dataset is designed to be compatible with train and eval pipelines that use COLMAP-style camera conventions.</p> <p>Parameters:</p> Name Type Description Default <code>data_dir</code> <code>str</code> <p>Root directory where the dataset file is located.</p> required <code>split</code> <code>str</code> <p>Dataset split to use, either \"train\" or \"eval\".</p> <code>Literal['train', 'eval']</code> <code>data_name</code> <code>str</code> <p>Name of the dataset file (default: \"gs_data.pt\").</p> <code>'gs_data.pt'</code> <code>max_sample_num</code> <code>int</code> <p>Maximum number of samples to load. If None, all available samples in the split will be used.</p> <code>None</code> Source code in <code>embodied_gen/data/datasets.py</code> <pre><code>def __init__(\n    self,\n    data_dir: str,\n    split: str = Literal[\"train\", \"eval\"],\n    data_name: str = \"gs_data.pt\",\n    max_sample_num: int = None,\n) -&gt; None:\n    self.data_path = os.path.join(data_dir, data_name)\n    self.split = split\n    self.max_sample_num = max_sample_num\n    if not os.path.exists(self.data_path):\n        raise FileNotFoundError(\n            f\"Dataset file {self.data_path} not found. Please provide the correct path.\"\n        )\n    self.data = torch.load(self.data_path, weights_only=False)\n    self.frames = self.data[split]\n    if max_sample_num is not None:\n        self.frames = self.frames[:max_sample_num]\n    self.points = self.data.get(\"points\", None)\n    self.points_rgb = self.data.get(\"points_rgb\", None)\n</code></pre>"},{"location":"api/data/#embodied_gen.data.differentiable_render","title":"embodied_gen.data.differentiable_render","text":""},{"location":"api/data/#embodied_gen.data.differentiable_render.ImageRender","title":"ImageRender","text":"<pre><code>ImageRender(render_items: list[RenderItems], camera_params: CameraSetting, recompute_vtx_normal: bool = True, with_mtl: bool = False, gen_color_gif: bool = False, gen_color_mp4: bool = False, gen_viewnormal_mp4: bool = False, gen_glonormal_mp4: bool = False, no_index_file: bool = False, light_factor: float = 1.0)\n</code></pre> <p>               Bases: <code>object</code></p> <p>A differentiable mesh renderer supporting multi-view rendering.</p> <p>This class wraps a differentiable rasterization using <code>nvdiffrast</code> to render mesh geometry to various maps (normal, depth, alpha, albedo, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>render_items</code> <code>list[RenderItems]</code> <p>A list of rendering targets to generate (e.g., IMAGE, DEPTH, NORMAL, etc.).</p> required <code>camera_params</code> <code>CameraSetting</code> <p>The camera parameters for rendering, including intrinsic and extrinsic matrices.</p> required <code>recompute_vtx_normal</code> <code>bool</code> <p>If True, recomputes vertex normals from the mesh geometry. Defaults to True.</p> <code>True</code> <code>with_mtl</code> <code>bool</code> <p>Whether to load <code>.mtl</code> material files for meshes. Defaults to False.</p> <code>False</code> <code>gen_color_gif</code> <code>bool</code> <p>Generate a GIF of rendered color images. Defaults to False.</p> <code>False</code> <code>gen_color_mp4</code> <code>bool</code> <p>Generate an MP4 video of rendered color images. Defaults to False.</p> <code>False</code> <code>gen_viewnormal_mp4</code> <code>bool</code> <p>Generate an MP4 video of view-space normals. Defaults to False.</p> <code>False</code> <code>gen_glonormal_mp4</code> <code>bool</code> <p>Generate an MP4 video of global-space normals. Defaults to False.</p> <code>False</code> <code>no_index_file</code> <code>bool</code> <p>If True, skip saving the <code>index.json</code> summary file. Defaults to False.</p> <code>False</code> <code>light_factor</code> <code>float</code> <p>A scalar multiplier for PBR light intensity. Defaults to 1.0.</p> <code>1.0</code> Source code in <code>embodied_gen/data/differentiable_render.py</code> <pre><code>def __init__(\n    self,\n    render_items: list[RenderItems],\n    camera_params: CameraSetting,\n    recompute_vtx_normal: bool = True,\n    with_mtl: bool = False,\n    gen_color_gif: bool = False,\n    gen_color_mp4: bool = False,\n    gen_viewnormal_mp4: bool = False,\n    gen_glonormal_mp4: bool = False,\n    no_index_file: bool = False,\n    light_factor: float = 1.0,\n) -&gt; None:\n    camera = init_kal_camera(camera_params)\n    self.camera = camera\n\n    # Setup MVP matrix and renderer.\n    mv = camera.view_matrix()  # (n 4 4) world2cam\n    p = camera.intrinsics.projection_matrix()\n    # NOTE: add a negative sign at P[0, 2] as the y axis is flipped in `nvdiffrast` output.  # noqa\n    p[:, 1, 1] = -p[:, 1, 1]\n    # mvp = torch.bmm(p, mv) # camera.view_projection_matrix()\n    self.mv = mv\n    self.p = p\n\n    renderer = DiffrastRender(\n        p_matrix=p,\n        mv_matrix=mv,\n        resolution_hw=camera_params.resolution_hw,\n        context=dr.RasterizeCudaContext(),\n        mask_thresh=0.5,\n        grad_db=False,\n        device=camera_params.device,\n        antialias_mask=True,\n    )\n    self.renderer = renderer\n    self.recompute_vtx_normal = recompute_vtx_normal\n    self.render_items = render_items\n    self.device = camera_params.device\n    self.with_mtl = with_mtl\n    self.gen_color_gif = gen_color_gif\n    self.gen_color_mp4 = gen_color_mp4\n    self.gen_viewnormal_mp4 = gen_viewnormal_mp4\n    self.gen_glonormal_mp4 = gen_glonormal_mp4\n    self.light_factor = light_factor\n    self.no_index_file = no_index_file\n</code></pre>"},{"location":"api/data/#embodied_gen.data.differentiable_render.ImageRender.__call__","title":"__call__","text":"<pre><code>__call__(mesh_path: str, output_dir: str, prompt: str = None) -&gt; dict[str, str]\n</code></pre> <p>Render a single mesh and return paths to the rendered outputs.</p> <p>Processes the input mesh, renders multiple modalities (e.g., normals, depth, albedo), and optionally saves video or image sequences.</p> <p>Parameters:</p> Name Type Description Default <code>mesh_path</code> <code>str</code> <p>Path to the mesh file (.obj/.glb).</p> required <code>output_dir</code> <code>str</code> <p>Directory to save rendered outputs.</p> required <code>prompt</code> <code>str</code> <p>Optional caption prompt for MP4 metadata.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, str]</code> <p>dict[str, str]: A mapping render types to the saved image paths.</p> Source code in <code>embodied_gen/data/differentiable_render.py</code> <pre><code>def __call__(\n    self, mesh_path: str, output_dir: str, prompt: str = None\n) -&gt; dict[str, str]:\n    \"\"\"Render a single mesh and return paths to the rendered outputs.\n\n    Processes the input mesh, renders multiple modalities (e.g., normals,\n    depth, albedo), and optionally saves video or image sequences.\n\n    Args:\n        mesh_path (str): Path to the mesh file (.obj/.glb).\n        output_dir (str): Directory to save rendered outputs.\n        prompt (str, optional): Optional caption prompt for MP4 metadata.\n\n    Returns:\n        dict[str, str]: A mapping render types to the saved image paths.\n    \"\"\"\n    try:\n        mesh = import_kaolin_mesh(mesh_path, self.with_mtl)\n    except Exception as e:\n        logger.error(f\"[ERROR MESH LOAD]: {e}, skip {mesh_path}\")\n        return\n\n    mesh.vertices, scale, center = normalize_vertices_array(mesh.vertices)\n    if self.recompute_vtx_normal:\n        mesh.vertex_normals = calc_vertex_normals(\n            mesh.vertices, mesh.faces\n        )\n\n    mesh = mesh.to(self.device)\n    vertices, faces, vertex_normals = (\n        mesh.vertices,\n        mesh.faces,\n        mesh.vertex_normals,\n    )\n\n    # Perform rendering.\n    data_dict = defaultdict(list)\n    if RenderItems.ALPHA.value in self.render_items:\n        masks, _ = self.renderer.render_rast_alpha(vertices, faces)\n        render_paths = save_images(\n            masks, f\"{output_dir}/{RenderItems.ALPHA}\"\n        )\n        data_dict[RenderItems.ALPHA.value] = render_paths\n\n    if RenderItems.GLOBAL_NORMAL.value in self.render_items:\n        rendered_normals, masks = self.renderer.render_global_normal(\n            vertices, faces, vertex_normals\n        )\n        if self.gen_glonormal_mp4:\n            if isinstance(rendered_normals, torch.Tensor):\n                rendered_normals = rendered_normals.detach().cpu().numpy()\n            create_mp4_from_images(\n                rendered_normals,\n                output_path=f\"{output_dir}/normal.mp4\",\n                fps=15,\n                prompt=prompt,\n            )\n        else:\n            render_paths = save_images(\n                rendered_normals,\n                f\"{output_dir}/{RenderItems.GLOBAL_NORMAL}\",\n                cvt_color=cv2.COLOR_BGR2RGB,\n            )\n            data_dict[RenderItems.GLOBAL_NORMAL.value] = render_paths\n\n        if RenderItems.VIEW_NORMAL.value in self.render_items:\n            assert (\n                RenderItems.GLOBAL_NORMAL in self.render_items\n            ), f\"Must render global normal firstly, got render_items: {self.render_items}.\"  # noqa\n            rendered_view_normals = self.renderer.transform_normal(\n                rendered_normals, self.mv, masks, to_view=True\n            )\n\n            if self.gen_viewnormal_mp4:\n                create_mp4_from_images(\n                    rendered_view_normals,\n                    output_path=f\"{output_dir}/view_normal.mp4\",\n                    fps=15,\n                    prompt=prompt,\n                )\n            else:\n                render_paths = save_images(\n                    rendered_view_normals,\n                    f\"{output_dir}/{RenderItems.VIEW_NORMAL}\",\n                    cvt_color=cv2.COLOR_BGR2RGB,\n                )\n                data_dict[RenderItems.VIEW_NORMAL.value] = render_paths\n\n    if RenderItems.POSITION_MAP.value in self.render_items:\n        rendered_position, masks = self.renderer.render_position(\n            vertices, faces\n        )\n        norm_position = self.renderer.normalize_map_by_mask(\n            rendered_position, masks\n        )\n        render_paths = save_images(\n            norm_position,\n            f\"{output_dir}/{RenderItems.POSITION_MAP}\",\n            cvt_color=cv2.COLOR_BGR2RGB,\n        )\n        data_dict[RenderItems.POSITION_MAP.value] = render_paths\n\n    if RenderItems.DEPTH.value in self.render_items:\n        rendered_depth, masks = self.renderer.render_depth(vertices, faces)\n        norm_depth = self.renderer.normalize_map_by_mask(\n            rendered_depth, masks\n        )\n        render_paths = save_images(\n            norm_depth,\n            f\"{output_dir}/{RenderItems.DEPTH}\",\n        )\n        data_dict[RenderItems.DEPTH.value] = render_paths\n\n        render_paths = save_images(\n            rendered_depth,\n            f\"{output_dir}/{RenderItems.DEPTH}_exr\",\n            to_uint8=False,\n            format=\".exr\",\n        )\n        data_dict[f\"{RenderItems.DEPTH.value}_exr\"] = render_paths\n\n    if RenderItems.IMAGE.value in self.render_items:\n        images = []\n        albedos = []\n        diffuses = []\n        masks, _ = self.renderer.render_rast_alpha(vertices, faces)\n        try:\n            for idx, cam in enumerate(self.camera):\n                image, albedo, diffuse, _ = render_pbr(\n                    mesh, cam, light_factor=self.light_factor\n                )\n                image = torch.cat([image[0], masks[idx]], axis=-1)\n                images.append(image.detach().cpu().numpy())\n\n                if RenderItems.ALBEDO.value in self.render_items:\n                    albedo = torch.cat([albedo[0], masks[idx]], axis=-1)\n                    albedos.append(albedo.detach().cpu().numpy())\n\n                if RenderItems.DIFFUSE.value in self.render_items:\n                    diffuse = torch.cat([diffuse[0], masks[idx]], axis=-1)\n                    diffuses.append(diffuse.detach().cpu().numpy())\n\n        except Exception as e:\n            logger.error(f\"[ERROR pbr render]: {e}, skip {mesh_path}\")\n            return\n\n        if self.gen_color_gif:\n            create_gif_from_images(\n                images,\n                output_path=f\"{output_dir}/color.gif\",\n                fps=15,\n            )\n\n        if self.gen_color_mp4:\n            create_mp4_from_images(\n                images,\n                output_path=f\"{output_dir}/color.mp4\",\n                fps=15,\n                prompt=prompt,\n            )\n\n        if self.gen_color_mp4 or self.gen_color_gif:\n            return data_dict\n\n        render_paths = save_images(\n            images,\n            f\"{output_dir}/{RenderItems.IMAGE}\",\n            cvt_color=cv2.COLOR_BGRA2RGBA,\n        )\n        data_dict[RenderItems.IMAGE.value] = render_paths\n\n        render_paths = save_images(\n            albedos,\n            f\"{output_dir}/{RenderItems.ALBEDO}\",\n            cvt_color=cv2.COLOR_BGRA2RGBA,\n        )\n        data_dict[RenderItems.ALBEDO.value] = render_paths\n\n        render_paths = save_images(\n            diffuses,\n            f\"{output_dir}/{RenderItems.DIFFUSE}\",\n            cvt_color=cv2.COLOR_BGRA2RGBA,\n        )\n        data_dict[RenderItems.DIFFUSE.value] = render_paths\n\n    data_dict[\"status\"] = \"success\"\n\n    logger.info(f\"Finish rendering in {output_dir}\")\n\n    return data_dict\n</code></pre>"},{"location":"api/data/#embodied_gen.data.mesh_operator","title":"embodied_gen.data.mesh_operator","text":""},{"location":"api/data/#embodied_gen.data.mesh_operator.MeshFixer","title":"MeshFixer","text":"<pre><code>MeshFixer(vertices: Union[Tensor, ndarray], faces: Union[Tensor, ndarray], device: str = 'cuda')\n</code></pre> <p>               Bases: <code>object</code></p> <p>MeshFixer simplifies and repairs 3D triangle meshes by TSDF.</p> <p>Attributes:</p> Name Type Description <code>vertices</code> <code>Tensor</code> <p>A tensor of shape (V, 3) representing vertex positions.</p> <code>faces</code> <code>Tensor</code> <p>A tensor of shape (F, 3) representing face indices.</p> <code>device</code> <code>str</code> <p>Device to run computations on, typically \"cuda\" or \"cpu\".</p> <p>Main logic reference: https://github.com/microsoft/TRELLIS/blob/main/trellis/utils/postprocessing_utils.py#L22</p> Source code in <code>embodied_gen/data/mesh_operator.py</code> <pre><code>def __init__(\n    self,\n    vertices: Union[torch.Tensor, np.ndarray],\n    faces: Union[torch.Tensor, np.ndarray],\n    device: str = \"cuda\",\n) -&gt; None:\n    self.device = device\n    if isinstance(vertices, np.ndarray):\n        vertices = torch.tensor(vertices)\n    self.vertices = vertices\n\n    if isinstance(faces, np.ndarray):\n        faces = torch.tensor(faces)\n    self.faces = faces\n</code></pre>"},{"location":"api/data/#embodied_gen.data.mesh_operator.MeshFixer.__call__","title":"__call__","text":"<pre><code>__call__(filter_ratio: float, max_hole_size: float, resolution: int, num_views: int, norm_mesh_ratio: float = 1.0) -&gt; Tuple[np.ndarray, np.ndarray]\n</code></pre> <p>Post-process the mesh by simplifying and filling holes.</p> <p>This method performs a two-step process: 1. Simplifies mesh by reducing faces using quadric edge decimation. 2. Fills holes by removing invisible faces, repairing small boundaries.</p> <p>Parameters:</p> Name Type Description Default <code>filter_ratio</code> <code>float</code> <p>Ratio of faces to simplify out. Must be in the range (0, 1).</p> required <code>max_hole_size</code> <code>float</code> <p>Maximum area of a hole to fill. Connected components of holes larger than this size will not be repaired.</p> required <code>resolution</code> <code>int</code> <p>Resolution of the rasterization buffer.</p> required <code>num_views</code> <code>int</code> <p>Number of viewpoints to sample for rasterization.</p> required <code>norm_mesh_ratio</code> <code>float</code> <p>A scaling factor applied to the vertices of the mesh during processing.</p> <code>1.0</code> <p>Returns:</p> Type Description <code>Tuple[ndarray, ndarray]</code> <p>Tuple[np.ndarray, np.ndarray]: - vertices: Simplified and repaired vertex array of (V, 3). - faces: Simplified and repaired face array of (F, 3).</p> Source code in <code>embodied_gen/data/mesh_operator.py</code> <pre><code>@spaces.GPU\ndef __call__(\n    self,\n    filter_ratio: float,\n    max_hole_size: float,\n    resolution: int,\n    num_views: int,\n    norm_mesh_ratio: float = 1.0,\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"Post-process the mesh by simplifying and filling holes.\n\n    This method performs a two-step process:\n    1. Simplifies mesh by reducing faces using quadric edge decimation.\n    2. Fills holes by removing invisible faces, repairing small boundaries.\n\n    Args:\n        filter_ratio (float): Ratio of faces to simplify out.\n            Must be in the range (0, 1).\n        max_hole_size (float): Maximum area of a hole to fill. Connected\n            components of holes larger than this size will not be repaired.\n        resolution (int): Resolution of the rasterization buffer.\n        num_views (int): Number of viewpoints to sample for rasterization.\n        norm_mesh_ratio (float, optional): A scaling factor applied to the\n            vertices of the mesh during processing.\n\n    Returns:\n        Tuple[np.ndarray, np.ndarray]:\n            - vertices: Simplified and repaired vertex array of (V, 3).\n            - faces: Simplified and repaired face array of (F, 3).\n    \"\"\"\n    self.vertices = self.vertices.to(self.device)\n    self.faces = self.faces.to(self.device)\n\n    self.simplify(ratio=filter_ratio)\n    self.fill_holes(\n        max_hole_size=max_hole_size,\n        max_hole_nbe=int(250 * np.sqrt(1 - filter_ratio)),\n        resolution=resolution,\n        num_views=num_views,\n        norm_mesh_ratio=norm_mesh_ratio,\n    )\n\n    return self.vertices_np, self.faces_np\n</code></pre>"},{"location":"api/data/#embodied_gen.data.mesh_operator.MeshFixer.simplify","title":"simplify","text":"<pre><code>simplify(ratio: float) -&gt; None\n</code></pre> <p>Simplify the mesh using quadric edge collapse decimation.</p> <p>Parameters:</p> Name Type Description Default <code>ratio</code> <code>float</code> <p>Ratio of faces to filter out.</p> required Source code in <code>embodied_gen/data/mesh_operator.py</code> <pre><code>@log_mesh_changes\ndef simplify(self, ratio: float) -&gt; None:\n    \"\"\"Simplify the mesh using quadric edge collapse decimation.\n\n    Args:\n        ratio (float): Ratio of faces to filter out.\n    \"\"\"\n    if ratio &lt;= 0 or ratio &gt;= 1:\n        raise ValueError(\"Simplify ratio must be between 0 and 1.\")\n\n    # Convert to PyVista format for simplification\n    mesh = pv.PolyData(\n        self.vertices_np,\n        np.hstack([np.full((self.faces.shape[0], 1), 3), self.faces_np]),\n    )\n    mesh.clean(inplace=True)\n    mesh.clear_data()\n    mesh = mesh.triangulate()\n    mesh = mesh.decimate(ratio, progress_bar=True)\n\n    # Update vertices and faces\n    self.vertices = torch.tensor(\n        mesh.points, device=self.device, dtype=torch.float32\n    )\n    self.faces = torch.tensor(\n        mesh.faces.reshape(-1, 4)[:, 1:],\n        device=self.device,\n        dtype=torch.int32,\n    )\n</code></pre>"},{"location":"api/data/#embodied_gen.data.backproject_v2","title":"embodied_gen.data.backproject_v2","text":""},{"location":"api/data/#embodied_gen.data.backproject_v2.TextureBacker","title":"TextureBacker","text":"<pre><code>TextureBacker(camera_params: CameraSetting, view_weights: list[float], render_wh: tuple[int, int] = (2048, 2048), texture_wh: tuple[int, int] = (2048, 2048), bake_angle_thresh: int = 75, mask_thresh: float = 0.5, smooth_texture: bool = True, inpaint_smooth: bool = False)\n</code></pre> <p>Texture baking pipeline for multi-view projection and fusion.</p> <p>This class performs UV-based texture generation for a 3D mesh using multi-view color images, depth, and normal information. The pipeline includes mesh normalization and UV unwrapping, visibility-aware back-projection, confidence-weighted texture fusion, and inpainting of missing texture regions.</p> <p>Parameters:</p> Name Type Description Default <code>camera_params</code> <code>CameraSetting</code> <p>Camera intrinsics and extrinsics used for rendering each view.</p> required <code>view_weights</code> <code>list[float]</code> <p>A list of weights for each view, used to blend confidence maps during texture fusion.</p> required <code>render_wh</code> <code>tuple[int, int]</code> <p>Resolution (width, height) for intermediate rendering passes. Defaults to (2048, 2048).</p> <code>(2048, 2048)</code> <code>texture_wh</code> <code>tuple[int, int]</code> <p>Output texture resolution (width, height). Defaults to (2048, 2048).</p> <code>(2048, 2048)</code> <code>bake_angle_thresh</code> <code>int</code> <p>Maximum angle (in degrees) between view direction and surface normal for projection to be considered valid. Defaults to 75.</p> <code>75</code> <code>mask_thresh</code> <code>float</code> <p>Threshold applied to visibility masks during rendering. Defaults to 0.5.</p> <code>0.5</code> <code>smooth_texture</code> <code>bool</code> <p>If True, apply post-processing (e.g., blurring) to the final texture. Defaults to True.</p> <code>True</code> <code>inpaint_smooth</code> <code>bool</code> <p>If True, apply inpainting to smooth.</p> <code>False</code> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def __init__(\n    self,\n    camera_params: CameraSetting,\n    view_weights: list[float],\n    render_wh: tuple[int, int] = (2048, 2048),\n    texture_wh: tuple[int, int] = (2048, 2048),\n    bake_angle_thresh: int = 75,\n    mask_thresh: float = 0.5,\n    smooth_texture: bool = True,\n    inpaint_smooth: bool = False,\n) -&gt; None:\n    self.camera_params = camera_params\n    self.renderer = None\n    self.view_weights = view_weights\n    self.device = camera_params.device\n    self.render_wh = render_wh\n    self.texture_wh = texture_wh\n    self.mask_thresh = mask_thresh\n    self.smooth_texture = smooth_texture\n    self.inpaint_smooth = inpaint_smooth\n\n    self.bake_angle_thresh = bake_angle_thresh\n    self.bake_unreliable_kernel_size = int(\n        (2 / 512) * max(self.render_wh[0], self.render_wh[1])\n    )\n</code></pre>"},{"location":"api/data/#embodied_gen.data.backproject_v2.TextureBacker.__call__","title":"__call__","text":"<pre><code>__call__(colors: list[Image], mesh: Trimesh, output_path: str) -&gt; trimesh.Trimesh\n</code></pre> <p>Runs the texture baking and exports the textured mesh.</p> <p>Parameters:</p> Name Type Description Default <code>colors</code> <code>list[Image]</code> <p>List of input view images.</p> required <code>mesh</code> <code>Trimesh</code> <p>Input mesh to be textured.</p> required <code>output_path</code> <code>str</code> <p>Path to save the output textured mesh (.obj or .glb).</p> required <p>Returns:</p> Type Description <code>Trimesh</code> <p>trimesh.Trimesh: The textured mesh with UV and texture image.</p> Source code in <code>embodied_gen/data/backproject_v2.py</code> <pre><code>def __call__(\n    self,\n    colors: list[Image.Image],\n    mesh: trimesh.Trimesh,\n    output_path: str,\n) -&gt; trimesh.Trimesh:\n    \"\"\"Runs the texture baking and exports the textured mesh.\n\n    Args:\n        colors (list[Image.Image]): List of input view images.\n        mesh (trimesh.Trimesh): Input mesh to be textured.\n        output_path (str): Path to save the output textured mesh (.obj or .glb).\n\n    Returns:\n        trimesh.Trimesh: The textured mesh with UV and texture image.\n    \"\"\"\n    mesh = self.load_mesh(mesh)\n    texture_np, mask_np = self.compute_texture(colors, mesh)\n\n    texture_np = self.uv_inpaint(mesh, texture_np, mask_np)\n    if self.smooth_texture:\n        texture_np = post_process_texture(texture_np)\n\n    vertices, faces, uv_map = self.get_mesh_np_attrs(\n        mesh, self.scale, self.center\n    )\n    textured_mesh = save_mesh_with_mtl(\n        vertices, faces, uv_map, texture_np, output_path\n    )\n\n    return textured_mesh\n</code></pre>"},{"location":"api/data/#embodied_gen.data.convex_decomposer","title":"embodied_gen.data.convex_decomposer","text":""},{"location":"api/data/#embodied_gen.data.convex_decomposer.decompose_convex_mesh","title":"decompose_convex_mesh","text":"<pre><code>decompose_convex_mesh(filename: str, outfile: str, threshold: float = 0.05, max_convex_hull: int = -1, preprocess_mode: str = 'auto', preprocess_resolution: int = 30, resolution: int = 2000, mcts_nodes: int = 20, mcts_iterations: int = 150, mcts_max_depth: int = 3, pca: bool = False, merge: bool = True, seed: int = 0, auto_scale: bool = True, scale_factor: float = 1.005, verbose: bool = False) -&gt; str\n</code></pre> <p>Decompose a mesh into convex parts using the CoACD algorithm.</p> Source code in <code>embodied_gen/data/convex_decomposer.py</code> <pre><code>def decompose_convex_mesh(\n    filename: str,\n    outfile: str,\n    threshold: float = 0.05,\n    max_convex_hull: int = -1,\n    preprocess_mode: str = \"auto\",\n    preprocess_resolution: int = 30,\n    resolution: int = 2000,\n    mcts_nodes: int = 20,\n    mcts_iterations: int = 150,\n    mcts_max_depth: int = 3,\n    pca: bool = False,\n    merge: bool = True,\n    seed: int = 0,\n    auto_scale: bool = True,\n    scale_factor: float = 1.005,\n    verbose: bool = False,\n) -&gt; str:\n    \"\"\"Decompose a mesh into convex parts using the CoACD algorithm.\"\"\"\n    coacd.set_log_level(\"info\" if verbose else \"warn\")\n\n    if os.path.exists(outfile):\n        logger.warning(f\"Output file {outfile} already exists, removing it.\")\n        os.remove(outfile)\n\n    params = dict(\n        threshold=threshold,\n        max_convex_hull=max_convex_hull,\n        preprocess_mode=preprocess_mode,\n        preprocess_resolution=preprocess_resolution,\n        resolution=resolution,\n        mcts_nodes=mcts_nodes,\n        mcts_iterations=mcts_iterations,\n        mcts_max_depth=mcts_max_depth,\n        pca=pca,\n        merge=merge,\n        seed=seed,\n    )\n\n    try:\n        decompose_convex_coacd(\n            filename, outfile, params, verbose, auto_scale, scale_factor\n        )\n        if os.path.exists(outfile):\n            return outfile\n    except Exception as e:\n        if verbose:\n            print(f\"Decompose convex first attempt failed: {e}.\")\n\n    if preprocess_mode != \"on\":\n        try:\n            params[\"preprocess_mode\"] = \"on\"\n            decompose_convex_coacd(\n                filename, outfile, params, verbose, auto_scale, scale_factor\n            )\n            if os.path.exists(outfile):\n                return outfile\n        except Exception as e:\n            if verbose:\n                print(\n                    f\"Decompose convex second attempt with preprocess_mode='on' failed: {e}\"\n                )\n\n    raise RuntimeError(f\"Convex decomposition failed on {filename}\")\n</code></pre>"},{"location":"api/data/#embodied_gen.data.convex_decomposer.decompose_convex_mp","title":"decompose_convex_mp","text":"<pre><code>decompose_convex_mp(filename: str, outfile: str, threshold: float = 0.05, max_convex_hull: int = -1, preprocess_mode: str = 'auto', preprocess_resolution: int = 30, resolution: int = 2000, mcts_nodes: int = 20, mcts_iterations: int = 150, mcts_max_depth: int = 3, pca: bool = False, merge: bool = True, seed: int = 0, verbose: bool = False, auto_scale: bool = True) -&gt; str\n</code></pre> <p>Decompose a mesh into convex parts using the CoACD algorithm in a separate process.</p> <p>See https://simulately.wiki/docs/toolkits/ConvexDecomp for details.</p> Source code in <code>embodied_gen/data/convex_decomposer.py</code> <pre><code>def decompose_convex_mp(\n    filename: str,\n    outfile: str,\n    threshold: float = 0.05,\n    max_convex_hull: int = -1,\n    preprocess_mode: str = \"auto\",\n    preprocess_resolution: int = 30,\n    resolution: int = 2000,\n    mcts_nodes: int = 20,\n    mcts_iterations: int = 150,\n    mcts_max_depth: int = 3,\n    pca: bool = False,\n    merge: bool = True,\n    seed: int = 0,\n    verbose: bool = False,\n    auto_scale: bool = True,\n) -&gt; str:\n    \"\"\"Decompose a mesh into convex parts using the CoACD algorithm in a separate process.\n\n    See https://simulately.wiki/docs/toolkits/ConvexDecomp for details.\n    \"\"\"\n    params = dict(\n        threshold=threshold,\n        max_convex_hull=max_convex_hull,\n        preprocess_mode=preprocess_mode,\n        preprocess_resolution=preprocess_resolution,\n        resolution=resolution,\n        mcts_nodes=mcts_nodes,\n        mcts_iterations=mcts_iterations,\n        mcts_max_depth=mcts_max_depth,\n        pca=pca,\n        merge=merge,\n        seed=seed,\n    )\n\n    ctx = mp.get_context(\"spawn\")\n    p = ctx.Process(\n        target=decompose_convex_coacd,\n        args=(filename, outfile, params, verbose, auto_scale),\n    )\n    p.start()\n    p.join()\n    if p.exitcode == 0 and os.path.exists(outfile):\n        return outfile\n\n    if preprocess_mode != \"on\":\n        params[\"preprocess_mode\"] = \"on\"\n        p = ctx.Process(\n            target=decompose_convex_coacd,\n            args=(filename, outfile, params, verbose, auto_scale),\n        )\n        p.start()\n        p.join()\n        if p.exitcode == 0 and os.path.exists(outfile):\n            return outfile\n\n    raise RuntimeError(f\"Convex decomposition failed on {filename}\")\n</code></pre>"},{"location":"api/envs/","title":"Envs API","text":"<p>Documentation for simulation environments and task definitions.</p>"},{"location":"api/envs/#embodied_gen.envs.pick_embodiedgen","title":"embodied_gen.envs.pick_embodiedgen","text":""},{"location":"api/models/","title":"Models API","text":""},{"location":"api/models/#embodied_gen.models.texture_model","title":"embodied_gen.models.texture_model","text":""},{"location":"api/models/#embodied_gen.models.texture_model.build_texture_gen_pipe","title":"build_texture_gen_pipe","text":"<pre><code>build_texture_gen_pipe(base_ckpt_dir: str, controlnet_ckpt: str = None, ip_adapt_scale: float = 0, device: str = 'cuda') -&gt; DiffusionPipeline\n</code></pre> <p>Build and initialize the Kolors + ControlNet (optional IP-Adapter) texture generation pipeline.</p> <p>Loads Kolors tokenizer, text encoder (ChatGLM), VAE, UNet, scheduler and (optionally) a ControlNet checkpoint plus IP-Adapter vision encoder. If <code>controlnet_ckpt</code> is not provided, the default multi-view texture ControlNet weights are downloaded automatically from the hub. When <code>ip_adapt_scale &gt; 0</code> an IP-Adapter vision encoder and its weights are also loaded and activated.</p> <p>Parameters:</p> Name Type Description Default <code>base_ckpt_dir</code> <code>str</code> <p>Root directory where Kolors (and optionally Kolors-IP-Adapter-Plus) weights are or will be stored. Required subfolders: <code>Kolors/{text_encoder,vae,unet,scheduler}</code>.</p> required <code>controlnet_ckpt</code> <code>str</code> <p>Directory containing a ControlNet checkpoint (safetensors). If <code>None</code>, downloads the default <code>texture_gen_mv_v1</code> snapshot.</p> <code>None</code> <code>ip_adapt_scale</code> <code>float</code> <p>Strength (&gt;=0) of IP-Adapter conditioning. Set &gt;0 to enable IP-Adapter; typical values: 0.4-0.8. Default: 0 (disabled).</p> <code>0</code> <code>device</code> <code>str</code> <p>Target device to move the pipeline to (e.g. <code>\"cuda\"</code>, <code>\"cuda:0\"</code>, <code>\"cpu\"</code>). Default: <code>\"cuda\"</code>.</p> <code>'cuda'</code> <p>Returns:</p> Name Type Description <code>DiffusionPipeline</code> <code>DiffusionPipeline</code> <p>A configured</p> <code>DiffusionPipeline</code> <p><code>StableDiffusionXLControlNetImg2ImgPipeline</code> ready for multi-view texture</p> <code>DiffusionPipeline</code> <p>generation (with optional IP-Adapter support).</p> Example <p>Initialize pipeline with IP-Adapter enabled. <pre><code>from embodied_gen.models.texture_model import build_texture_gen_pipe\nip_adapt_scale = 0.7\nPIPELINE = build_texture_gen_pipe(\n    base_ckpt_dir=\"./weights\",\n    ip_adapt_scale=ip_adapt_scale,\n    device=\"cuda\",\n)\nPIPELINE.set_ip_adapter_scale([ip_adapt_scale])\n</code></pre> Initialize pipeline without IP-Adapter. <pre><code>from embodied_gen.models.texture_model import build_texture_gen_pipe\nPIPELINE = build_texture_gen_pipe(\n    base_ckpt_dir=\"./weights\",\n    ip_adapt_scale=0,\n    device=\"cuda\",\n)\n</code></pre></p> Source code in <code>embodied_gen/models/texture_model.py</code> <pre><code>def build_texture_gen_pipe(\n    base_ckpt_dir: str,\n    controlnet_ckpt: str = None,\n    ip_adapt_scale: float = 0,\n    device: str = \"cuda\",\n) -&gt; DiffusionPipeline:\n    \"\"\"Build and initialize the Kolors + ControlNet (optional IP-Adapter) texture generation pipeline.\n\n    Loads Kolors tokenizer, text encoder (ChatGLM), VAE, UNet, scheduler and (optionally)\n    a ControlNet checkpoint plus IP-Adapter vision encoder. If ``controlnet_ckpt`` is\n    not provided, the default multi-view texture ControlNet weights are downloaded\n    automatically from the hub. When ``ip_adapt_scale &gt; 0`` an IP-Adapter vision\n    encoder and its weights are also loaded and activated.\n\n    Args:\n        base_ckpt_dir (str):\n            Root directory where Kolors (and optionally Kolors-IP-Adapter-Plus) weights\n            are or will be stored. Required subfolders: ``Kolors/{text_encoder,vae,unet,scheduler}``.\n        controlnet_ckpt (str, optional):\n            Directory containing a ControlNet checkpoint (safetensors). If ``None``,\n            downloads the default ``texture_gen_mv_v1`` snapshot.\n        ip_adapt_scale (float, optional):\n            Strength (&gt;=0) of IP-Adapter conditioning. Set &gt;0 to enable IP-Adapter;\n            typical values: 0.4-0.8. Default: 0 (disabled).\n        device (str, optional):\n            Target device to move the pipeline to (e.g. ``\"cuda\"``, ``\"cuda:0\"``, ``\"cpu\"``).\n            Default: ``\"cuda\"``.\n\n    Returns:\n        DiffusionPipeline: A configured\n        ``StableDiffusionXLControlNetImg2ImgPipeline`` ready for multi-view texture\n        generation (with optional IP-Adapter support).\n\n    Example:\n        Initialize pipeline with IP-Adapter enabled.\n        ```python\n        from embodied_gen.models.texture_model import build_texture_gen_pipe\n        ip_adapt_scale = 0.7\n        PIPELINE = build_texture_gen_pipe(\n            base_ckpt_dir=\"./weights\",\n            ip_adapt_scale=ip_adapt_scale,\n            device=\"cuda\",\n        )\n        PIPELINE.set_ip_adapter_scale([ip_adapt_scale])\n        ```\n        Initialize pipeline without IP-Adapter.\n        ```python\n        from embodied_gen.models.texture_model import build_texture_gen_pipe\n        PIPELINE = build_texture_gen_pipe(\n            base_ckpt_dir=\"./weights\",\n            ip_adapt_scale=0,\n            device=\"cuda\",\n        )\n        ```\n    \"\"\"\n\n    download_kolors_weights(f\"{base_ckpt_dir}/Kolors\")\n    logger.info(f\"Load Kolors weights...\")\n    tokenizer = ChatGLMTokenizer.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/text_encoder\"\n    )\n    text_encoder = ChatGLMModel.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/text_encoder\", torch_dtype=torch.float16\n    ).half()\n    vae = AutoencoderKL.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/vae\", revision=None\n    ).half()\n    unet = UNet2DConditionModel.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/unet\", revision=None\n    ).half()\n    scheduler = EulerDiscreteScheduler.from_pretrained(\n        f\"{base_ckpt_dir}/Kolors/scheduler\"\n    )\n\n    if controlnet_ckpt is None:\n        suffix = \"texture_gen_mv_v1\"  # \"geo_cond_mv\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        controlnet_ckpt = os.path.join(model_path, suffix)\n\n    controlnet = ControlNetModel.from_pretrained(\n        controlnet_ckpt, use_safetensors=True\n    ).half()\n\n    # IP-Adapter model\n    image_encoder = None\n    clip_image_processor = None\n    if ip_adapt_scale &gt; 0:\n        image_encoder = CLIPVisionModelWithProjection.from_pretrained(\n            f\"{base_ckpt_dir}/Kolors-IP-Adapter-Plus/image_encoder\",\n            # ignore_mismatched_sizes=True,\n        ).to(dtype=torch.float16)\n        ip_img_size = 336\n        clip_image_processor = CLIPImageProcessor(\n            size=ip_img_size, crop_size=ip_img_size\n        )\n\n    pipe = StableDiffusionXLControlNetImg2ImgPipeline(\n        vae=vae,\n        controlnet=controlnet,\n        text_encoder=text_encoder,\n        tokenizer=tokenizer,\n        unet=unet,\n        scheduler=scheduler,\n        image_encoder=image_encoder,\n        feature_extractor=clip_image_processor,\n        force_zeros_for_empty_prompt=False,\n    )\n\n    if ip_adapt_scale &gt; 0:\n        if hasattr(pipe.unet, \"encoder_hid_proj\"):\n            pipe.unet.text_encoder_hid_proj = pipe.unet.encoder_hid_proj\n        pipe.load_ip_adapter(\n            f\"{base_ckpt_dir}/Kolors-IP-Adapter-Plus\",\n            subfolder=\"\",\n            weight_name=[\"ip_adapter_plus_general.bin\"],\n        )\n        pipe.set_ip_adapter_scale([ip_adapt_scale])\n\n    pipe = pipe.to(device)\n    pipe.enable_model_cpu_offload()\n\n    return pipe\n</code></pre>"},{"location":"api/models/#embodied_gen.models.gs_model","title":"embodied_gen.models.gs_model","text":""},{"location":"api/models/#embodied_gen.models.gs_model.GaussianOperator","title":"GaussianOperator  <code>dataclass</code>","text":"<pre><code>GaussianOperator(_opacities: Tensor, _means: Tensor, _scales: Tensor, _quats: Tensor, _rgbs: Optional[Tensor] = None, _features_dc: Optional[Tensor] = None, _features_rest: Optional[Tensor] = None, sh_degree: Optional[int] = 0, device: str = 'cuda')\n</code></pre> <p>               Bases: <code>GaussianBase</code></p> <p>Gaussian Splatting operator.</p> <p>Supports transformation, scaling, color computation, and rasterization-based rendering.</p> Inherits <p>GaussianBase: Base class with Gaussian params (means, scales, etc.)</p> <p>Functionality includes: - Applying instance poses to transform Gaussian means and quaternions. - Scaling Gaussians to a real-world size. - Computing colors using spherical harmonics. - Rendering images via differentiable rasterization. - Exporting transformed and rescaled models to .ply format.</p>"},{"location":"api/models/#embodied_gen.models.gs_model.GaussianOperator.get_gaussians","title":"get_gaussians","text":"<pre><code>get_gaussians(c2w: Tensor = None, instance_pose: Tensor = None, apply_activate: bool = False) -&gt; GaussianBase\n</code></pre> <p>Get Gaussian data under the given instance_pose.</p> Source code in <code>embodied_gen/models/gs_model.py</code> <pre><code>def get_gaussians(\n    self,\n    c2w: torch.Tensor = None,\n    instance_pose: torch.Tensor = None,\n    apply_activate: bool = False,\n) -&gt; \"GaussianBase\":\n    \"\"\"Get Gaussian data under the given instance_pose.\"\"\"\n    if c2w is None:\n        c2w = torch.eye(4).to(self.device)\n\n    if instance_pose is not None:\n        # compute the transformed gs means and quats\n        world_means, world_quats = self._compute_transform(\n            self._means, self._quats, instance_pose.float().to(self.device)\n        )\n    else:\n        world_means, world_quats = self._means, self._quats\n\n    # get colors of gaussians\n    if self._features_rest is not None:\n        colors = torch.cat(\n            (self._features_dc[:, None, :], self._features_rest), dim=1\n        )\n    else:\n        colors = self._features_dc[:, None, :]\n\n    if self.sh_degree &gt; 0:\n        viewdirs = world_means.detach() - c2w[..., :3, 3]  # (N, 3)\n        viewdirs = viewdirs / viewdirs.norm(dim=-1, keepdim=True)\n        rgbs = spherical_harmonics(self.sh_degree, viewdirs, colors)\n        rgbs = torch.clamp(rgbs + 0.5, 0.0, 1.0)\n    else:\n        rgbs = torch.sigmoid(colors[:, 0, :])\n\n    gs_dict = dict(\n        _means=world_means,\n        _opacities=(\n            torch.sigmoid(self._opacities)\n            if apply_activate\n            else self._opacities\n        ),\n        _rgbs=rgbs,\n        _scales=(\n            torch.exp(self._scales) if apply_activate else self._scales\n        ),\n        _quats=self.quat_norm(world_quats),\n        _features_dc=self._features_dc,\n        _features_rest=self._features_rest,\n        sh_degree=self.sh_degree,\n        device=self.device,\n    )\n\n    return GaussianOperator(**gs_dict)\n</code></pre>"},{"location":"api/models/#embodied_gen.models.layout","title":"embodied_gen.models.layout","text":""},{"location":"api/models/#embodied_gen.models.text_model","title":"embodied_gen.models.text_model","text":""},{"location":"api/models/#embodied_gen.models.sr_model","title":"embodied_gen.models.sr_model","text":""},{"location":"api/models/#embodied_gen.models.sr_model.ImageRealESRGAN","title":"ImageRealESRGAN","text":"<pre><code>ImageRealESRGAN(outscale: int, model_path: str = None)\n</code></pre> <p>A wrapper for Real-ESRGAN-based image super-resolution.</p> <p>This class uses the RealESRGAN model to perform image upscaling, typically by a factor of 4.</p> <p>Attributes:</p> Name Type Description <code>outscale</code> <code>int</code> <p>The output image scale factor (e.g., 2, 4).</p> <code>model_path</code> <code>str</code> <p>Path to the pre-trained model weights.</p> Source code in <code>embodied_gen/models/sr_model.py</code> <pre><code>def __init__(self, outscale: int, model_path: str = None) -&gt; None:\n    # monkey patch to support torchvision&gt;=0.16\n    import torchvision\n    from packaging import version\n\n    if version.parse(torchvision.__version__) &gt; version.parse(\"0.16\"):\n        import sys\n        import types\n\n        import torchvision.transforms.functional as TF\n\n        functional_tensor = types.ModuleType(\n            \"torchvision.transforms.functional_tensor\"\n        )\n        functional_tensor.rgb_to_grayscale = TF.rgb_to_grayscale\n        sys.modules[\"torchvision.transforms.functional_tensor\"] = (\n            functional_tensor\n        )\n\n    self.outscale = outscale\n    self.upsampler = None\n\n    if model_path is None:\n        suffix = \"super_resolution\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        model_path = os.path.join(\n            model_path, suffix, \"RealESRGAN_x4plus.pth\"\n        )\n\n    self.model_path = model_path\n</code></pre>"},{"location":"api/models/#embodied_gen.models.sr_model.ImageStableSR","title":"ImageStableSR","text":"<pre><code>ImageStableSR(model_path: str = 'stabilityai/stable-diffusion-x4-upscaler', device='cuda')\n</code></pre> <p>Super-resolution image upscaler using Stable Diffusion x4 upscaling model from StabilityAI.</p> Source code in <code>embodied_gen/models/sr_model.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = \"stabilityai/stable-diffusion-x4-upscaler\",\n    device=\"cuda\",\n) -&gt; None:\n    from diffusers import StableDiffusionUpscalePipeline\n\n    self.up_pipeline_x4 = StableDiffusionUpscalePipeline.from_pretrained(\n        model_path,\n        torch_dtype=torch.float16,\n    ).to(device)\n    self.up_pipeline_x4.set_progress_bar_config(disable=True)\n    self.up_pipeline_x4.enable_model_cpu_offload()\n</code></pre>"},{"location":"api/models/#embodied_gen.models.segment_model","title":"embodied_gen.models.segment_model","text":""},{"location":"api/models/#embodied_gen.models.segment_model.SAMRemover","title":"SAMRemover","text":"<pre><code>SAMRemover(checkpoint: str = None, model_type: str = 'vit_h', area_ratio: float = 15)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Loading SAM models and performing background removal on images.</p> <p>Attributes:</p> Name Type Description <code>checkpoint</code> <code>str</code> <p>Path to the model checkpoint.</p> <code>model_type</code> <code>str</code> <p>Type of the SAM model to load (default: \"vit_h\").</p> <code>area_ratio</code> <code>float</code> <p>Area ratio filtering small connected components.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __init__(\n    self,\n    checkpoint: str = None,\n    model_type: str = \"vit_h\",\n    area_ratio: float = 15,\n):\n    self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    self.model_type = model_type\n    self.area_ratio = area_ratio\n\n    if checkpoint is None:\n        suffix = \"sam\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        checkpoint = os.path.join(\n            model_path, suffix, \"sam_vit_h_4b8939.pth\"\n        )\n\n    self.mask_generator = self._load_sam_model(checkpoint)\n</code></pre>"},{"location":"api/models/#embodied_gen.models.segment_model.SAMRemover.__call__","title":"__call__","text":"<pre><code>__call__(image: Union[str, Image, ndarray], save_path: str = None) -&gt; Image.Image\n</code></pre> <p>Removes the background from an image using the SAM model.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Union[str, Image, ndarray]</code> <p>Input image, can be a file path, PIL Image, or numpy array.</p> required <code>save_path</code> <code>str</code> <p>Path to save the output image (default: None).</p> <code>None</code> <p>Returns:</p> Type Description <code>Image</code> <p>Image.Image: The image with background removed, including an alpha channel.</p> Source code in <code>embodied_gen/models/segment_model.py</code> <pre><code>def __call__(\n    self, image: Union[str, Image.Image, np.ndarray], save_path: str = None\n) -&gt; Image.Image:\n    \"\"\"Removes the background from an image using the SAM model.\n\n    Args:\n        image (Union[str, Image.Image, np.ndarray]): Input image,\n            can be a file path, PIL Image, or numpy array.\n        save_path (str): Path to save the output image (default: None).\n\n    Returns:\n        Image.Image: The image with background removed,\n            including an alpha channel.\n    \"\"\"\n    # Convert input to numpy array\n    if isinstance(image, str):\n        image = Image.open(image)\n    elif isinstance(image, np.ndarray):\n        image = Image.fromarray(image).convert(\"RGB\")\n    image = resize_pil(image)\n    image = np.array(image.convert(\"RGB\"))\n\n    # Generate masks\n    masks = self.mask_generator.generate(image)\n    masks = sorted(masks, key=lambda x: x[\"area\"], reverse=True)\n\n    if not masks:\n        logger.warning(\n            \"Segmentation failed: No mask generated, return raw image.\"\n        )\n        output_image = Image.fromarray(image, mode=\"RGB\")\n    else:\n        # Use the largest mask\n        best_mask = masks[0][\"segmentation\"]\n        mask = (best_mask * 255).astype(np.uint8)\n        mask = filter_small_connected_components(\n            mask, area_ratio=self.area_ratio\n        )\n        # Apply the mask to remove the background\n        background_removed = cv2.bitwise_and(image, image, mask=mask)\n        output_image = np.dstack((background_removed, mask))\n        output_image = Image.fromarray(output_image, mode=\"RGBA\")\n\n    if save_path is not None:\n        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n        output_image.save(save_path)\n\n    return output_image\n</code></pre>"},{"location":"api/models/#embodied_gen.models.image_comm_model","title":"embodied_gen.models.image_comm_model","text":""},{"location":"api/models/#embodied_gen.models.delight_model","title":"embodied_gen.models.delight_model","text":""},{"location":"api/models/#embodied_gen.models.delight_model.DelightingModel","title":"DelightingModel","text":"<pre><code>DelightingModel(model_path: str = None, num_infer_step: int = 50, mask_erosion_size: int = 3, image_guide_scale: float = 1.5, text_guide_scale: float = 1.0, device: str = 'cuda', seed: int = 0)\n</code></pre> <p>               Bases: <code>object</code></p> <p>A model to remove the lighting in image space.</p> <p>This model is encapsulated based on the Hunyuan3D-Delight model from https://huggingface.co/tencent/Hunyuan3D-2/tree/main/hunyuan3d-delight-v2-0 # noqa</p> <p>Attributes:</p> Name Type Description <code>image_guide_scale</code> <code>float</code> <p>Weight of image guidance in diffusion process.</p> <code>text_guide_scale</code> <code>float</code> <p>Weight of text (prompt) guidance in diffusion process.</p> <code>num_infer_step</code> <code>int</code> <p>Number of inference steps for diffusion model.</p> <code>mask_erosion_size</code> <code>int</code> <p>Size of erosion kernel for alpha mask cleanup.</p> <code>device</code> <code>str</code> <p>Device used for inference, e.g., 'cuda' or 'cpu'.</p> <code>seed</code> <code>int</code> <p>Random seed for diffusion model reproducibility.</p> <code>model_path</code> <code>str</code> <p>Filesystem path to pretrained model weights.</p> <code>pipeline</code> <p>Lazy-loaded diffusion pipeline instance.</p> Source code in <code>embodied_gen/models/delight_model.py</code> <pre><code>def __init__(\n    self,\n    model_path: str = None,\n    num_infer_step: int = 50,\n    mask_erosion_size: int = 3,\n    image_guide_scale: float = 1.5,\n    text_guide_scale: float = 1.0,\n    device: str = \"cuda\",\n    seed: int = 0,\n) -&gt; None:\n    self.image_guide_scale = image_guide_scale\n    self.text_guide_scale = text_guide_scale\n    self.num_infer_step = num_infer_step\n    self.mask_erosion_size = mask_erosion_size\n    self.kernel = np.ones(\n        (self.mask_erosion_size, self.mask_erosion_size), np.uint8\n    )\n    self.seed = seed\n    self.device = device\n    self.pipeline = None  # lazy load model adapt to @spaces.GPU\n\n    if model_path is None:\n        suffix = \"hunyuan3d-delight-v2-0\"\n        model_path = snapshot_download(\n            repo_id=\"tencent/Hunyuan3D-2\", allow_patterns=f\"{suffix}/*\"\n        )\n        model_path = os.path.join(model_path, suffix)\n\n    self.model_path = model_path\n</code></pre>"},{"location":"api/trainer/","title":"Trainer API","text":"<p>This section covers the training pipelines for various models.</p>"},{"location":"api/trainer/#embodied_gen.trainer.gsplat_trainer","title":"embodied_gen.trainer.gsplat_trainer","text":""},{"location":"api/trainer/#embodied_gen.trainer.gsplat_trainer.Runner","title":"Runner","text":"<pre><code>Runner(local_rank: int, world_rank, world_size: int, cfg: GsplatTrainConfig)\n</code></pre> <p>Engine for training and testing from gsplat example.</p> <p>Code from https://github.com/nerfstudio-project/gsplat/blob/main/examples/simple_trainer.py</p> Source code in <code>embodied_gen/trainer/gsplat_trainer.py</code> <pre><code>def __init__(\n    self,\n    local_rank: int,\n    world_rank,\n    world_size: int,\n    cfg: GsplatTrainConfig,\n) -&gt; None:\n    set_random_seed(42 + local_rank)\n\n    self.cfg = cfg\n    self.world_rank = world_rank\n    self.local_rank = local_rank\n    self.world_size = world_size\n    self.device = f\"cuda:{local_rank}\"\n\n    # Where to dump results.\n    os.makedirs(cfg.result_dir, exist_ok=True)\n\n    # Setup output directories.\n    self.ckpt_dir = f\"{cfg.result_dir}/ckpts\"\n    os.makedirs(self.ckpt_dir, exist_ok=True)\n    self.stats_dir = f\"{cfg.result_dir}/stats\"\n    os.makedirs(self.stats_dir, exist_ok=True)\n    self.render_dir = f\"{cfg.result_dir}/renders\"\n    os.makedirs(self.render_dir, exist_ok=True)\n    self.ply_dir = f\"{cfg.result_dir}/ply\"\n    os.makedirs(self.ply_dir, exist_ok=True)\n\n    # Tensorboard\n    self.writer = SummaryWriter(log_dir=f\"{cfg.result_dir}/tb\")\n    self.trainset = PanoGSplatDataset(cfg.data_dir, split=\"train\")\n    self.valset = PanoGSplatDataset(\n        cfg.data_dir, split=\"train\", max_sample_num=6\n    )\n    self.testset = PanoGSplatDataset(cfg.data_dir, split=\"eval\")\n    self.scene_scale = cfg.scene_scale\n\n    # Model\n    self.splats, self.optimizers = create_splats_with_optimizers(\n        self.trainset.points,\n        self.trainset.points_rgb,\n        init_num_pts=cfg.init_num_pts,\n        init_extent=cfg.init_extent,\n        init_opacity=cfg.init_opa,\n        init_scale=cfg.init_scale,\n        means_lr=cfg.means_lr,\n        scales_lr=cfg.scales_lr,\n        opacities_lr=cfg.opacities_lr,\n        quats_lr=cfg.quats_lr,\n        sh0_lr=cfg.sh0_lr,\n        shN_lr=cfg.shN_lr,\n        scene_scale=self.scene_scale,\n        sh_degree=cfg.sh_degree,\n        sparse_grad=cfg.sparse_grad,\n        visible_adam=cfg.visible_adam,\n        batch_size=cfg.batch_size,\n        feature_dim=None,\n        device=self.device,\n        world_rank=world_rank,\n        world_size=world_size,\n    )\n    print(\"Model initialized. Number of GS:\", len(self.splats[\"means\"]))\n\n    # Densification Strategy\n    self.cfg.strategy.check_sanity(self.splats, self.optimizers)\n\n    if isinstance(self.cfg.strategy, DefaultStrategy):\n        self.strategy_state = self.cfg.strategy.initialize_state(\n            scene_scale=self.scene_scale\n        )\n    elif isinstance(self.cfg.strategy, MCMCStrategy):\n        self.strategy_state = self.cfg.strategy.initialize_state()\n    else:\n        assert_never(self.cfg.strategy)\n\n    # Losses &amp; Metrics.\n    self.ssim = StructuralSimilarityIndexMeasure(data_range=1.0).to(\n        self.device\n    )\n    self.psnr = PeakSignalNoiseRatio(data_range=1.0).to(self.device)\n\n    if cfg.lpips_net == \"alex\":\n        self.lpips = LearnedPerceptualImagePatchSimilarity(\n            net_type=\"alex\", normalize=True\n        ).to(self.device)\n    elif cfg.lpips_net == \"vgg\":\n        # The 3DGS official repo uses lpips vgg, which is equivalent with the following:\n        self.lpips = LearnedPerceptualImagePatchSimilarity(\n            net_type=\"vgg\", normalize=False\n        ).to(self.device)\n    else:\n        raise ValueError(f\"Unknown LPIPS network: {cfg.lpips_net}\")\n</code></pre>"},{"location":"api/trainer/#embodied_gen.trainer.gsplat_trainer.Runner.eval","title":"eval","text":"<pre><code>eval(step: int, stage: str = 'val', canvas_h: int = 512, canvas_w: int = 1024)\n</code></pre> <p>Entry for evaluation.</p> Source code in <code>embodied_gen/trainer/gsplat_trainer.py</code> <pre><code>@torch.no_grad()\ndef eval(\n    self,\n    step: int,\n    stage: str = \"val\",\n    canvas_h: int = 512,\n    canvas_w: int = 1024,\n):\n    \"\"\"Entry for evaluation.\"\"\"\n    print(\"Running evaluation...\")\n    cfg = self.cfg\n    device = self.device\n    world_rank = self.world_rank\n\n    valloader = torch.utils.data.DataLoader(\n        self.valset, batch_size=1, shuffle=False, num_workers=1\n    )\n    ellipse_time = 0\n    metrics = defaultdict(list)\n    for i, data in enumerate(valloader):\n        camtoworlds = data[\"camtoworld\"].to(device)\n        Ks = data[\"K\"].to(device)\n        pixels = data[\"image\"].to(device) / 255.0\n        height, width = pixels.shape[1:3]\n        masks = data[\"mask\"].to(device) if \"mask\" in data else None\n\n        pixels = pixels.permute(0, 3, 1, 2)  # NHWC -&gt; NCHW\n        pixels = F.interpolate(pixels, size=(canvas_h, canvas_w // 2))\n\n        torch.cuda.synchronize()\n        tic = time.time()\n        colors, _, _ = self.rasterize_splats(\n            camtoworlds=camtoworlds,\n            Ks=Ks,\n            width=width,\n            height=height,\n            sh_degree=cfg.sh_degree,\n            near_plane=cfg.near_plane,\n            far_plane=cfg.far_plane,\n            masks=masks,\n        )  # [1, H, W, 3]\n        torch.cuda.synchronize()\n        ellipse_time += max(time.time() - tic, 1e-10)\n\n        colors = colors.permute(0, 3, 1, 2)  # NHWC -&gt; NCHW\n        colors = F.interpolate(colors, size=(canvas_h, canvas_w // 2))\n        colors = torch.clamp(colors, 0.0, 1.0)\n        canvas_list = [pixels, colors]\n\n        if world_rank == 0:\n            canvas = torch.cat(canvas_list, dim=2).squeeze(0)\n            canvas = canvas.permute(1, 2, 0)  # CHW -&gt; HWC\n            canvas = (canvas * 255).to(torch.uint8).cpu().numpy()\n            cv2.imwrite(\n                f\"{self.render_dir}/{stage}_step{step}_{i:04d}.png\",\n                canvas[..., ::-1],\n            )\n            metrics[\"psnr\"].append(self.psnr(colors, pixels))\n            metrics[\"ssim\"].append(self.ssim(colors, pixels))\n            metrics[\"lpips\"].append(self.lpips(colors, pixels))\n\n    if world_rank == 0:\n        ellipse_time /= len(valloader)\n\n        stats = {\n            k: torch.stack(v).mean().item() for k, v in metrics.items()\n        }\n        stats.update(\n            {\n                \"ellipse_time\": ellipse_time,\n                \"num_GS\": len(self.splats[\"means\"]),\n            }\n        )\n        print(\n            f\"PSNR: {stats['psnr']:.3f}, SSIM: {stats['ssim']:.4f}, LPIPS: {stats['lpips']:.3f} \"\n            f\"Time: {stats['ellipse_time']:.3f}s/image \"\n            f\"Number of GS: {stats['num_GS']}\"\n        )\n        # save stats as json\n        with open(\n            f\"{self.stats_dir}/{stage}_step{step:04d}.json\", \"w\"\n        ) as f:\n            json.dump(stats, f)\n        # save stats to tensorboard\n        for k, v in stats.items():\n            self.writer.add_scalar(f\"{stage}/{k}\", v, step)\n        self.writer.flush()\n</code></pre>"},{"location":"api/trainer/#embodied_gen.trainer.pono2mesh_trainer","title":"embodied_gen.trainer.pono2mesh_trainer","text":""},{"location":"api/trainer/#embodied_gen.trainer.pono2mesh_trainer.Pano2MeshSRPipeline","title":"Pano2MeshSRPipeline","text":"<pre><code>Pano2MeshSRPipeline(config: Pano2MeshSRConfig)\n</code></pre> <p>Converting panoramic RGB image into 3D mesh representations, followed by inpainting and mesh refinement.</p> <p>This class integrates several key components including: - Depth estimation from RGB panorama - Inpainting of missing regions under offsets - RGB-D to mesh conversion - Multi-view mesh repair - 3D Gaussian Splatting (3DGS) dataset generation</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>Pano2MeshSRConfig</code> <p>Configuration object containing model and pipeline parameters.</p> required Example <pre><code>pipeline = Pano2MeshSRPipeline(config)\npipeline(pano_image='example.png', output_dir='./output')\n</code></pre> Source code in <code>embodied_gen/trainer/pono2mesh_trainer.py</code> <pre><code>def __init__(self, config: Pano2MeshSRConfig) -&gt; None:\n    self.cfg = config\n    self.device = config.device\n\n    # Init models.\n    self.inpainter = PanoPersFusionInpainter(save_path=None)\n    self.geo_predictor = PanoJointPredictor(save_path=None)\n    self.pano_fusion_distance_predictor = PanoFusionDistancePredictor()\n    self.super_model = ImageRealESRGAN(outscale=self.cfg.upscale_factor)\n\n    # Init poses.\n    cubemap_w2cs = get_cubemap_views_world_to_cam()\n    self.cubemap_w2cs = [p.to(self.device) for p in cubemap_w2cs]\n    self.camera_poses = self.load_camera_poses(self.cfg.trajectory_dir)\n\n    kernel = cv2.getStructuringElement(\n        cv2.MORPH_ELLIPSE, self.cfg.kernel_size\n    )\n    self.kernel = torch.from_numpy(kernel).float().to(self.device)\n</code></pre>"},{"location":"api/utils/","title":"Utilities API","text":"<p>General-purpose utility functions, configuration, and helper classes.</p>"},{"location":"api/utils/#embodied_gen.utils.config","title":"embodied_gen.utils.config","text":""},{"location":"api/utils/#embodied_gen.utils.log","title":"embodied_gen.utils.log","text":""},{"location":"api/utils/#embodied_gen.utils.enum","title":"embodied_gen.utils.enum","text":""},{"location":"api/utils/#embodied_gen.utils.geometry","title":"embodied_gen.utils.geometry","text":""},{"location":"api/utils/#embodied_gen.utils.geometry.bfs_placement","title":"bfs_placement","text":"<pre><code>bfs_placement(layout_file: str, floor_margin: float = 0, beside_margin: float = 0.1, max_attempts: int = 3000, init_rpy: tuple = (1.5708, 0.0, 0.0), rotate_objs: bool = True, rotate_bg: bool = True, rotate_context: bool = True, limit_reach_range: tuple[float, float] | None = (0.2, 0.85), max_orient_diff: float | None = 60, robot_dim: float = 0.12, seed: int = None) -&gt; LayoutInfo\n</code></pre> <p>Place objects in the layout using BFS traversal.</p> <p>Parameters:</p> Name Type Description Default <code>layout_file</code> <code>str</code> <p>Path to the JSON file defining the layout structure and assets.</p> required <code>floor_margin</code> <code>float</code> <p>Z-offset for the background object, typically for objects placed on the floor.</p> <code>0</code> <code>beside_margin</code> <code>float</code> <p>Minimum margin for objects placed 'beside' their parent, used when 'on' placement fails.</p> <code>0.1</code> <code>max_attempts</code> <code>int</code> <p>Maximum number of attempts to find a non-overlapping position for an object.</p> <code>3000</code> <code>init_rpy</code> <code>tuple</code> <p>Initial Roll-Pitch-Yaw rotation rad applied to all object meshes to align the mesh's coordinate system with the world's (e.g., Z-up).</p> <code>(1.5708, 0.0, 0.0)</code> <code>rotate_objs</code> <code>bool</code> <p>If True, apply a random rotation around the Z-axis for manipulated and distractor objects.</p> <code>True</code> <code>rotate_bg</code> <code>bool</code> <p>If True, apply a random rotation around the Y-axis for the background object.</p> <code>True</code> <code>rotate_context</code> <code>bool</code> <p>If True, apply a random rotation around the Z-axis for the context object.</p> <code>True</code> <code>limit_reach_range</code> <code>tuple[float, float] | None</code> <p>If set, enforce a check that manipulated objects are within the robot's reach range, in meter.</p> <code>(0.2, 0.85)</code> <code>max_orient_diff</code> <code>float | None</code> <p>If set, enforce a check that manipulated objects are within the robot's orientation range, in degree.</p> <code>60</code> <code>robot_dim</code> <code>float</code> <p>The approximate dimension (e.g., diameter) of the robot for box representation.</p> <code>0.12</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible placement.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>A</code> <code>LayoutInfo</code> <p>class:<code>LayoutInfo</code> object containing the objects and their final computed 7D poses</p> <code>LayoutInfo</code> <p>([x, y, z, qx, qy, qz, qw]).</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>@with_seed(\"seed\")\ndef bfs_placement(\n    layout_file: str,\n    floor_margin: float = 0,\n    beside_margin: float = 0.1,\n    max_attempts: int = 3000,\n    init_rpy: tuple = (1.5708, 0.0, 0.0),\n    rotate_objs: bool = True,\n    rotate_bg: bool = True,\n    rotate_context: bool = True,\n    limit_reach_range: tuple[float, float] | None = (0.20, 0.85),\n    max_orient_diff: float | None = 60,\n    robot_dim: float = 0.12,\n    seed: int = None,\n) -&gt; LayoutInfo:\n    \"\"\"Place objects in the layout using BFS traversal.\n\n    Args:\n        layout_file: Path to the JSON file defining the layout structure and assets.\n        floor_margin: Z-offset for the background object, typically for objects placed on the floor.\n        beside_margin: Minimum margin for objects placed 'beside' their parent, used when 'on' placement fails.\n        max_attempts: Maximum number of attempts to find a non-overlapping position for an object.\n        init_rpy: Initial Roll-Pitch-Yaw rotation rad applied to all object meshes to align the mesh's\n            coordinate system with the world's (e.g., Z-up).\n        rotate_objs: If True, apply a random rotation around the Z-axis for manipulated and distractor objects.\n        rotate_bg: If True, apply a random rotation around the Y-axis for the background object.\n        rotate_context: If True, apply a random rotation around the Z-axis for the context object.\n        limit_reach_range: If set, enforce a check that manipulated objects are within the robot's reach range, in meter.\n        max_orient_diff: If set, enforce a check that manipulated objects are within the robot's orientation range, in degree.\n        robot_dim: The approximate dimension (e.g., diameter) of the robot for box representation.\n        seed: Random seed for reproducible placement.\n\n    Returns:\n        A :class:`LayoutInfo` object containing the objects and their final computed 7D poses\n        ([x, y, z, qx, qy, qz, qw]).\n    \"\"\"\n    layout_info = LayoutInfo.from_dict(json.load(open(layout_file, \"r\")))\n    asset_dir = os.path.dirname(layout_file)\n    object_mapping = layout_info.objs_mapping\n    position = {}  # node: [x, y, z, qx, qy, qz, qw]\n    parent_bbox_xy = {}\n    placed_boxes_map = defaultdict(list)\n    mesh_info = defaultdict(dict)\n    robot_node = layout_info.relation[Scene3DItemEnum.ROBOT.value]\n    for node in object_mapping:\n        if object_mapping[node] == Scene3DItemEnum.BACKGROUND.value:\n            bg_quat = (\n                compute_axis_rotation_quat(\n                    axis=\"y\",\n                    angle_rad=np.random.uniform(0, 2 * np.pi),\n                )\n                if rotate_bg\n                else [0, 0, 0, 1]\n            )\n            bg_quat = [round(q, 4) for q in bg_quat]\n            continue\n\n        mesh_path = (\n            f\"{layout_info.assets[node]}/mesh/{node.replace(' ', '_')}.obj\"\n        )\n        mesh_path = os.path.join(asset_dir, mesh_path)\n        mesh_info[node][\"path\"] = mesh_path\n        mesh = trimesh.load(mesh_path)\n        rotation = R.from_euler(\"xyz\", init_rpy, degrees=False)\n        vertices = mesh.vertices @ rotation.as_matrix().T\n        z1 = np.percentile(vertices[:, 2], 1)\n        z2 = np.percentile(vertices[:, 2], 99)\n\n        if object_mapping[node] == Scene3DItemEnum.CONTEXT.value:\n            object_quat = [0, 0, 0, 1]\n            if rotate_context:\n                angle_rad = np.random.uniform(0, 2 * np.pi)\n                object_quat = compute_axis_rotation_quat(\n                    axis=\"z\", angle_rad=angle_rad\n                )\n                rotation = R.from_quat(object_quat).as_matrix()\n                vertices = vertices @ rotation.T\n\n            mesh_info[node][\"surface\"] = compute_convex_hull_path(vertices)\n\n            # Put robot in the CONTEXT edge.\n            x, y = random.choice(mesh_info[node][\"surface\"].vertices)\n            theta = np.arctan2(y, x)\n            quat_initial = Quaternion(axis=[0, 0, 1], angle=theta)\n            quat_extra = Quaternion(axis=[0, 0, 1], angle=np.pi)\n            quat = quat_extra * quat_initial\n            _pose = [x, y, z2 - z1, quat.x, quat.y, quat.z, quat.w]\n            position[robot_node] = [round(v, 4) for v in _pose]\n            node_box = [\n                x - robot_dim / 2,\n                x + robot_dim / 2,\n                y - robot_dim / 2,\n                y + robot_dim / 2,\n            ]\n            placed_boxes_map[node].append(node_box)\n        elif rotate_objs:\n            # For manipulated and distractor objects, apply random rotation\n            angle_rad = np.random.uniform(0, 2 * np.pi)\n            object_quat = compute_axis_rotation_quat(\n                axis=\"z\", angle_rad=angle_rad\n            )\n            rotation = R.from_quat(object_quat).as_matrix()\n            vertices = vertices @ rotation.T\n\n        x1, x2, y1, y2 = compute_xy_bbox(vertices)\n        mesh_info[node][\"pose\"] = [x1, x2, y1, y2, z1, z2, *object_quat]\n        mesh_info[node][\"area\"] = max(1e-5, (x2 - x1) * (y2 - y1))\n\n    root = list(layout_info.tree.keys())[0]\n    queue = deque([((root, None), layout_info.tree.get(root, []))])\n    while queue:\n        (node, relation), children = queue.popleft()\n        if node not in object_mapping:\n            continue\n\n        if object_mapping[node] == Scene3DItemEnum.BACKGROUND.value:\n            position[node] = [0, 0, floor_margin, *bg_quat]\n        else:\n            x1, x2, y1, y2, z1, z2, qx, qy, qz, qw = mesh_info[node][\"pose\"]\n            if object_mapping[node] == Scene3DItemEnum.CONTEXT.value:\n                position[node] = [0, 0, -round(z1, 4), qx, qy, qz, qw]\n                parent_bbox_xy[node] = [x1, x2, y1, y2, z1, z2]\n            elif object_mapping[node] in [\n                Scene3DItemEnum.MANIPULATED_OBJS.value,\n                Scene3DItemEnum.DISTRACTOR_OBJS.value,\n            ]:\n                parent_node = find_parent_node(node, layout_info.tree)\n                parent_pos = position[parent_node]\n                (\n                    p_x1,\n                    p_x2,\n                    p_y1,\n                    p_y2,\n                    p_z1,\n                    p_z2,\n                ) = parent_bbox_xy[parent_node]\n\n                obj_dx = x2 - x1\n                obj_dy = y2 - y1\n                hull_path = mesh_info[parent_node].get(\"surface\")\n                for _ in range(max_attempts):\n                    node_x1 = random.uniform(p_x1, p_x2 - obj_dx)\n                    node_y1 = random.uniform(p_y1, p_y2 - obj_dy)\n                    node_box = [\n                        node_x1,\n                        node_x1 + obj_dx,\n                        node_y1,\n                        node_y1 + obj_dy,\n                    ]\n                    if hull_path and not all_corners_inside(\n                        hull_path, node_box\n                    ):\n                        continue\n                    # Make sure the manipulated object is reachable by robot.\n                    if (\n                        limit_reach_range is not None\n                        and object_mapping[node]\n                        == Scene3DItemEnum.MANIPULATED_OBJS.value\n                    ):\n                        cx = parent_pos[0] + node_box[0] + obj_dx / 2\n                        cy = parent_pos[1] + node_box[2] + obj_dy / 2\n                        cz = parent_pos[2] + p_z2 - z1\n                        robot_pos = position[robot_node][:3]\n                        if not check_reachable(\n                            base_xyz=np.array(robot_pos),\n                            reach_xyz=np.array([cx, cy, cz]),\n                            min_reach=limit_reach_range[0],\n                            max_reach=limit_reach_range[1],\n                        ):\n                            continue\n\n                    # Make sure the manipulated object is inside the robot's orientation.\n                    if (\n                        max_orient_diff is not None\n                        and object_mapping[node]\n                        == Scene3DItemEnum.MANIPULATED_OBJS.value\n                    ):\n                        cx = parent_pos[0] + node_box[0] + obj_dx / 2\n                        cy = parent_pos[1] + node_box[2] + obj_dy / 2\n                        cx2, cy2 = position[robot_node][:2]\n                        v1 = np.array([-cx2, -cy2])\n                        v2 = np.array([cx - cx2, cy - cy2])\n                        dot = np.dot(v1, v2)\n                        norms = np.linalg.norm(v1) * np.linalg.norm(v2)\n                        theta = np.arccos(np.clip(dot / norms, -1.0, 1.0))\n                        theta = np.rad2deg(theta)\n                        if theta &gt; max_orient_diff:\n                            continue\n\n                    if not has_iou_conflict(\n                        node_box, placed_boxes_map[parent_node]\n                    ):\n                        z_offset = 0\n                        break\n                else:\n                    logger.warning(\n                        f\"Cannot place {node} on {parent_node} without overlap\"\n                        f\" after {max_attempts} attempts, place beside {parent_node}.\"\n                    )\n                    for _ in range(max_attempts):\n                        node_x1 = random.choice(\n                            [\n                                random.uniform(\n                                    p_x1 - obj_dx - beside_margin,\n                                    p_x1 - obj_dx,\n                                ),\n                                random.uniform(p_x2, p_x2 + beside_margin),\n                            ]\n                        )\n                        node_y1 = random.choice(\n                            [\n                                random.uniform(\n                                    p_y1 - obj_dy - beside_margin,\n                                    p_y1 - obj_dy,\n                                ),\n                                random.uniform(p_y2, p_y2 + beside_margin),\n                            ]\n                        )\n                        node_box = [\n                            node_x1,\n                            node_x1 + obj_dx,\n                            node_y1,\n                            node_y1 + obj_dy,\n                        ]\n                        z_offset = -(parent_pos[2] + p_z2)\n                        if not has_iou_conflict(\n                            node_box, placed_boxes_map[parent_node]\n                        ):\n                            break\n\n                placed_boxes_map[parent_node].append(node_box)\n\n                abs_cx = parent_pos[0] + node_box[0] + obj_dx / 2\n                abs_cy = parent_pos[1] + node_box[2] + obj_dy / 2\n                abs_cz = parent_pos[2] + p_z2 - z1 + z_offset\n                position[node] = [\n                    round(v, 4)\n                    for v in [abs_cx, abs_cy, abs_cz, qx, qy, qz, qw]\n                ]\n                parent_bbox_xy[node] = [x1, x2, y1, y2, z1, z2]\n\n        sorted_children = sorted(\n            children, key=lambda x: -mesh_info[x[0]].get(\"area\", 0)\n        )\n        for child, rel in sorted_children:\n            queue.append(((child, rel), layout_info.tree.get(child, [])))\n\n    layout_info.position = position\n\n    return layout_info\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.geometry.check_reachable","title":"check_reachable","text":"<pre><code>check_reachable(base_xyz: ndarray, reach_xyz: ndarray, min_reach: float = 0.25, max_reach: float = 0.85) -&gt; bool\n</code></pre> <p>Check if the target point is within the reachable range.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def check_reachable(\n    base_xyz: np.ndarray,\n    reach_xyz: np.ndarray,\n    min_reach: float = 0.25,\n    max_reach: float = 0.85,\n) -&gt; bool:\n    \"\"\"Check if the target point is within the reachable range.\"\"\"\n    distance = np.linalg.norm(reach_xyz - base_xyz)\n\n    return min_reach &lt; distance &lt; max_reach\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.geometry.matrix_to_pose","title":"matrix_to_pose","text":"<pre><code>matrix_to_pose(matrix: ndarray) -&gt; list[float]\n</code></pre> <p>Convert a 4x4 transformation matrix to a pose (x, y, z, qx, qy, qz, qw).</p> <p>Parameters:</p> Name Type Description Default <code>matrix</code> <code>ndarray</code> <p>4x4 transformation matrix.</p> required <p>Returns:</p> Type Description <code>list[float]</code> <p>List[float]: Pose as [x, y, z, qx, qy, qz, qw].</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def matrix_to_pose(matrix: np.ndarray) -&gt; list[float]:\n    \"\"\"Convert a 4x4 transformation matrix to a pose (x, y, z, qx, qy, qz, qw).\n\n    Args:\n        matrix (np.ndarray): 4x4 transformation matrix.\n\n    Returns:\n        List[float]: Pose as [x, y, z, qx, qy, qz, qw].\n    \"\"\"\n    x, y, z = matrix[:3, 3]\n    rot_mat = matrix[:3, :3]\n    quat = R.from_matrix(rot_mat).as_quat()\n    qx, qy, qz, qw = quat\n\n    return [x, y, z, qx, qy, qz, qw]\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.geometry.pose_to_matrix","title":"pose_to_matrix","text":"<pre><code>pose_to_matrix(pose: list[float]) -&gt; np.ndarray\n</code></pre> <p>Convert pose (x, y, z, qx, qy, qz, qw) to a 4x4 transformation matrix.</p> <p>Parameters:</p> Name Type Description Default <code>List[float]</code> <p>Pose as [x, y, z, qx, qy, qz, qw].</p> required <p>Returns:</p> Name Type Description <code>matrix</code> <code>ndarray</code> <p>4x4 transformation matrix.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def pose_to_matrix(pose: list[float]) -&gt; np.ndarray:\n    \"\"\"Convert pose (x, y, z, qx, qy, qz, qw) to a 4x4 transformation matrix.\n\n    Args:\n        List[float]: Pose as [x, y, z, qx, qy, qz, qw].\n\n    Returns:\n        matrix (np.ndarray): 4x4 transformation matrix.\n    \"\"\"\n    x, y, z, qx, qy, qz, qw = pose\n    r = R.from_quat([qx, qy, qz, qw])\n    matrix = np.eye(4)\n    matrix[:3, :3] = r.as_matrix()\n    matrix[:3, 3] = [x, y, z]\n\n    return matrix\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.geometry.with_seed","title":"with_seed","text":"<pre><code>with_seed(seed_attr_name: str = 'seed')\n</code></pre> <p>A parameterized decorator that temporarily sets the random seed.</p> Source code in <code>embodied_gen/utils/geometry.py</code> <pre><code>def with_seed(seed_attr_name: str = \"seed\"):\n    \"\"\"A parameterized decorator that temporarily sets the random seed.\"\"\"\n\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            seed = kwargs.get(seed_attr_name, None)\n            if seed is not None:\n                py_state = random.getstate()\n                np_state = np.random.get_state()\n                torch_state = torch.get_rng_state()\n\n                random.seed(seed)\n                np.random.seed(seed)\n                torch.manual_seed(seed)\n                try:\n                    result = func(*args, **kwargs)\n                finally:\n                    random.setstate(py_state)\n                    np.random.set_state(np_state)\n                    torch.set_rng_state(torch_state)\n                return result\n            else:\n                return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.gaussian","title":"embodied_gen.utils.gaussian","text":""},{"location":"api/utils/#embodied_gen.utils.gaussian.export_splats","title":"export_splats","text":"<pre><code>export_splats(means: Tensor, scales: Tensor, quats: Tensor, opacities: Tensor, sh0: Tensor, shN: Tensor, format: Literal['ply'] = 'ply', save_to: Optional[str] = None) -&gt; bytes\n</code></pre> <p>Export a Gaussian Splats model to bytes in PLY file format.</p> Source code in <code>embodied_gen/utils/gaussian.py</code> <pre><code>def export_splats(\n    means: torch.Tensor,\n    scales: torch.Tensor,\n    quats: torch.Tensor,\n    opacities: torch.Tensor,\n    sh0: torch.Tensor,\n    shN: torch.Tensor,\n    format: Literal[\"ply\"] = \"ply\",\n    save_to: Optional[str] = None,\n) -&gt; bytes:\n    \"\"\"Export a Gaussian Splats model to bytes in PLY file format.\"\"\"\n    total_splats = means.shape[0]\n    assert means.shape == (total_splats, 3), \"Means must be of shape (N, 3)\"\n    assert scales.shape == (total_splats, 3), \"Scales must be of shape (N, 3)\"\n    assert quats.shape == (\n        total_splats,\n        4,\n    ), \"Quaternions must be of shape (N, 4)\"\n    assert opacities.shape == (\n        total_splats,\n    ), \"Opacities must be of shape (N,)\"\n    assert sh0.shape == (total_splats, 1, 3), \"sh0 must be of shape (N, 1, 3)\"\n    assert (\n        shN.ndim == 3 and shN.shape[0] == total_splats and shN.shape[2] == 3\n    ), f\"shN must be of shape (N, K, 3), got {shN.shape}\"\n\n    # Reshape spherical harmonics\n    sh0 = sh0.squeeze(1)  # Shape (N, 3)\n    shN = shN.permute(0, 2, 1).reshape(means.shape[0], -1)  # Shape (N, K * 3)\n\n    # Check for NaN or Inf values\n    invalid_mask = (\n        torch.isnan(means).any(dim=1)\n        | torch.isinf(means).any(dim=1)\n        | torch.isnan(scales).any(dim=1)\n        | torch.isinf(scales).any(dim=1)\n        | torch.isnan(quats).any(dim=1)\n        | torch.isinf(quats).any(dim=1)\n        | torch.isnan(opacities).any(dim=0)\n        | torch.isinf(opacities).any(dim=0)\n        | torch.isnan(sh0).any(dim=1)\n        | torch.isinf(sh0).any(dim=1)\n        | torch.isnan(shN).any(dim=1)\n        | torch.isinf(shN).any(dim=1)\n    )\n\n    # Filter out invalid entries\n    valid_mask = ~invalid_mask\n    means = means[valid_mask]\n    scales = scales[valid_mask]\n    quats = quats[valid_mask]\n    opacities = opacities[valid_mask]\n    sh0 = sh0[valid_mask]\n    shN = shN[valid_mask]\n\n    if format == \"ply\":\n        data = splat2ply_bytes(means, scales, quats, opacities, sh0, shN)\n    else:\n        raise ValueError(f\"Unsupported format: {format}\")\n\n    if save_to:\n        with open(save_to, \"wb\") as binary_file:\n            binary_file.write(data)\n\n    return data\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.gaussian.restore_scene_scale_and_position","title":"restore_scene_scale_and_position","text":"<pre><code>restore_scene_scale_and_position(real_height: float, mesh_path: str, gs_path: str) -&gt; None\n</code></pre> <p>Scales a mesh and corresponding GS model to match a given real-world height.</p> <p>Uses the 1st and 99th percentile of mesh Z-axis to estimate height, applies scaling and vertical alignment, and updates both the mesh and GS model.</p> <p>Parameters:</p> Name Type Description Default <code>real_height</code> <code>float</code> <p>Target real-world height among Z axis.</p> required <code>mesh_path</code> <code>str</code> <p>Path to the input mesh file.</p> required <code>gs_path</code> <code>str</code> <p>Path to the Gaussian Splatting model file.</p> required Source code in <code>embodied_gen/utils/gaussian.py</code> <pre><code>def restore_scene_scale_and_position(\n    real_height: float, mesh_path: str, gs_path: str\n) -&gt; None:\n    \"\"\"Scales a mesh and corresponding GS model to match a given real-world height.\n\n    Uses the 1st and 99th percentile of mesh Z-axis to estimate height,\n    applies scaling and vertical alignment, and updates both the mesh and GS model.\n\n    Args:\n        real_height (float): Target real-world height among Z axis.\n        mesh_path (str): Path to the input mesh file.\n        gs_path (str): Path to the Gaussian Splatting model file.\n    \"\"\"\n    mesh = trimesh.load(mesh_path)\n    z_min = np.percentile(mesh.vertices[:, 1], 1)\n    z_max = np.percentile(mesh.vertices[:, 1], 99)\n    height = z_max - z_min\n    scale = real_height / height\n\n    rot = Rotation.from_quat([0, 1, 0, 0])\n    mesh.vertices = rot.apply(mesh.vertices)\n    mesh.vertices[:, 1] -= z_min\n    mesh.vertices *= scale\n    mesh.export(mesh_path)\n\n    gs_model: GaussianOperator = GaussianOperator.load_from_ply(gs_path)\n    gs_model = gs_model.get_gaussians(\n        instance_pose=torch.tensor([0.0, -z_min, 0, 0, 1, 0, 0])\n    )\n    gs_model.rescale(scale)\n    gs_model.save_to_ply(gs_path)\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.gpt_clients","title":"embodied_gen.utils.gpt_clients","text":""},{"location":"api/utils/#embodied_gen.utils.gpt_clients.GPTclient","title":"GPTclient","text":"<pre><code>GPTclient(endpoint: str, api_key: str, model_name: str = 'yfb-gpt-4o', api_version: str = None, check_connection: bool = True, verbose: bool = False)\n</code></pre> <p>A client to interact with the GPT model via OpenAI or Azure API.</p> Source code in <code>embodied_gen/utils/gpt_clients.py</code> <pre><code>def __init__(\n    self,\n    endpoint: str,\n    api_key: str,\n    model_name: str = \"yfb-gpt-4o\",\n    api_version: str = None,\n    check_connection: bool = True,\n    verbose: bool = False,\n):\n    if api_version is not None:\n        self.client = AzureOpenAI(\n            azure_endpoint=endpoint,\n            api_key=api_key,\n            api_version=api_version,\n        )\n    else:\n        self.client = OpenAI(\n            base_url=endpoint,\n            api_key=api_key,\n        )\n\n    self.endpoint = endpoint\n    self.model_name = model_name\n    self.image_formats = {\".png\", \".jpg\", \".jpeg\", \".webp\", \".bmp\", \".gif\"}\n    self.verbose = verbose\n    if check_connection:\n        self.check_connection()\n\n    logger.info(f\"Using GPT model: {self.model_name}.\")\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.gpt_clients.GPTclient.check_connection","title":"check_connection","text":"<pre><code>check_connection() -&gt; None\n</code></pre> <p>Check whether the GPT API connection is working.</p> Source code in <code>embodied_gen/utils/gpt_clients.py</code> <pre><code>def check_connection(self) -&gt; None:\n    \"\"\"Check whether the GPT API connection is working.\"\"\"\n    try:\n        response = self.completion_with_backoff(\n            messages=[\n                {\"role\": \"system\", \"content\": \"You are a test system.\"},\n                {\"role\": \"user\", \"content\": \"Hello\"},\n            ],\n            model=self.model_name,\n            temperature=0,\n            max_tokens=100,\n        )\n        content = response.choices[0].message.content\n        logger.info(f\"Connection check success.\")\n    except Exception as e:\n        raise ConnectionError(\n            f\"Failed to connect to GPT API at {self.endpoint}, \"\n            f\"please check setting in `{CONFIG_FILE}` and `README`.\"\n        )\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.gpt_clients.GPTclient.query","title":"query","text":"<pre><code>query(text_prompt: str, image_base64: Optional[list[str | Image]] = None, system_role: Optional[str] = None, params: Optional[dict] = None) -&gt; Optional[str]\n</code></pre> <p>Queries the GPT model with a text and optional image prompts.</p> <p>Parameters:</p> Name Type Description Default <code>text_prompt</code> <code>str</code> <p>The main text input that the model responds to.</p> required <code>image_base64</code> <code>Optional[List[str]]</code> <p>A list of image base64 strings or local image paths or PIL.Image to accompany the text prompt.</p> <code>None</code> <code>system_role</code> <code>Optional[str]</code> <p>Optional system-level instructions that specify the behavior of the assistant.</p> <code>None</code> <code>params</code> <code>Optional[dict]</code> <p>Additional parameters for GPT setting.</p> <code>None</code> <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Optional[str]: The response content generated by the model based on the prompt. Returns <code>None</code> if an error occurs.</p> Source code in <code>embodied_gen/utils/gpt_clients.py</code> <pre><code>def query(\n    self,\n    text_prompt: str,\n    image_base64: Optional[list[str | Image.Image]] = None,\n    system_role: Optional[str] = None,\n    params: Optional[dict] = None,\n) -&gt; Optional[str]:\n    \"\"\"Queries the GPT model with a text and optional image prompts.\n\n    Args:\n        text_prompt (str): The main text input that the model responds to.\n        image_base64 (Optional[List[str]]): A list of image base64 strings\n            or local image paths or PIL.Image to accompany the text prompt.\n        system_role (Optional[str]): Optional system-level instructions\n            that specify the behavior of the assistant.\n        params (Optional[dict]): Additional parameters for GPT setting.\n\n    Returns:\n        Optional[str]: The response content generated by the model based on\n            the prompt. Returns `None` if an error occurs.\n    \"\"\"\n    if system_role is None:\n        system_role = \"You are a highly knowledgeable assistant specializing in physics, engineering, and object properties.\"  # noqa\n\n    content_user = [\n        {\n            \"type\": \"text\",\n            \"text\": text_prompt,\n        },\n    ]\n\n    # Process images if provided\n    if image_base64 is not None:\n        if not isinstance(image_base64, list):\n            image_base64 = [image_base64]\n        # Hardcode tmp because of the openrouter can't input multi images.\n        if \"openrouter\" in self.endpoint:\n            image_base64 = combine_images_to_grid(image_base64)\n        for img in image_base64:\n            if isinstance(img, Image.Image):\n                buffer = BytesIO()\n                img.save(buffer, format=img.format or \"PNG\")\n                buffer.seek(0)\n                image_binary = buffer.read()\n                img = base64.b64encode(image_binary).decode(\"utf-8\")\n            elif (\n                len(os.path.splitext(img)) &gt; 1\n                and os.path.splitext(img)[-1].lower() in self.image_formats\n            ):\n                if not os.path.exists(img):\n                    raise FileNotFoundError(f\"Image file not found: {img}\")\n                with open(img, \"rb\") as f:\n                    img = base64.b64encode(f.read()).decode(\"utf-8\")\n\n            content_user.append(\n                {\n                    \"type\": \"image_url\",\n                    \"image_url\": {\"url\": f\"data:image/png;base64,{img}\"},\n                }\n            )\n\n    payload = {\n        \"messages\": [\n            {\"role\": \"system\", \"content\": system_role},\n            {\"role\": \"user\", \"content\": content_user},\n        ],\n        \"temperature\": 0.1,\n        \"max_tokens\": 500,\n        \"top_p\": 0.1,\n        \"frequency_penalty\": 0,\n        \"presence_penalty\": 0,\n        \"stop\": None,\n        \"model\": self.model_name,\n    }\n\n    if params:\n        payload.update(params)\n\n    response = None\n    try:\n        response = self.completion_with_backoff(**payload)\n        response = response.choices[0].message.content\n    except Exception as e:\n        logger.error(f\"Error GPTclint {self.endpoint} API call: {e}\")\n        response = None\n\n    if self.verbose:\n        logger.info(f\"Prompt: {text_prompt}\")\n        logger.info(f\"Response: {response}\")\n\n    return response\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.process_media","title":"embodied_gen.utils.process_media","text":""},{"location":"api/utils/#embodied_gen.utils.process_media.alpha_blend_rgba","title":"alpha_blend_rgba","text":"<pre><code>alpha_blend_rgba(fg_image: Union[str, Image, ndarray], bg_image: Union[str, Image, ndarray]) -&gt; Image.Image\n</code></pre> <p>Alpha blends a foreground RGBA image over a background RGBA image.</p> <p>Parameters:</p> Name Type Description Default <code>fg_image</code> <code>Union[str, Image, ndarray]</code> <p>Foreground image. Can be a file path (str), a PIL Image, or a NumPy ndarray.</p> required <code>bg_image</code> <code>Union[str, Image, ndarray]</code> <p>Background image. Can be a file path (str), a PIL Image, or a NumPy ndarray.</p> required <p>Returns:</p> Type Description <code>Image</code> <p>A PIL Image representing the alpha-blended result in RGBA mode.</p> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def alpha_blend_rgba(\n    fg_image: Union[str, Image.Image, np.ndarray],\n    bg_image: Union[str, Image.Image, np.ndarray],\n) -&gt; Image.Image:\n    \"\"\"Alpha blends a foreground RGBA image over a background RGBA image.\n\n    Args:\n        fg_image: Foreground image. Can be a file path (str), a PIL Image,\n            or a NumPy ndarray.\n        bg_image: Background image. Can be a file path (str), a PIL Image,\n            or a NumPy ndarray.\n\n    Returns:\n        A PIL Image representing the alpha-blended result in RGBA mode.\n    \"\"\"\n    if isinstance(fg_image, str):\n        fg_image = Image.open(fg_image)\n    elif isinstance(fg_image, np.ndarray):\n        fg_image = Image.fromarray(fg_image)\n\n    if isinstance(bg_image, str):\n        bg_image = Image.open(bg_image)\n    elif isinstance(bg_image, np.ndarray):\n        bg_image = Image.fromarray(bg_image)\n\n    if fg_image.size != bg_image.size:\n        raise ValueError(\n            f\"Image sizes not match {fg_image.size} v.s. {bg_image.size}.\"\n        )\n\n    fg = fg_image.convert(\"RGBA\")\n    bg = bg_image.convert(\"RGBA\")\n\n    return Image.alpha_composite(bg, fg)\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.process_media.check_object_edge_truncated","title":"check_object_edge_truncated","text":"<pre><code>check_object_edge_truncated(mask: ndarray, edge_threshold: int = 5) -&gt; bool\n</code></pre> <p>Checks if a binary object mask is truncated at the image edges.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray</code> <p>A 2D binary NumPy array where nonzero values indicate the object region.</p> required <code>edge_threshold</code> <code>int</code> <p>Number of pixels from each image edge to consider for truncation. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the object is fully enclosed (not truncated).</p> <code>bool</code> <p>False if the object touches or crosses any image boundary.</p> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def check_object_edge_truncated(\n    mask: np.ndarray, edge_threshold: int = 5\n) -&gt; bool:\n    \"\"\"Checks if a binary object mask is truncated at the image edges.\n\n    Args:\n        mask: A 2D binary NumPy array where nonzero values indicate the object region.\n        edge_threshold: Number of pixels from each image edge to consider for truncation.\n            Defaults to 5.\n\n    Returns:\n        True if the object is fully enclosed (not truncated).\n        False if the object touches or crosses any image boundary.\n    \"\"\"\n    top = mask[:edge_threshold, :].any()\n    bottom = mask[-edge_threshold:, :].any()\n    left = mask[:, :edge_threshold].any()\n    right = mask[:, -edge_threshold:].any()\n\n    return not (top or bottom or left or right)\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.process_media.merge_video_video","title":"merge_video_video","text":"<pre><code>merge_video_video(video_path1: str, video_path2: str, output_path: str) -&gt; None\n</code></pre> <p>Merge two videos by the left half and the right half of the videos.</p> Source code in <code>embodied_gen/utils/process_media.py</code> <pre><code>def merge_video_video(\n    video_path1: str, video_path2: str, output_path: str\n) -&gt; None:\n    \"\"\"Merge two videos by the left half and the right half of the videos.\"\"\"\n    clip1 = VideoFileClip(video_path1)\n    clip2 = VideoFileClip(video_path2)\n\n    if clip1.size != clip2.size:\n        raise ValueError(\"The resolutions of the two videos do not match.\")\n\n    width, height = clip1.size\n    clip1_half = clip1.crop(x1=0, y1=0, x2=width // 2, y2=height)\n    clip2_half = clip2.crop(x1=width // 2, y1=0, x2=width, y2=height)\n    final_clip = clips_array([[clip1_half, clip2_half]])\n    final_clip.write_videofile(output_path, codec=\"libx264\")\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.simulation","title":"embodied_gen.utils.simulation","text":""},{"location":"api/utils/#embodied_gen.utils.simulation.SapienSceneManager","title":"SapienSceneManager","text":"<pre><code>SapienSceneManager(sim_freq: int, ray_tracing: bool, device: str = 'cuda')\n</code></pre> <p>A class to manage SAPIEN simulator.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def __init__(\n    self, sim_freq: int, ray_tracing: bool, device: str = \"cuda\"\n) -&gt; None:\n    self.sim_freq = sim_freq\n    self.ray_tracing = ray_tracing\n    self.device = device\n    self.renderer = sapien.SapienRenderer()\n    self.scene = self._setup_scene()\n    self.cameras: list[sapien.render.RenderCameraComponent] = []\n    self.actors: dict[str, sapien.pysapien.Entity] = {}\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.simulation.SapienSceneManager.create_camera","title":"create_camera","text":"<pre><code>create_camera(cam_name: str, pose: Pose, image_hw: tuple[int, int], fovy_deg: float) -&gt; sapien.render.RenderCameraComponent\n</code></pre> <p>Create a single camera in the scene.</p> <p>Parameters:</p> Name Type Description Default <code>cam_name</code> <code>str</code> <p>Name of the camera.</p> required <code>pose</code> <code>Pose</code> <p>Camera pose p=(x, y, z), q=(w, x, y, z)</p> required <code>image_hw</code> <code>Tuple[int, int]</code> <p>Image resolution (height, width) for cameras.</p> required <code>fovy_deg</code> <code>float</code> <p>Field of view in degrees for cameras.</p> required <p>Returns:</p> Type Description <code>RenderCameraComponent</code> <p>sapien.render.RenderCameraComponent: The created camera.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def create_camera(\n    self,\n    cam_name: str,\n    pose: sapien.Pose,\n    image_hw: tuple[int, int],\n    fovy_deg: float,\n) -&gt; sapien.render.RenderCameraComponent:\n    \"\"\"Create a single camera in the scene.\n\n    Args:\n        cam_name (str): Name of the camera.\n        pose (sapien.Pose): Camera pose p=(x, y, z), q=(w, x, y, z)\n        image_hw (Tuple[int, int]): Image resolution (height, width) for cameras.\n        fovy_deg (float): Field of view in degrees for cameras.\n\n    Returns:\n        sapien.render.RenderCameraComponent: The created camera.\n    \"\"\"\n    cam_actor = self.scene.create_actor_builder().build_kinematic()\n    cam_actor.set_pose(pose)\n    camera = self.scene.add_mounted_camera(\n        name=cam_name,\n        mount=cam_actor,\n        pose=sapien.Pose(p=[0, 0, 0], q=[1, 0, 0, 0]),\n        width=image_hw[1],\n        height=image_hw[0],\n        fovy=np.deg2rad(fovy_deg),\n        near=0.01,\n        far=100,\n    )\n    self.cameras.append(camera)\n\n    return camera\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.simulation.SapienSceneManager.initialize_circular_cameras","title":"initialize_circular_cameras","text":"<pre><code>initialize_circular_cameras(num_cameras: int, radius: float, height: float, target_pt: list[float], image_hw: tuple[int, int], fovy_deg: float) -&gt; list[sapien.render.RenderCameraComponent]\n</code></pre> <p>Initialize multiple cameras arranged in a circle.</p> <p>Parameters:</p> Name Type Description Default <code>num_cameras</code> <code>int</code> <p>Number of cameras to create.</p> required <code>radius</code> <code>float</code> <p>Radius of the camera circle.</p> required <code>height</code> <code>float</code> <p>Fixed Z-coordinate of the cameras.</p> required <code>target_pt</code> <code>list[float]</code> <p>3D point (x, y, z) that cameras look at.</p> required <code>image_hw</code> <code>Tuple[int, int]</code> <p>Image resolution (height, width) for cameras.</p> required <code>fovy_deg</code> <code>float</code> <p>Field of view in degrees for cameras.</p> required <p>Returns:</p> Type Description <code>list[RenderCameraComponent]</code> <p>List[sapien.render.RenderCameraComponent]: List of created cameras.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def initialize_circular_cameras(\n    self,\n    num_cameras: int,\n    radius: float,\n    height: float,\n    target_pt: list[float],\n    image_hw: tuple[int, int],\n    fovy_deg: float,\n) -&gt; list[sapien.render.RenderCameraComponent]:\n    \"\"\"Initialize multiple cameras arranged in a circle.\n\n    Args:\n        num_cameras (int): Number of cameras to create.\n        radius (float): Radius of the camera circle.\n        height (float): Fixed Z-coordinate of the cameras.\n        target_pt (list[float]): 3D point (x, y, z) that cameras look at.\n        image_hw (Tuple[int, int]): Image resolution (height, width) for cameras.\n        fovy_deg (float): Field of view in degrees for cameras.\n\n    Returns:\n        List[sapien.render.RenderCameraComponent]: List of created cameras.\n    \"\"\"\n    angle_step = 2 * np.pi / num_cameras\n    world_up_vec = np.array([0.0, 0.0, 1.0])\n    target_pt = np.array(target_pt)\n\n    for i in range(num_cameras):\n        angle = i * angle_step\n        cam_x = radius * np.cos(angle)\n        cam_y = radius * np.sin(angle)\n        cam_z = height\n        eye_pos = [cam_x, cam_y, cam_z]\n\n        forward_vec = target_pt - eye_pos\n        forward_vec = forward_vec / np.linalg.norm(forward_vec)\n        temp_right_vec = np.cross(forward_vec, world_up_vec)\n\n        if np.linalg.norm(temp_right_vec) &lt; 1e-6:\n            temp_right_vec = np.array([1.0, 0.0, 0.0])\n            if np.abs(np.dot(temp_right_vec, forward_vec)) &gt; 0.99:\n                temp_right_vec = np.array([0.0, 1.0, 0.0])\n\n        right_vec = temp_right_vec / np.linalg.norm(temp_right_vec)\n        up_vec = np.cross(right_vec, forward_vec)\n        rotation_matrix = np.array([forward_vec, -right_vec, up_vec]).T\n\n        rot = R.from_matrix(rotation_matrix)\n        scipy_quat = rot.as_quat()  # (x, y, z, w)\n        quat = [\n            scipy_quat[3],\n            scipy_quat[0],\n            scipy_quat[1],\n            scipy_quat[2],\n        ]  # (w, x, y, z)\n\n        self.create_camera(\n            f\"camera_{i}\",\n            sapien.Pose(p=eye_pos, q=quat),\n            image_hw,\n            fovy_deg,\n        )\n\n    return self.cameras\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.simulation.load_assets_from_layout_file","title":"load_assets_from_layout_file","text":"<pre><code>load_assets_from_layout_file(scene: ManiSkillScene | Scene, layout: str, z_offset: float = 0.0, init_quat: list[float] = [0, 0, 0, 1], env_idx: int = None) -&gt; dict[str, sapien.pysapien.Entity]\n</code></pre> <p>Load assets from <code>EmbodiedGen</code> layout-gen output and create actors in the scene.</p> <p>Parameters:</p> Name Type Description Default <code>scene</code> <code>Scene | ManiSkillScene</code> <p>The SAPIEN or ManiSkill scene to load assets into.</p> required <code>layout</code> <code>str</code> <p>The layout file path.</p> required <code>z_offset</code> <code>float</code> <p>Offset to apply to the Z-coordinate of non-context objects.</p> <code>0.0</code> <code>init_quat</code> <code>List[float]</code> <p>Initial quaternion (x, y, z, w) for orientation adjustment.</p> <code>[0, 0, 0, 1]</code> <code>env_idx</code> <code>int</code> <p>Environment index for multi-environment setup.</p> <code>None</code> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def load_assets_from_layout_file(\n    scene: ManiSkillScene | sapien.Scene,\n    layout: str,\n    z_offset: float = 0.0,\n    init_quat: list[float] = [0, 0, 0, 1],\n    env_idx: int = None,\n) -&gt; dict[str, sapien.pysapien.Entity]:\n    \"\"\"Load assets from `EmbodiedGen` layout-gen output and create actors in the scene.\n\n    Args:\n        scene (sapien.Scene | ManiSkillScene): The SAPIEN or ManiSkill scene to load assets into.\n        layout (str): The layout file path.\n        z_offset (float): Offset to apply to the Z-coordinate of non-context objects.\n        init_quat (List[float]): Initial quaternion (x, y, z, w) for orientation adjustment.\n        env_idx (int): Environment index for multi-environment setup.\n    \"\"\"\n    asset_root = os.path.dirname(layout)\n    layout = LayoutInfo.from_dict(json.load(open(layout, \"r\")))\n    actors = dict()\n    for node in layout.assets:\n        file_dir = layout.assets[node]\n        file_name = f\"{node.replace(' ', '_')}.urdf\"\n        urdf_file = os.path.join(asset_root, file_dir, file_name)\n\n        if layout.objs_mapping[node] == Scene3DItemEnum.BACKGROUND.value:\n            continue\n\n        position = layout.position[node].copy()\n        if layout.objs_mapping[node] != Scene3DItemEnum.CONTEXT.value:\n            position[2] += z_offset\n\n        use_static = (\n            layout.relation.get(Scene3DItemEnum.CONTEXT.value, None) == node\n        )\n\n        # Combine initial quaternion with object quaternion\n        x, y, z, qx, qy, qz, qw = position\n        qx, qy, qz, qw = quaternion_multiply([qx, qy, qz, qw], init_quat)\n        actor = load_actor_from_urdf(\n            scene,\n            urdf_file,\n            sapien.Pose(p=[x, y, z], q=[qw, qx, qy, qz]),\n            env_idx,\n            use_static=use_static,\n            update_mass=False,\n        )\n        actors[node] = actor\n\n    return actors\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.simulation.render_images","title":"render_images","text":"<pre><code>render_images(camera: RenderCameraComponent, render_keys: list[Literal['Color', 'Segmentation', 'Normal', 'Mask', 'Depth', 'Foreground']] = None) -&gt; dict[str, Image.Image]\n</code></pre> <p>Render images from a given sapien camera.</p> <p>Parameters:</p> Name Type Description Default <code>camera</code> <code>RenderCameraComponent</code> <p>The camera to render from.</p> required <code>render_keys</code> <code>List[str]</code> <p>Types of images to render (e.g., Color, Segmentation).</p> <code>None</code> <p>Returns:</p> Type Description <code>dict[str, Image]</code> <p>Dict[str, Image.Image]: Dictionary of rendered images.</p> Source code in <code>embodied_gen/utils/simulation.py</code> <pre><code>def render_images(\n    camera: sapien.render.RenderCameraComponent,\n    render_keys: list[\n        Literal[\n            \"Color\",\n            \"Segmentation\",\n            \"Normal\",\n            \"Mask\",\n            \"Depth\",\n            \"Foreground\",\n        ]\n    ] = None,\n) -&gt; dict[str, Image.Image]:\n    \"\"\"Render images from a given sapien camera.\n\n    Args:\n        camera (sapien.render.RenderCameraComponent): The camera to render from.\n        render_keys (List[str]): Types of images to render (e.g., Color, Segmentation).\n\n    Returns:\n        Dict[str, Image.Image]: Dictionary of rendered images.\n    \"\"\"\n    if render_keys is None:\n        render_keys = [\n            \"Color\",\n            \"Segmentation\",\n            \"Normal\",\n            \"Mask\",\n            \"Depth\",\n            \"Foreground\",\n        ]\n\n    results: dict[str, Image.Image] = {}\n    if \"Color\" in render_keys:\n        color = camera.get_picture(\"Color\")\n        color_rgb = (np.clip(color[..., :3], 0, 1) * 255).astype(np.uint8)\n        results[\"Color\"] = Image.fromarray(color_rgb)\n\n    if \"Mask\" in render_keys:\n        alpha = (np.clip(color[..., 3], 0, 1) * 255).astype(np.uint8)\n        results[\"Mask\"] = Image.fromarray(alpha)\n\n    if \"Segmentation\" in render_keys:\n        seg_labels = camera.get_picture(\"Segmentation\")\n        label0 = seg_labels[..., 0].astype(np.uint8)\n        seg_color = COLOR_PALETTE[label0]\n        results[\"Segmentation\"] = Image.fromarray(seg_color)\n\n    if \"Foreground\" in render_keys:\n        seg_labels = camera.get_picture(\"Segmentation\")\n        label0 = seg_labels[..., 0]\n        mask = np.where((label0 &gt; 1), 255, 0).astype(np.uint8)\n        color = camera.get_picture(\"Color\")\n        color_rgb = (np.clip(color[..., :3], 0, 1) * 255).astype(np.uint8)\n        foreground = np.concatenate([color_rgb, mask[..., None]], axis=-1)\n        results[\"Foreground\"] = Image.fromarray(foreground)\n\n    if \"Normal\" in render_keys:\n        normal = camera.get_picture(\"Normal\")[..., :3]\n        normal_img = (((normal + 1) / 2) * 255).astype(np.uint8)\n        results[\"Normal\"] = Image.fromarray(normal_img)\n\n    if \"Depth\" in render_keys:\n        position_map = camera.get_picture(\"Position\")\n        depth = -position_map[..., 2]\n        alpha = torch.tensor(color[..., 3], dtype=torch.float32)\n        norm_depth = DiffrastRender.normalize_map_by_mask(\n            torch.tensor(depth), alpha\n        )\n        depth_img = (norm_depth * 255).to(torch.uint8).numpy()\n        results[\"Depth\"] = Image.fromarray(depth_img)\n\n    return results\n</code></pre>"},{"location":"api/utils/#embodied_gen.utils.tags","title":"embodied_gen.utils.tags","text":""},{"location":"api/utils/#embodied_gen.utils.trender","title":"embodied_gen.utils.trender","text":""},{"location":"api/utils/#embodied_gen.utils.monkey_patches","title":"embodied_gen.utils.monkey_patches","text":""},{"location":"api/validators/","title":"Validators API","text":"<p>Tools for asset validation, quality control, and conversion.</p>"},{"location":"api/validators/#embodied_gen.validators.aesthetic_predictor","title":"embodied_gen.validators.aesthetic_predictor","text":""},{"location":"api/validators/#embodied_gen.validators.aesthetic_predictor.AestheticPredictor","title":"AestheticPredictor","text":"<pre><code>AestheticPredictor(clip_model_dir=None, sac_model_path=None, device='cpu')\n</code></pre> <p>Aesthetic Score Predictor.</p> <p>Checkpoints from https://github.com/christophschuhmann/improved-aesthetic-predictor/tree/main</p> <p>Parameters:</p> Name Type Description Default <code>clip_model_dir</code> <code>str</code> <p>Path to the directory of the CLIP model.</p> <code>None</code> <code>sac_model_path</code> <code>str</code> <p>Path to the pre-trained SAC model.</p> <code>None</code> <code>device</code> <code>str</code> <p>Device to use for computation (\"cuda\" or \"cpu\").</p> <code>'cpu'</code> Source code in <code>embodied_gen/validators/aesthetic_predictor.py</code> <pre><code>def __init__(self, clip_model_dir=None, sac_model_path=None, device=\"cpu\"):\n\n    self.device = device\n\n    if clip_model_dir is None:\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=\"aesthetic/*\"\n        )\n        suffix = \"aesthetic\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        clip_model_dir = os.path.join(model_path, suffix)\n\n    if sac_model_path is None:\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=\"aesthetic/*\"\n        )\n        suffix = \"aesthetic\"\n        model_path = snapshot_download(\n            repo_id=\"xinjjj/RoboAssetGen\", allow_patterns=f\"{suffix}/*\"\n        )\n        sac_model_path = os.path.join(\n            model_path, suffix, \"sac+logos+ava1-l14-linearMSE.pth\"\n        )\n\n    self.clip_model, self.preprocess = self._load_clip_model(\n        clip_model_dir\n    )\n    self.sac_model = self._load_sac_model(sac_model_path, input_size=768)\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.aesthetic_predictor.AestheticPredictor.normalized","title":"normalized  <code>staticmethod</code>","text":"<pre><code>normalized(a, axis=-1, order=2)\n</code></pre> <p>Normalize the array to unit norm.</p> Source code in <code>embodied_gen/validators/aesthetic_predictor.py</code> <pre><code>@staticmethod\ndef normalized(a, axis=-1, order=2):\n    \"\"\"Normalize the array to unit norm.\"\"\"\n    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n    l2[l2 == 0] = 1\n    return a / np.expand_dims(l2, axis)\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.aesthetic_predictor.AestheticPredictor.predict","title":"predict","text":"<pre><code>predict(image_path)\n</code></pre> <p>Predict the aesthetic score for a given image.</p> <p>Parameters:</p> Name Type Description Default <code>image_path</code> <code>str</code> <p>Path to the image file.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Predicted aesthetic score.</p> Source code in <code>embodied_gen/validators/aesthetic_predictor.py</code> <pre><code>def predict(self, image_path):\n    \"\"\"Predict the aesthetic score for a given image.\n\n    Args:\n        image_path (str): Path to the image file.\n\n    Returns:\n        float: Predicted aesthetic score.\n    \"\"\"\n    pil_image = Image.open(image_path)\n    image = self.preprocess(pil_image).unsqueeze(0).to(self.device)\n\n    with torch.no_grad():\n        # Extract CLIP features\n        image_features = self.clip_model.encode_image(image)\n        # Normalize features\n        normalized_features = self.normalized(\n            image_features.cpu().detach().numpy()\n        )\n        # Predict score\n        prediction = self.sac_model(\n            torch.from_numpy(normalized_features)\n            .type(torch.FloatTensor)\n            .to(self.device)\n        )\n\n    return prediction.item()\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.quality_checkers","title":"embodied_gen.validators.quality_checkers","text":""},{"location":"api/validators/#embodied_gen.validators.quality_checkers.ImageAestheticChecker","title":"ImageAestheticChecker","text":"<pre><code>ImageAestheticChecker(clip_model_dir: str = None, sac_model_path: str = None, thresh: float = 4.5, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>A class for evaluating the aesthetic quality of images.</p> <p>Attributes:</p> Name Type Description <code>clip_model_dir</code> <code>str</code> <p>Path to the CLIP model directory.</p> <code>sac_model_path</code> <code>str</code> <p>Path to the aesthetic predictor model weights.</p> <code>thresh</code> <code>float</code> <p>Threshold above which images are considered aesthetically acceptable.</p> <code>verbose</code> <code>bool</code> <p>Whether to print detailed log messages.</p> <code>predictor</code> <code>AestheticPredictor</code> <p>The model used to predict aesthetic scores.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    clip_model_dir: str = None,\n    sac_model_path: str = None,\n    thresh: float = 4.50,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(verbose=verbose)\n    self.clip_model_dir = clip_model_dir\n    self.sac_model_path = sac_model_path\n    self.thresh = thresh\n    self.predictor = AestheticPredictor(clip_model_dir, sac_model_path)\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.quality_checkers.ImageSegChecker","title":"ImageSegChecker","text":"<pre><code>ImageSegChecker(gpt_client: GPTclient, prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>A segmentation quality checker for 3D assets using GPT-based reasoning.</p> <p>This class compares an original image with its segmented version to evaluate whether the segmentation successfully isolates the main object with minimal truncation and correct foreground extraction.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>GPT client used for multi-modal image analysis.</p> <code>prompt</code> <code>str</code> <p>The prompt used to guide the GPT model for evaluation.</p> <code>verbose</code> <code>bool</code> <p>Whether to enable verbose logging.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        Task: Evaluate the quality of object segmentation between two images:\n            the first is the original, the second is the segmented result.\n\n        Criteria:\n        - The main foreground object should be clearly extracted (not the background).\n        - The object must appear realistic, with reasonable geometry and color.\n        - The object should be geometrically complete \u2014 no missing, truncated, or cropped parts.\n        - The object must be centered, with a margin on all sides.\n        - Ignore minor imperfections (e.g., small holes or fine edge artifacts).\n\n        Output Rules:\n        If segmentation is acceptable, respond with \"YES\" (and nothing else).\n        If not acceptable, respond with \"NO\", followed by a brief reason (max 20 words).\n        \"\"\"\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.quality_checkers.MeshGeoChecker","title":"MeshGeoChecker","text":"<pre><code>MeshGeoChecker(gpt_client: GPTclient, prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>A geometry quality checker for 3D mesh assets using GPT-based reasoning.</p> <p>This class leverages a multi-modal GPT client to analyze rendered images of a 3D object and determine if its geometry is complete.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>The GPT client used for multi-modal querying.</p> <code>prompt</code> <code>str</code> <p>The prompt sent to the GPT model. If not provided, a default one is used.</p> <code>verbose</code> <code>bool</code> <p>Whether to print debug information during evaluation.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        You are an expert in evaluating the geometry quality of generated 3D asset.\n        You will be given rendered views of a generated 3D asset, type {}, with black background.\n        Your task is to evaluate the quality of the 3D asset generation,\n        including geometry, structure, and appearance, based on the rendered views.\n        Criteria:\n        - Is the object in the image a single, complete, and well-formed instance,\n            without truncation, missing parts, overlapping duplicates, or redundant geometry?\n        - Minor flaws, asymmetries, or simplifications (e.g., less detail on sides or back,\n            soft edges) are acceptable if the object is structurally sound and recognizable.\n        - Only evaluate geometry. Do not assess texture quality.\n        - The asset should not contain any unrelated elements, such as\n            ground planes, platforms, or background props (e.g., paper, flooring).\n\n        If all the above criteria are met, return \"YES\". Otherwise, return\n            \"NO\" followed by a brief explanation (no more than 20 words).\n\n        Example:\n        Images show a yellow cup standing on a flat white plane -&gt; NO\n        -&gt; Response: NO: extra white surface under the object.\n        Image shows a chair with simplified back legs and soft edges \u2192 YES\n        \"\"\"\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.quality_checkers.PanoHeightEstimator","title":"PanoHeightEstimator","text":"<pre><code>PanoHeightEstimator(gpt_client: GPTclient, default_value: float = 3.5)\n</code></pre> <p>               Bases: <code>object</code></p> <p>Estimate the real ceiling height of an indoor space from a 360\u00b0 panoramic image.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>The GPT client used to perform image-based reasoning and return height estimates.</p> <code>default_value</code> <code>float</code> <p>The fallback height in meters if parsing the GPT output fails.</p> <code>prompt</code> <code>str</code> <p>The textual instruction used to guide the GPT model for height estimation.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    default_value: float = 3.5,\n) -&gt; None:\n    self.gpt_client = gpt_client\n    self.default_value = default_value\n    self.prompt = \"\"\"\n    You are an expert in building height estimation and panoramic image analysis.\n    Your task is to analyze a 360\u00b0 indoor panoramic image and estimate the **actual height** of the space in meters.\n\n    Consider the following visual cues:\n    1. Ceiling visibility and reference objects (doors, windows, furniture, appliances).\n    2. Floor features or level differences.\n    3. Room type (e.g., residential, office, commercial).\n    4. Object-to-ceiling proportions (e.g., height of doors relative to ceiling).\n    5. Architectural elements (e.g., chandeliers, shelves, kitchen cabinets).\n\n    Input: A full 360\u00b0 panoramic indoor photo.\n    Output: A single number in meters representing the estimated room height. Only return the number (e.g., `3.2`)\n    \"\"\"\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.quality_checkers.PanoImageGenChecker","title":"PanoImageGenChecker","text":"<pre><code>PanoImageGenChecker(gpt_client: GPTclient, prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>A checker class that validates the quality and realism of generated panoramic indoor images.</p> <p>Attributes:</p> Name Type Description <code>gpt_client</code> <code>GPTclient</code> <p>A GPT client instance used to query for image validation.</p> <code>prompt</code> <code>str</code> <p>The instruction prompt passed to the GPT model. If None, a default prompt is used.</p> <code>verbose</code> <code>bool</code> <p>Whether to print internal processing information for debugging.</p> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        You are a panoramic image analyzer specializing in indoor room structure validation.\n\n        Given a generated panoramic image, assess if it meets all the criteria:\n        - Floor Space: \u226530 percent of the floor is free of objects or obstructions.\n        - Visual Clarity: Floor, walls, and ceiling are clear, with no distortion, blur, noise.\n        - Structural Continuity: Surfaces form plausible, continuous geometry\n            without breaks, floating parts, or abrupt cuts.\n        - Spatial Completeness: Full 360\u00b0 coverage without missing areas,\n            seams, gaps, or stitching artifacts.\n        Instructions:\n        - If all criteria are met, reply with \"YES\".\n        - Otherwise, reply with \"NO: &lt;brief explanation&gt;\" (max 20 words).\n\n        Respond exactly as:\n        \"YES\"\n        or\n        \"NO: brief explanation.\"\n        \"\"\"\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.quality_checkers.PanoImageOccChecker","title":"PanoImageOccChecker","text":"<pre><code>PanoImageOccChecker(gpt_client: GPTclient, box_hw: tuple[int, int], prompt: str = None, verbose: bool = False)\n</code></pre> <p>               Bases: <code>BaseChecker</code></p> <p>Checks for physical obstacles in the bottom-center region of a panoramic image.</p> <p>This class crops a specified region from the input panoramic image and uses a GPT client to determine whether any physical obstacles there.</p> <p>Parameters:</p> Name Type Description Default <code>gpt_client</code> <code>GPTclient</code> <p>The GPT-based client used for visual reasoning.</p> required <code>box_hw</code> <code>tuple[int, int]</code> <p>The height and width of the crop box.</p> required <code>prompt</code> <code>str</code> <p>Custom prompt for the GPT client. Defaults to a predefined one.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print verbose logs. Defaults to False.</p> <code>False</code> Source code in <code>embodied_gen/validators/quality_checkers.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    box_hw: tuple[int, int],\n    prompt: str = None,\n    verbose: bool = False,\n) -&gt; None:\n    super().__init__(prompt, verbose)\n    self.gpt_client = gpt_client\n    self.box_hw = box_hw\n    if self.prompt is None:\n        self.prompt = \"\"\"\n        This image is a cropped region from the bottom-center of a panoramic view.\n        Please determine whether there is any obstacle present \u2014 such as furniture, tables, or other physical objects.\n        Ignore floor textures, rugs, carpets, shadows, and lighting effects \u2014 they do not count as obstacles.\n        Only consider real, physical objects that could block walking or movement.\n\n        Instructions:\n        - If there is no obstacle, reply: \"YES\".\n        - Otherwise, reply: \"NO: &lt;brief explanation&gt;\" (max 20 words).\n\n        Respond exactly as:\n        \"YES\"\n        or\n        \"NO: brief explanation.\"\n        \"\"\"\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.urdf_convertor","title":"embodied_gen.validators.urdf_convertor","text":""},{"location":"api/validators/#embodied_gen.validators.urdf_convertor.URDFGenerator","title":"URDFGenerator","text":"<pre><code>URDFGenerator(gpt_client: GPTclient, mesh_file_list: list[str] = ['material_0.png', 'material.mtl'], prompt_template: str = None, attrs_name: list[str] = None, render_dir: str = 'urdf_renders', render_view_num: int = 4, decompose_convex: bool = False, rotate_xyzw: list[float] = (0.7071, 0, 0, 0.7071))\n</code></pre> <p>               Bases: <code>object</code></p> Source code in <code>embodied_gen/validators/urdf_convertor.py</code> <pre><code>def __init__(\n    self,\n    gpt_client: GPTclient,\n    mesh_file_list: list[str] = [\"material_0.png\", \"material.mtl\"],\n    prompt_template: str = None,\n    attrs_name: list[str] = None,\n    render_dir: str = \"urdf_renders\",\n    render_view_num: int = 4,\n    decompose_convex: bool = False,\n    rotate_xyzw: list[float] = (0.7071, 0, 0, 0.7071),\n) -&gt; None:\n    if mesh_file_list is None:\n        mesh_file_list = []\n    self.mesh_file_list = mesh_file_list\n    self.output_mesh_dir = \"mesh\"\n    self.output_render_dir = render_dir\n    self.gpt_client = gpt_client\n    self.render_view_num = render_view_num\n    if render_view_num == 4:\n        view_desc = \"This is orthographic projection showing the front, left, right and back views \"  # noqa\n    else:\n        view_desc = \"This is the rendered views \"\n\n    if prompt_template is None:\n        prompt_template = (\n            view_desc\n            + \"\"\"of the 3D object asset,\n        category: {category}.\n        You are an expert in 3D object analysis and physical property estimation.\n        Give the category of this object asset (within 3 words), (if category is\n        already provided, use it directly), accurately describe this 3D object asset (within 15 words),\n        Determine the pose of the object in the first image and estimate the true vertical height\n        (vertical projection) range of the object (in meters), i.e., how tall the object appears from top\n        to bottom in the first image. also weight range (unit: kilogram), the average\n        static friction coefficient of the object relative to rubber and the average dynamic friction\n        coefficient of the object relative to rubber. Return response in format as shown in Output Example.\n\n        Output Example:\n        Category: cup\n        Description: shiny golden cup with floral design\n        Pose: &lt;short_description_within_10_words&gt;\n        Height: 0.10-0.15 m\n        Weight: 0.3-0.6 kg\n        Static friction coefficient: 0.6\n        Dynamic friction coefficient: 0.5\n\n        IMPORTANT: Estimating Vertical Height from the First (Front View) Image and pose estimation based on all views.\n        - The \"vertical height\" refers to the real-world vertical size of the object\n        as projected in the first image, aligned with the image's vertical axis.\n        - For flat objects like plates or disks or book, if their face is visible in the front view,\n        use the diameter as the vertical height. If the edge is visible, use the thickness instead.\n        - This is not necessarily the full length of the object, but how tall it appears\n        in the first image vertically, based on its pose and orientation estimation on all views.\n        - For objects(e.g., spoons, forks, writing instruments etc.) at an angle showing in images,\n            e.g., tilted at 45\u00b0 will appear shorter vertically than when upright.\n        Estimate the vertical projection of their real length based on its pose.\n        For example:\n          - A pen standing upright in the first image (aligned with the image's vertical axis)\n            full body visible in the first image: \u2192 vertical height \u2248 0.14-0.20 m\n          - A pen lying flat in the first image or either the tip or the tail is facing the image\n            (showing thickness or as a circle) \u2192 vertical height \u2248 0.018-0.025 m\n          - Tilted pen in the first image (e.g., ~45\u00b0 angle): vertical height \u2248 0.07-0.12 m\n        - Use the rest views to help determine the object's 3D pose and orientation.\n        Assume the object is in real-world scale and estimate the approximate vertical height\n        based on the pose estimation and how large it appears vertically in the first image.\n        \"\"\"\n        )\n\n    self.prompt_template = prompt_template\n    if attrs_name is None:\n        attrs_name = [\n            \"category\",\n            \"description\",\n            \"min_height\",\n            \"max_height\",\n            \"real_height\",\n            \"min_mass\",\n            \"max_mass\",\n            \"version\",\n            \"generate_time\",\n            \"gs_model\",\n        ]\n    self.attrs_name = attrs_name\n    self.decompose_convex = decompose_convex\n    # Rotate 90 degrees around the X-axis from blender to align with simulators.\n    self.rotate_xyzw = rotate_xyzw\n</code></pre>"},{"location":"api/validators/#embodied_gen.validators.urdf_convertor.URDFGenerator.generate_urdf","title":"generate_urdf","text":"<pre><code>generate_urdf(input_mesh: str, output_dir: str, attr_dict: dict, output_name: str = None) -&gt; str\n</code></pre> <p>Generate a URDF file for a given mesh with specified attributes.</p> <p>Parameters:</p> Name Type Description Default <code>input_mesh</code> <code>str</code> <p>Path to the input mesh file.</p> required <code>output_dir</code> <code>str</code> <p>Directory to store the generated URDF and processed mesh.</p> required <code>attr_dict</code> <code>dict</code> <p>Dictionary containing attributes like height, mass, and friction coefficients.</p> required <code>output_name</code> <code>str</code> <p>Name for the generated URDF and robot.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path to the generated URDF file.</p> Source code in <code>embodied_gen/validators/urdf_convertor.py</code> <pre><code>def generate_urdf(\n    self,\n    input_mesh: str,\n    output_dir: str,\n    attr_dict: dict,\n    output_name: str = None,\n) -&gt; str:\n    \"\"\"Generate a URDF file for a given mesh with specified attributes.\n\n    Args:\n        input_mesh (str): Path to the input mesh file.\n        output_dir (str): Directory to store the generated URDF\n            and processed mesh.\n        attr_dict (dict): Dictionary containing attributes like height,\n            mass, and friction coefficients.\n        output_name (str, optional): Name for the generated URDF and robot.\n\n    Returns:\n        str: Path to the generated URDF file.\n    \"\"\"\n\n    # 1. Load and normalize the mesh\n    mesh = trimesh.load(input_mesh)\n    mesh_scale = np.ptp(mesh.vertices, axis=0).max()\n    mesh.vertices /= mesh_scale  # Normalize to [-0.5, 0.5]\n    raw_height = np.ptp(mesh.vertices, axis=0)[1]\n\n    # 2. Scale the mesh to real height\n    real_height = attr_dict[\"real_height\"]\n    scale = round(real_height / raw_height, 6)\n    mesh = mesh.apply_scale(scale)\n\n    # 3. Prepare output directories and save scaled mesh\n    mesh_folder = os.path.join(output_dir, self.output_mesh_dir)\n    os.makedirs(mesh_folder, exist_ok=True)\n\n    obj_name = os.path.basename(input_mesh)\n    mesh_output_path = os.path.join(mesh_folder, obj_name)\n    mesh.export(mesh_output_path)\n\n    # 4. Copy additional mesh files, if any\n    input_dir = os.path.dirname(input_mesh)\n    for file in self.mesh_file_list:\n        src_file = os.path.join(input_dir, file)\n        dest_file = os.path.join(mesh_folder, file)\n        if os.path.isfile(src_file):\n            shutil.copy(src_file, dest_file)\n\n    # 5. Determine output name\n    if output_name is None:\n        output_name = os.path.splitext(obj_name)[0]\n\n    # 6. Load URDF template and update attributes\n    robot = ET.fromstring(URDF_TEMPLATE)\n    robot.set(\"name\", output_name)\n\n    link = robot.find(\"link\")\n    if link is None:\n        raise ValueError(\"URDF template is missing 'link' element.\")\n    link.set(\"name\", output_name)\n\n    if self.rotate_xyzw is not None:\n        rpy = Rotation.from_quat(self.rotate_xyzw).as_euler(\n            \"xyz\", degrees=False\n        )\n        rpy = [str(round(num, 4)) for num in rpy]\n        link.find(\"visual/origin\").set(\"rpy\", \" \".join(rpy))\n        link.find(\"collision/origin\").set(\"rpy\", \" \".join(rpy))\n\n    # Update visual geometry\n    visual = link.find(\"visual/geometry/mesh\")\n    if visual is not None:\n        visual.set(\n            \"filename\", os.path.join(self.output_mesh_dir, obj_name)\n        )\n        visual.set(\"scale\", \"1.0 1.0 1.0\")\n\n    # Update collision geometry\n    collision = link.find(\"collision/geometry/mesh\")\n    if collision is not None:\n        collision_mesh = os.path.join(self.output_mesh_dir, obj_name)\n        if self.decompose_convex:\n            try:\n                d_params = dict(\n                    threshold=0.05, max_convex_hull=100, verbose=False\n                )\n                filename = f\"{os.path.splitext(obj_name)[0]}_collision.obj\"\n                output_path = os.path.join(mesh_folder, filename)\n                decompose_convex_mesh(\n                    mesh_output_path, output_path, **d_params\n                )\n                collision_mesh = f\"{self.output_mesh_dir}/{filename}\"\n            except Exception as e:\n                logger.warning(\n                    f\"Convex decomposition failed for {output_path}, {e}.\"\n                    \"Use original mesh for collision computation.\"\n                )\n        collision.set(\"filename\", collision_mesh)\n        collision.set(\"scale\", \"1.0 1.0 1.0\")\n\n    # Update friction coefficients\n    gazebo = link.find(\"collision/gazebo\")\n    if gazebo is not None:\n        for param, key in zip([\"mu1\", \"mu2\"], [\"mu1\", \"mu2\"]):\n            element = gazebo.find(param)\n            if element is not None:\n                element.text = f\"{attr_dict[key]:.2f}\"\n\n    # Update mass\n    inertial = link.find(\"inertial/mass\")\n    if inertial is not None:\n        mass_value = (attr_dict[\"min_mass\"] + attr_dict[\"max_mass\"]) / 2\n        inertial.set(\"value\", f\"{mass_value:.4f}\")\n\n    # Add extra_info element to the link\n    extra_info = link.find(\"extra_info/scale\")\n    if extra_info is not None:\n        extra_info.text = f\"{scale:.6f}\"\n\n    for key in self.attrs_name:\n        extra_info = link.find(f\"extra_info/{key}\")\n        if extra_info is not None and key in attr_dict:\n            extra_info.text = f\"{attr_dict[key]}\"\n\n    # 7. Write URDF to file\n    os.makedirs(output_dir, exist_ok=True)\n    urdf_path = os.path.join(output_dir, f\"{output_name}.urdf\")\n    tree = ET.ElementTree(robot)\n    tree.write(urdf_path, encoding=\"utf-8\", xml_declaration=True)\n\n    logger.info(f\"URDF file saved to {urdf_path}\")\n\n    return urdf_path\n</code></pre>"},{"location":"services/","title":"Interactive 3D Generation &amp; Visualization Services","text":"<p>EmbodiedGen provides a suite of interactive services that transform images and text into physically plausible, simulator-ready 3D assets. Each service is optimized for visual quality, simulation compatibility, and scalability \u2014 making it easy to create, edit, and explore assets for digital twin, robotic simulation, and AI embodiment scenarios.</p>"},{"location":"services/#prerequisites","title":"\u2699\ufe0f Prerequisites","text":"<p>Prerequisites</p> <p>Make sure to finish the Installation Guide before launching any service. Missing dependencies will cause initialization errors. Model weights are automatically downloaded on first run.</p>"},{"location":"services/#overview-of-available-services","title":"\ud83e\udde9 Overview of Available Services","text":"Service Description \ud83d\uddbc\ufe0f Image to 3D Generate physically plausible 3D asset URDF from single input image, offering high-quality support for digital twin systems. \ud83d\udcdd Text to 3D Generate physically plausible 3D assets from text descriptions for a wide range of geometry and styles. \ud83c\udfa8 Texture Edit Generate visually rich textures for existing 3D meshes. \ud83d\udcf8 Asset Gallery Explore and download EmbodiedGen All-Simulators-Ready Assets."},{"location":"services/#how-to-run-locally","title":"\u2699\ufe0f How to Run Locally","text":"<p>Quick Start</p> <p>Each service can be launched directly as a local Gradio app: <pre><code># Example: Run the Image-to-3D service\npython apps/image_to_3d.py\n</code></pre></p> <p>Models are automatically downloaded on first run. For full CLI usage, please check the corresponding tutorials.</p>"},{"location":"services/#next-steps","title":"\ud83e\udded Next Steps","text":"<ul> <li>\ud83d\udcd8 Tutorials \u2013 Learn how to use EmbodiedGen in generating interactive 3D scenes for embodied intelligence.</li> <li>\ud83e\uddf1 API Reference \u2013 Integrate EmbodiedGen code programmatically.</li> </ul> <p>\ud83d\udca1 EmbodiedGen bridges the gap between AI-driven 3D generation and physically grounded simulation, enabling true embodiment for intelligent agents.</p>"},{"location":"services/image_to_3d/","title":"Image-to-3D","text":""},{"location":"services/image_to_3d/#image-to-3d-service","title":"\ud83d\uddbc\ufe0f Image-to-3D Service","text":"<p>This service launches a web application to generate physically plausible 3D asset URDF from single input image, offering high-quality support for digital twin systems.</p>"},{"location":"services/image_to_3d/#run-the-app-service","title":"\u2601\ufe0f Run the App Service","text":"<p>Note</p> <p>Gradio servive is a simplified demonstration. For the full functionality, please refer to img3d-cli.</p> <p>Run the image-to-3D generation service locally. Models are automatically downloaded on first run, please be patient.</p> <pre><code># Run in foreground\npython apps/image_to_3d.py\n\n# Or run in the background\nCUDA_VISIBLE_DEVICES=0 nohup python apps/image_to_3d.py &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre> <p>Getting Started</p> <ul> <li>Try it directly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"services/text_to_3d/","title":"Text-to-3D","text":""},{"location":"services/text_to_3d/#text-to-3d-service","title":"\ud83d\udcdd Text-to-3D Service","text":"<p>This service launches a web application to generate physically plausible 3D assets from text descriptions for a wide range of geometry and styles.</p> <p>\"Antique brass key, intricate filigree\"</p> <p>\"Rusty old wrench, peeling paint\"</p> <p>\"Sleek black drone, red sensors\"</p> <p>\"Miniature screwdriver with bright orange handle\"</p> <p>\"European style wooden dressing table\"</p>"},{"location":"services/text_to_3d/#run-the-app-service","title":"\u2601\ufe0f Run the App Service","text":"<p>Create 3D assets from text descriptions for a wide range of geometry and styles.</p> <p>Note</p> <p>Gradio servive is a simplified demonstration. For the full functionality, please refer to text3d-cli.</p> <p>Text-to-image model based on the Kolors model, supporting Chinese and English prompts. Models downloaded automatically on first run, please be patient.</p> <pre><code># Run in foreground\npython apps/text_to_3d.py\n\n# Or run in the background\nCUDA_VISIBLE_DEVICES=0 nohup python apps/text_to_3d.py &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre> <p>Getting Started</p> <ul> <li>You can also try Text-to-3D instantly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"services/texture_edit/","title":"Texture Generation","text":""},{"location":"services/texture_edit/#texture-generation-service","title":"\ud83c\udfa8 Texture Generation Service","text":"<p>This service launches a web application to generate visually rich textures for 3D mesh.</p>"},{"location":"services/texture_edit/#run-the-app-service","title":"\u2601\ufe0f Run the App Service","text":"<p>Note</p> <p>Gradio servive is a simplified demonstration. For the full functionality, please refer to texture-cli.</p> <p>Run the texture generation service locally. Models downloaded automatically on first run, see <code>download_kolors_weights</code>, <code>geo_cond_mv</code>.</p> <pre><code># Run in foreground\npython apps/texture_edit.py\n\n# Or run in the background\nCUDA_VISIBLE_DEVICES=0 nohup python apps/texture_edit.py &gt; /dev/null 2&gt;&amp;1 &amp;\n</code></pre> <p>Getting Started</p> <ul> <li>Try it directly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"services/visualize_asset/","title":"\ud83d\udcf8 EmbodiedGen All-Simulators-Ready Assets Gallery","text":"<p>Getting Started</p> <ul> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"tutorials/","title":"Overview","text":""},{"location":"tutorials/#tutorials-interface-usage","title":"Tutorials &amp; Interface Usage","text":"<p>Welcome to the tutorials for <code>EmbodiedGen</code>. <code>EmbodiedGen</code> is a powerful toolset for generating 3D assets, textures, scenes, and interactive layouts ready for simulators and digital twin environments.</p>"},{"location":"tutorials/#prerequisites","title":"\u2699\ufe0f Prerequisites","text":"<p>Prerequisites</p> <p>Make sure to finish the Installation Guide before starting tutorial. Missing dependencies will cause initialization errors. Model weights are automatically downloaded on first run.</p>"},{"location":"tutorials/#image-to-3d","title":"\ud83d\uddbc\ufe0f Image-to-3D","text":"<p>Generate physically plausible 3D assets from a single input image, supporting digital twin and simulation environments.</p>"},{"location":"tutorials/#text-to-3d","title":"\ud83d\udcdd Text-to-3D","text":"<p>Create physically plausible 3D assets from text descriptions, supporting a wide range of geometry, style, and material details.</p> <p>\"small bronze figurine of a lion\"</p> <p>\"A globe with wooden base\"</p> <p>\"wooden table with embroidery\"</p>"},{"location":"tutorials/#texture-generation","title":"\ud83c\udfa8 Texture Generation","text":"<p>Generate high-quality textures for 3D meshes using text prompts, supporting both Chinese and English, to enhance the visual appearance of existing 3D assets.</p>"},{"location":"tutorials/#3d-scene-generation","title":"\ud83c\udf0d 3D Scene Generation","text":"<p>Generate physically consistent and visually coherent 3D environments from text prompts. Typically used as background 3DGS scenes in simulators for efficient and photo-realistic rendering.</p> <p></p>"},{"location":"tutorials/#layout-generation","title":"\ud83c\udfde\ufe0f Layout Generation","text":"<p>Generate diverse, physically realistic, and scalable interactive 3D scenes from natural language task descriptions, while also modeling the robot and manipulable objects.</p>"},{"location":"tutorials/#parallel-simulation","title":"\ud83c\udfce\ufe0f Parallel Simulation","text":"<p>Generate multiple parallel simulation environments with <code>gym.make</code> and record sensor and trajectory data.</p>"},{"location":"tutorials/#use-in-any-simulator","title":"\ud83c\udfae Use in Any Simulator","text":"<p>Seamlessly use EmbodiedGen-generated assets in major simulators like IsaacSim, MuJoCo, Genesis, PyBullet, IsaacGym, and SAPIEN, featuring accurate physical collisions and consistent visual appearance.</p>"},{"location":"tutorials/#real-to-sim-digital-twin-creation","title":"\ud83d\udd27 Real-to-Sim Digital Twin Creation","text":""},{"location":"tutorials/any_simulators/","title":"\ud83c\udfae Use EmbodiedGen in Any Simulator","text":"<p>Leverage EmbodiedGen-generated assets with accurate physical collisions and consistent visual appearance across major simulation engines \u2014  IsaacSim, MuJoCo, Genesis, PyBullet, IsaacGym, and SAPIEN.</p> <p>Universal Compatibility</p> <p>EmbodiedGen assets follow standardized URDF semantics with physically consistent collision meshes, enabling seamless loading across multiple simulation frameworks \u2014 no manual editing needed.</p>"},{"location":"tutorials/any_simulators/#supported-simulators","title":"\ud83e\udde9 Supported Simulators","text":"Simulator Conversion Class IsaacSim <code>MeshtoUSDConverter</code> MuJoCo / Genesis <code>MeshtoMJCFConverter</code> SAPIEN / IsaacGym / PyBullet <code>.urdf</code> generated by EmbodiedGen can be used directly <p>Simulator Integration Overview</p> <p>This table summarizes the compatibility of EmbodiedGen assets with various simulators:</p> Simulator Supported Format Notes IsaacSim USD / .usda Use <code>MeshtoUSDConverter</code> to convert mesh to USD format. MuJoCo MJCF (.xml) Use <code>MeshtoMJCFConverter</code> for physics-ready assets. Genesis MJCF (.xml) Same as MuJoCo; fully compatible with Genesis scenes. SAPIEN URDF (.urdf) Can directly load EmbodiedGen <code>.urdf</code> assets. IsaacGym URDF (.urdf) Directly usable. PyBullet URDF (.urdf) Directly usable."},{"location":"tutorials/any_simulators/#example-conversion-to-target-simulator","title":"\ud83e\uddf1 Example: Conversion to Target Simulator","text":"<pre><code>from embodied_gen.data.asset_converter import SimAssetMapper, cvt_embodiedgen_asset_to_anysim\nfrom typing import Literal\n\nsimulator_name: Literal[\n    \"isaacsim\",\n    \"isaacgym\",\n    \"genesis\",\n    \"pybullet\",\n    \"sapien3\",\n    \"mujoco\",\n] = \"mujoco\"\n\ndst_asset_path = cvt_embodiedgen_asset_to_anysim(\n    urdf_files=[\n        \"path1_to_embodiedgen_asset/asset.urdf\",\n        \"path2_to_embodiedgen_asset/asset.urdf\",\n    ],\n    target_type=SimAssetMapper[simulator_name],\n    source_type=AssetType.MESH,\n    overwrite=True,\n)\n</code></pre> <p>Collision and visualization mesh across simulators, showing consistent geometry and material fidelity.</p>"},{"location":"tutorials/digital_twin/","title":"Real-to-Sim Digital Twin Creation","text":""},{"location":"tutorials/gym_env/","title":"Simulation in Parallel Envs","text":"<p>Generate multiple parallel simulation environments with <code>gym.make</code> and record sensor and trajectory data.</p>"},{"location":"tutorials/gym_env/#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<pre><code>python embodied_gen/scripts/parallel_sim.py \\\n--layout_file \"outputs/layouts_gen/task_0000/layout.json\" \\\n--output_dir \"outputs/parallel_sim/task_0000\" \\\n--num_envs 16\n</code></pre>"},{"location":"tutorials/image_to_3d/","title":"Image-to-3D","text":""},{"location":"tutorials/image_to_3d/#image-to-3d-physically-plausible-3d-asset-generation","title":"\ud83d\uddbc\ufe0f Image-to-3D: Physically Plausible 3D Asset Generation","text":"<p>Generate physically plausible 3D assets from a single input image, supporting digital twin and simulation environments.</p>"},{"location":"tutorials/image_to_3d/#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<pre><code>img3d-cli --image_path apps/assets/example_image/sample_00.jpg \\\napps/assets/example_image/sample_01.jpg apps/assets/example_image/sample_19.jpg \\\n--n_retry 1 --output_root outputs/imageto3d\n</code></pre> <p>You will get the following results:</p> <p>The generated results are organized as follows: <pre><code>outputs/imageto3d/sample_xx/result\n\u251c\u2500\u2500 mesh\n\u2502   \u251c\u2500\u2500 material_0.png\n\u2502   \u251c\u2500\u2500 material.mtl\n\u2502   \u251c\u2500\u2500 sample_xx_collision.ply\n\u2502   \u251c\u2500\u2500 sample_xx.glb\n\u2502   \u251c\u2500\u2500 sample_xx_gs.ply\n\u2502   \u2514\u2500\u2500 sample_xx.obj\n\u251c\u2500\u2500 sample_xx.urdf\n\u2514\u2500\u2500 video.mp4\n</code></pre></p> <ul> <li><code>mesh/</code> \u2192 Geometry and texture files, including visual mesh, collision mesh and 3DGS.</li> <li><code>*.urdf</code> \u2192 Simulator-ready URDF with collision and visual meshes</li> <li><code>video.mp4</code> \u2192 Preview of the generated 3D asset</li> </ul> <p>Getting Started</p> <ul> <li>Try it directly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"tutorials/layout_gen/","title":"\ud83c\udfde\ufe0f Layout Generation \u2014 Interactive 3D Scenes","text":"<p>Layout Generation enables the generation of diverse, physically realistic, and scalable interactive 3D scenes directly from natural language task descriptions, while also modeling the robot's pose and relationships with manipulable objects. Target objects are randomly placed within the robot's reachable range, making the scenes readily usable for downstream simulation and reinforcement learning tasks in any mainstream simulator.</p> <p>Model Requirement</p> <p>The text-to-image model is based on <code>SD3.5 Medium</code>. Usage requires agreement to the model license.</p>"},{"location":"tutorials/layout_gen/#prerequisites-prepare-background-3d-scenes","title":"Prerequisites \u2014 Prepare Background 3D Scenes","text":"<p>Before running <code>layout-cli</code>, you need to prepare background 3D scenes. You can either generate your own using the <code>scene3d-cli</code>, or download pre-generated backgrounds for convenience.</p> <p>Each scene takes approximately 30 minutes to generate. For efficiency, we recommend pre-generating and listing them in <code>outputs/bg_scenes/scene_list.txt</code>.</p> <pre><code># Option 1: Download pre-generated backgrounds (~4 GB)\nhf download xinjjj/scene3d-bg --repo-type dataset --local-dir outputs\n\n# Option 2: Download a larger background set (~14 GB)\nhf download xinjjj/EmbodiedGenRLv2-BG --repo-type dataset --local-dir outputs\n</code></pre>"},{"location":"tutorials/layout_gen/#generate-interactive-layout-scenes","title":"Generate Interactive Layout Scenes","text":"<p>Use the <code>layout-cli</code> to create interactive 3D scenes based on task descriptions. Each layout generation takes approximately 30 minutes.</p> <pre><code>layout-cli \\\n  --task_descs \"Place the pen in the mug on the desk\" \\\n               \"Put the fruit on the table on the plate\" \\\n  --bg_list \"outputs/bg_scenes/scene_list.txt\" \\\n  --output_root \"outputs/layouts_gen\" \\\n  --insert_robot\n</code></pre> <p>You will get the following results:</p>"},{"location":"tutorials/layout_gen/#batch-generation","title":"Batch Generation","text":"<p>You can also run multiple tasks via a task list file in the backend.</p> <pre><code>CUDA_VISIBLE_DEVICES=0 nohup layout-cli \\\n  --task_descs \"apps/assets/example_layout/task_list.txt\" \\\n  --bg_list \"outputs/bg_scenes/scene_list.txt\" \\\n  --output_root \"outputs/layouts_gens\" \\\n  --insert_robot &gt; layouts_gens.log &amp;\n</code></pre> <p>\ud83d\udca1 Remove <code>--insert_robot</code> if you don\u2019t need robot pose consideration in layout generation.</p>"},{"location":"tutorials/layout_gen/#layout-randomization","title":"Layout Randomization","text":"<p>Using <code>compose_layout.py</code>, you can recompose the layout of the generated interactive 3D scenes.</p> <pre><code>python embodied_gen/scripts/compose_layout.py \\\n--layout_path \"outputs/layouts_gens/task_0000/layout.json\" \\\n--output_dir \"outputs/layouts_gens/task_0000/recompose\" \\\n--insert_robot\n</code></pre>"},{"location":"tutorials/layout_gen/#load-interactive-3d-scenes-in-simulators","title":"Load Interactive 3D Scenes in Simulators","text":"<p>We provide <code>sim-cli</code>, that allows users to easily load generated layouts into an interactive 3D simulation using the SAPIEN engine.</p> <pre><code>sim-cli --layout_path \"outputs/layouts_gen/task_0000/layout.json\" \\\n--output_dir \"outputs/layouts_gen/task_0000/sapien_render\" --insert_robot\n</code></pre> <p>Recommended Workflow</p> <ol> <li>Generate or download background scenes using <code>scene3d-cli</code>.</li> <li>Create interactive layouts from task descriptions using <code>layout-cli</code>.</li> <li>Optionally recompose them using <code>compose_layout.py</code>.</li> <li>Load the final layouts into simulators with <code>sim-cli</code>.</li> </ol>"},{"location":"tutorials/scene_gen/","title":"\ud83c\udf0d 3D Scene Generation","text":"<p>Generate physically consistent and visually coherent 3D environments from text prompts. Typically used as background 3DGS scenes in simulators for efficient and photo-realistic rendering.</p> <p></p>"},{"location":"tutorials/scene_gen/#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<p>\ud83d\udca1 Run <code>bash install.sh extra</code> to install additional dependencies if you plan to use <code>scene3d-cli</code>.</p> <p>It typically takes ~30 minutes per scene to generate both the colored mesh and 3D Gaussian Splat(3DGS) representation.</p> <pre><code>CUDA_VISIBLE_DEVICES=0 scene3d-cli \\\n  --prompts \"Art studio with easel and canvas\" \\\n  --output_dir outputs/bg_scenes/ \\\n  --seed 0 \\\n  --gs3d.max_steps 4000 \\\n  --disable_pano_check\n</code></pre> <p>The generated results are organized as follows: <pre><code>outputs/bg_scenes/scene_000\n\u251c\u2500\u2500 gs_model.ply\n\u251c\u2500\u2500 gsplat_cfg.yml\n\u251c\u2500\u2500 mesh_model.ply\n\u251c\u2500\u2500 pano_image.png\n\u251c\u2500\u2500 prompt.txt\n\u2514\u2500\u2500 video.mp4\n</code></pre></p> <ul> <li><code>gs_model.ply</code> \u2192 Generated 3D scene in 3D Gaussian Splat representation.</li> <li><code>mesh_model.ply</code> \u2192 Color mesh representation of the generated scene.</li> <li><code>gsplat_cfg.yml</code> \u2192 Configuration file for 3DGS training and rendering parameters.</li> <li><code>pano_image.png</code> \u2192 Generated panoramic view image.</li> <li><code>prompt.txt</code> \u2192 Original scene generation prompt for traceability.</li> <li><code>video.mp4</code> \u2192 Preview RGB and depth preview of the generated 3D scene.</li> </ul> <p>Usage Notes</p> <ul> <li><code>3D Scene Generation</code> produces background 3DGS scenes optimized for efficient rendering in simulation environments. We also provide hybrid rendering examples combining background 3DGS with foreground interactive assets, see the example for details.</li> <li>In Layout Generation, we further demonstrate task-desc-driven interactive 3D scene generation, building complete 3D scenes based on natural language task descriptions. See the Layout Generation Guide.</li> </ul>"},{"location":"tutorials/text_to_3d/","title":"Text-to-3D","text":""},{"location":"tutorials/text_to_3d/#text-to-3d-generate-3d-assets-from-text","title":"\ud83d\udcdd Text-to-3D: Generate 3D Assets from Text","text":"<p>Create physically plausible 3D assets from text descriptions, supporting a wide range of geometry, style, and material details.</p>"},{"location":"tutorials/text_to_3d/#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<p>Basic CLI(recommend)</p> <p>Text-to-image model based on Stable Diffusion 3.5 Medium\uff0c English prompts only. Usage requires agreement to the model license (click \u201cAccept\u201d).</p> <pre><code>text3d-cli \\\n  --prompts \"small bronze figurine of a lion\" \"A globe with wooden base\" \"wooden table with embroidery\" \\\n  --n_image_retry 1 \\\n  --n_asset_retry 1 \\\n  --n_pipe_retry 1 \\\n  --seed_img 0 \\\n  --output_root outputs/textto3d\n</code></pre> <ul> <li><code>--n_image_retry</code>: Number of retries per prompt for text-to-image generation</li> <li><code>--n_asset_retry</code>: Retry attempts for image-to-3D assets generation</li> <li><code>--n_pipe_retry</code>: Pipeline retry for end-to-end 3D asset quality check</li> <li><code>--seed_img</code>: Optional initial seed image for style guidance</li> <li><code>--output_root</code>: Directory to save generated assets</li> </ul> <p>For large-scale 3D asset generation, set <code>--n_image_retry=4</code> <code>--n_asset_retry=3</code> <code>--n_pipe_retry=2</code>, slower but better, via automatic checking and retries. For more diverse results, omit <code>--seed_img</code>.</p> <p>You will get the following results:</p> <p>\"small bronze figurine of a lion\"</p> <p>\"A globe with wooden base\"</p> <p>\"wooden table with embroidery\"</p> <p>Kolors Model CLI (Supports Chinese &amp; English Prompts): <pre><code>bash embodied_gen/scripts/textto3d.sh \\\n  --prompts \"small bronze figurine of a lion\" \"A globe with wooden base and latitude and longitude lines\" \"\u6a59\u8272\u7535\u52a8\u624b\u94bb\uff0c\u6709\u78e8\u635f\u7ec6\u8282\" \\\n  --output_root outputs/textto3d_k\n</code></pre></p> <p>Models with more permissive licenses can be found in <code>embodied_gen/models/image_comm_model.py</code>.</p> <p>The generated results are organized as follows: <pre><code>outputs/textto3d\n\u251c\u2500\u2500 asset3d\n\u2502   \u251c\u2500\u2500 sample3d_xx\n\u2502   \u2502   \u2514\u2500\u2500 result\n\u2502   \u2502       \u251c\u2500\u2500 mesh\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 material_0.png\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 material.mtl\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 sample3d_xx_collision.obj\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 sample3d_xx.glb\n\u2502   \u2502       \u2502   \u251c\u2500\u2500 sample3d_xx_gs.ply\n\u2502   \u2502       \u2502   \u2514\u2500\u2500 sample3d_xx.obj\n\u2502   \u2502       \u251c\u2500\u2500 sample3d_xx.urdf\n\u2502   \u2502       \u2514\u2500\u2500 video.mp4\n\u2514\u2500\u2500 images\n    \u251c\u2500\u2500 sample3d_xx.png\n    \u251c\u2500\u2500 sample3d_xx_raw.png\n</code></pre></p> <ul> <li><code>mesh/</code> \u2192 3D geometry and texture files for the asset, including visual mesh, collision mesh and 3DGS</li> <li><code>*.urdf</code> \u2192 Simulator-ready URDF including collision and visual meshes</li> <li><code>video.mp4</code> \u2192 Preview video of the generated 3D asset</li> <li><code>images/sample3d_xx.png</code> \u2192 Foreground-extracted image used for image-to-3D step</li> <li><code>images/sample3d_xx_raw.png</code> \u2192 Original generated image from the text-to-image step</li> </ul> <p>Getting Started</p> <ul> <li>You can also try Text-to-3D instantly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"},{"location":"tutorials/texture_edit/","title":"Texture Generation","text":""},{"location":"tutorials/texture_edit/#texture-generation-create-visually-rich-textures-for-3d-meshes","title":"\ud83c\udfa8 Texture Generation: Create Visually Rich Textures for 3D Meshes","text":"<p>Generate high-quality textures for 3D meshes using text prompts, supporting both Chinese and English. This allows you to enhance the visual appearance of existing 3D assets for simulation, visualization, or digital twin applications.</p>"},{"location":"tutorials/texture_edit/#command-line-usage","title":"\u26a1 Command-Line Usage","text":"<pre><code>texture-cli \\\n  --mesh_path \"apps/assets/example_texture/meshes/robot_text.obj\" \\\n              \"apps/assets/example_texture/meshes/horse.obj\" \\\n  --prompt \"\u4e3e\u7740\u724c\u5b50\u7684\u5199\u5b9e\u98ce\u683c\u673a\u5668\u4eba\uff0c\u5927\u773c\u775b\uff0c\u724c\u5b50\u4e0a\u5199\u7740\u201cHello\u201d\u7684\u6587\u5b57\" \\\n           \"A gray horse head with flying mane and brown eyes\" \\\n  --output_root \"outputs/texture_gen\" \\\n  --seed 0\n</code></pre> <ul> <li><code>--mesh_path</code> \u2192 Path(s) to input 3D mesh files</li> <li><code>--prompt</code> \u2192 Text prompt(s) describing desired texture/style for each mesh</li> <li><code>--output_root</code> \u2192 Directory to save textured meshes and related outputs</li> <li><code>--seed</code> \u2192 Random seed for reproducible texture generation</li> </ul> <p>You will get the following results:</p> <p>Getting Started</p> <ul> <li>Try it directly online via our Hugging Face Space \u2014 no installation required.</li> <li>Explore EmbodiedGen generated sim-ready Assets Gallery.</li> <li>For instructions on using the generated asset in any simulator, see Any Simulators Tutorial.</li> </ul>"}]}